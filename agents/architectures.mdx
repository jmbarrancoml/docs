---
title: "Agent architectures"
description: "Comprehensive guide to AI agent architectures, from reactive systems to sophisticated reasoning agents, including ReAct, chain-of-thought, planning agents, and hybrid architectures."
---

# Agent architectures

AI agent architectures define how autonomous systems perceive, reason, and act in their environments. This comprehensive guide explores the spectrum of agent designs, from simple reactive systems to sophisticated reasoning agents capable of complex problem-solving and planning.

## Understanding agent architectures

### Foundational concepts

Agent architectures provide the structural framework for how AI systems process information and make decisions:

```python
import numpy as np
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum
import json
import time

class AgentState(Enum):
    IDLE = "idle"
    PERCEIVING = "perceiving"
    REASONING = "reasoning"
    PLANNING = "planning"
    ACTING = "acting"
    LEARNING = "learning"

@dataclass
class Observation:
    """Represents an observation from the environment"""
    data: Any
    timestamp: float
    source: str
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Action:
    """Represents an action to be taken in the environment"""
    type: str
    parameters: Dict[str, Any]
    target: Optional[str] = None
    priority: float = 1.0
    expected_outcome: Optional[str] = None

@dataclass
class Memory:
    """Agent memory structure"""
    working_memory: List[Any] = field(default_factory=list)
    episodic_memory: List[Dict] = field(default_factory=list)
    semantic_memory: Dict[str, Any] = field(default_factory=dict)
    procedural_memory: Dict[str, Callable] = field(default_factory=dict)

class AgentArchitecture(ABC):
    """Abstract base class for agent architectures"""
    
    def __init__(self, name: str):
        self.name = name
        self.state = AgentState.IDLE
        self.memory = Memory()
        self.observation_history = []
        self.action_history = []
        
    @abstractmethod
    def perceive(self, environment: Any) -> List[Observation]:
        """Perceive the current state of the environment"""
        pass
    
    @abstractmethod
    def reason(self, observations: List[Observation]) -> Any:
        """Reason about the observations and current situation"""
        pass
    
    @abstractmethod
    def act(self, reasoning_output: Any) -> List[Action]:
        """Decide and execute actions based on reasoning"""
        pass
    
    def step(self, environment: Any) -> List[Action]:
        """Execute one step of the agent's decision cycle"""
        # Perception
        self.state = AgentState.PERCEIVING
        observations = self.perceive(environment)
        self.observation_history.extend(observations)
        
        # Reasoning
        self.state = AgentState.REASONING
        reasoning_output = self.reason(observations)
        
        # Action
        self.state = AgentState.ACTING
        actions = self.act(reasoning_output)
        self.action_history.extend(actions)
        
        self.state = AgentState.IDLE
        return actions
    
    def get_state(self) -> Dict[str, Any]:
        """Get current agent state information"""
        return {
            'name': self.name,
            'current_state': self.state.value,
            'memory_size': len(self.memory.working_memory),
            'observation_count': len(self.observation_history),
            'action_count': len(self.action_history)
        }
```

### Agent taxonomy and classification

Different agent architectures serve different purposes and complexity levels:

```python
class ArchitectureType(Enum):
    REACTIVE = "reactive"
    DELIBERATIVE = "deliberative"
    HYBRID = "hybrid"
    LAYERED = "layered"
    REASONING = "reasoning"
    PLANNING = "planning"

class ComplexityLevel(Enum):
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    ADVANCED = "advanced"

@dataclass
class ArchitectureSpec:
    """Specification for an agent architecture"""
    name: str
    type: ArchitectureType
    complexity: ComplexityLevel
    capabilities: List[str]
    use_cases: List[str]
    advantages: List[str]
    limitations: List[str]
    
class ArchitectureTaxonomy:
    """Comprehensive taxonomy of agent architectures"""
    
    def __init__(self):
        self.architectures = {}
        self._initialize_standard_architectures()
    
    def _initialize_standard_architectures(self):
        """Initialize standard agent architectures"""
        
        # Reactive architectures
        self.register_architecture(ArchitectureSpec(
            name="Reflex Agent",
            type=ArchitectureType.REACTIVE,
            complexity=ComplexityLevel.SIMPLE,
            capabilities=["immediate_response", "rule_based_behavior"],
            use_cases=["simple_automation", "basic_robotics", "alert_systems"],
            advantages=["fast_response", "simple_implementation", "predictable"],
            limitations=["no_memory", "limited_flexibility", "no_planning"]
        ))
        
        self.register_architecture(ArchitectureSpec(
            name="Model-Based Reflex Agent",
            type=ArchitectureType.REACTIVE,
            complexity=ComplexityLevel.MODERATE,
            capabilities=["state_tracking", "model_based_decisions", "condition_action_rules"],
            use_cases=["game_ai", "monitoring_systems", "basic_assistants"],
            advantages=["state_awareness", "more_flexible", "handles_partial_observability"],
            limitations=["limited_reasoning", "reactive_only", "no_goal_planning"]
        ))
        
        # Deliberative architectures
        self.register_architecture(ArchitectureSpec(
            name="Goal-Based Agent",
            type=ArchitectureType.DELIBERATIVE,
            complexity=ComplexityLevel.COMPLEX,
            capabilities=["goal_setting", "planning", "deliberation"],
            use_cases=["task_automation", "personal_assistants", "resource_management"],
            advantages=["goal_oriented", "flexible_behavior", "can_plan_ahead"],
            limitations=["slower_response", "computational_overhead", "complexity"]
        ))
        
        self.register_architecture(ArchitectureSpec(
            name="Utility-Based Agent",
            type=ArchitectureType.DELIBERATIVE,
            complexity=ComplexityLevel.COMPLEX,
            capabilities=["utility_calculation", "decision_optimization", "trade_off_analysis"],
            use_cases=["resource_allocation", "financial_planning", "optimization_tasks"],
            advantages=["optimal_decisions", "handles_conflicting_goals", "quantitative_reasoning"],
            limitations=["utility_function_design", "computational_complexity", "preference_elicitation"]
        ))
        
        # Reasoning architectures
        self.register_architecture(ArchitectureSpec(
            name="ReAct Agent",
            type=ArchitectureType.REASONING,
            complexity=ComplexityLevel.ADVANCED,
            capabilities=["reasoning_action_cycles", "tool_usage", "self_reflection"],
            use_cases=["research_assistance", "complex_problem_solving", "interactive_tasks"],
            advantages=["transparent_reasoning", "tool_integration", "iterative_improvement"],
            limitations=["token_intensive", "potential_loops", "reasoning_quality_dependence"]
        ))
        
        self.register_architecture(ArchitectureSpec(
            name="Chain-of-Thought Agent",
            type=ArchitectureType.REASONING,
            complexity=ComplexityLevel.ADVANCED,
            capabilities=["step_by_step_reasoning", "explanation_generation", "logical_decomposition"],
            use_cases=["educational_systems", "analytical_tasks", "decision_support"],
            advantages=["interpretable_reasoning", "structured_thinking", "error_detection"],
            limitations=["verbose_outputs", "potential_hallucination", "linear_reasoning"]
        ))
    
    def register_architecture(self, spec: ArchitectureSpec):
        """Register a new architecture specification"""
        self.architectures[spec.name] = spec
    
    def get_architecture_by_type(self, arch_type: ArchitectureType) -> List[str]:
        """Get all architectures of a specific type"""
        return [name for name, spec in self.architectures.items() 
                if spec.type == arch_type]
    
    def recommend_architecture(self, 
                             requirements: Dict[str, Any]) -> List[Tuple[str, float]]:
        """Recommend architectures based on requirements"""
        
        scores = []
        
        for name, spec in self.architectures.items():
            score = 0.0
            
            # Match complexity requirements
            if requirements.get('complexity') == spec.complexity:
                score += 2.0
            
            # Match required capabilities
            required_caps = requirements.get('capabilities', [])
            matching_caps = set(required_caps) & set(spec.capabilities)
            if required_caps:
                score += len(matching_caps) / len(required_caps)
            
            # Match use cases
            required_use_cases = requirements.get('use_cases', [])
            matching_use_cases = set(required_use_cases) & set(spec.use_cases)
            if required_use_cases:
                score += len(matching_use_cases) / len(required_use_cases)
            
            scores.append((name, score))
        
        # Sort by score descending
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:3]  # Top 3 recommendations
```

## Reactive architectures

### Simple reflex agents

The most basic agents that react immediately to stimuli without internal state:

```python
class ReflexAgent(AgentArchitecture):
    """Simple reflex agent that responds based on condition-action rules"""
    
    def __init__(self, name: str = "ReflexAgent"):
        super().__init__(name)
        self.rules = {}  # condition -> action mapping
        self.sensors = []
        
    def add_rule(self, condition: Callable[[List[Observation]], bool], 
                 action_generator: Callable[[], List[Action]]):
        """Add a condition-action rule"""
        rule_id = len(self.rules)
        self.rules[rule_id] = {
            'condition': condition,
            'action_generator': action_generator
        }
        
    def perceive(self, environment: Any) -> List[Observation]:
        """Simple perception based on sensors"""
        observations = []
        
        # Mock sensor readings
        for sensor in self.sensors:
            reading = sensor.read(environment)
            obs = Observation(
                data=reading,
                timestamp=time.time(),
                source=sensor.name,
                confidence=sensor.reliability
            )
            observations.append(obs)
        
        return observations
    
    def reason(self, observations: List[Observation]) -> List[int]:
        """Find matching rules for current observations"""
        triggered_rules = []
        
        for rule_id, rule in self.rules.items():
            if rule['condition'](observations):
                triggered_rules.append(rule_id)
        
        return triggered_rules
    
    def act(self, reasoning_output: List[int]) -> List[Action]:
        """Execute actions for triggered rules"""
        actions = []
        
        for rule_id in reasoning_output:
            rule_actions = self.rules[rule_id]['action_generator']()
            actions.extend(rule_actions)
        
        return actions

class Sensor:
    """Simple sensor abstraction"""
    
    def __init__(self, name: str, reliability: float = 1.0):
        self.name = name
        self.reliability = reliability
    
    def read(self, environment: Any) -> Any:
        """Read from environment (mock implementation)"""
        # In practice, this would interface with actual sensors
        return f"sensor_reading_{np.random.random():.2f}"

# Example usage
def temperature_too_high(observations: List[Observation]) -> bool:
    """Check if temperature is too high"""
    for obs in observations:
        if obs.source == "temperature_sensor":
            try:
                temp = float(obs.data.split("_")[-1])
                return temp > 0.8
            except:
                pass
    return False

def turn_on_cooling() -> List[Action]:
    """Generate cooling action"""
    return [Action(type="cooling", parameters={"mode": "on", "intensity": 0.7})]

# Create and configure reflex agent
reflex_agent = ReflexAgent("ThermostatAgent")
temp_sensor = Sensor("temperature_sensor", reliability=0.95)
reflex_agent.sensors.append(temp_sensor)
reflex_agent.add_rule(temperature_too_high, turn_on_cooling)
```

### Model-based reflex agents

More sophisticated reactive agents that maintain internal state:

```python
class ModelBasedReflexAgent(AgentArchitecture):
    """Model-based reflex agent with internal state tracking"""
    
    def __init__(self, name: str = "ModelBasedAgent"):
        super().__init__(name)
        self.world_model = {}  # Internal representation of world state
        self.rules = {}
        self.sensors = []
        self.actuators = []
        
    def add_model_rule(self, 
                      condition: Callable[[Dict, List[Observation]], bool],
                      action_generator: Callable[[Dict], List[Action]],
                      state_updater: Callable[[Dict, List[Observation], List[Action]], Dict]):
        """Add rule that considers both state and observations"""
        rule_id = len(self.rules)
        self.rules[rule_id] = {
            'condition': condition,
            'action_generator': action_generator,
            'state_updater': state_updater
        }
    
    def perceive(self, environment: Any) -> List[Observation]:
        """Perceive environment state"""
        observations = []
        
        for sensor in self.sensors:
            reading = sensor.read(environment)
            obs = Observation(
                data=reading,
                timestamp=time.time(),
                source=sensor.name,
                confidence=sensor.reliability,
                metadata={'sensor_type': type(sensor).__name__}
            )
            observations.append(obs)
        
        return observations
    
    def reason(self, observations: List[Observation]) -> Dict[str, Any]:
        """Reason about observations considering current world model"""
        
        # Update world model based on observations
        self.update_world_model(observations)
        
        # Find applicable rules
        triggered_rules = []
        for rule_id, rule in self.rules.items():
            if rule['condition'](self.world_model, observations):
                triggered_rules.append(rule_id)
        
        return {
            'triggered_rules': triggered_rules,
            'world_state': self.world_model.copy(),
            'observations': observations
        }
    
    def act(self, reasoning_output: Dict[str, Any]) -> List[Action]:
        """Generate actions based on reasoning output"""
        actions = []
        
        for rule_id in reasoning_output['triggered_rules']:
            rule = self.rules[rule_id]
            rule_actions = rule['action_generator'](self.world_model)
            actions.extend(rule_actions)
            
            # Update world model based on expected action effects
            self.world_model = rule['state_updater'](
                self.world_model, 
                reasoning_output['observations'],
                rule_actions
            )
        
        return actions
    
    def update_world_model(self, observations: List[Observation]):
        """Update internal world model based on observations"""
        for obs in observations:
            if obs.source not in self.world_model:
                self.world_model[obs.source] = {}
            
            self.world_model[obs.source].update({
                'last_reading': obs.data,
                'timestamp': obs.timestamp,
                'confidence': obs.confidence
            })
            
            # Parse sensor data to extract meaningful state
            if 'temperature' in obs.source:
                try:
                    temp_val = float(obs.data.split("_")[-1])
                    self.world_model['environment_temperature'] = temp_val
                except:
                    pass
            elif 'motion' in obs.source:
                self.world_model['motion_detected'] = 'motion' in obs.data.lower()

# Example smart thermostat with model-based reasoning
def smart_cooling_condition(world_model: Dict, observations: List[Observation]) -> bool:
    """Condition that considers temperature trend and occupancy"""
    current_temp = world_model.get('environment_temperature', 0.0)
    motion_detected = world_model.get('motion_detected', False)
    
    # Cool if temperature is high AND someone is present
    return current_temp > 0.7 and motion_detected

def generate_smart_cooling(world_model: Dict) -> List[Action]:
    """Generate context-aware cooling action"""
    current_temp = world_model.get('environment_temperature', 0.0)
    
    # Adjust intensity based on temperature
    if current_temp > 0.9:
        intensity = 1.0
    elif current_temp > 0.8:
        intensity = 0.7
    else:
        intensity = 0.5
        
    return [Action(
        type="smart_cooling",
        parameters={"intensity": intensity, "duration": 300},
        expected_outcome=f"temperature_reduction_{intensity}"
    )]

def update_cooling_state(world_model: Dict, observations: List[Observation], actions: List[Action]) -> Dict:
    """Update world model after cooling action"""
    new_model = world_model.copy()
    
    for action in actions:
        if action.type == "smart_cooling":
            # Predict temperature decrease
            intensity = action.parameters.get('intensity', 0.5)
            current_temp = new_model.get('environment_temperature', 0.0)
            new_model['predicted_temperature'] = max(0.0, current_temp - intensity * 0.1)
            new_model['cooling_active'] = True
            new_model['cooling_start_time'] = time.time()
    
    return new_model

# Create smart thermostat agent
smart_thermostat = ModelBasedReflexAgent("SmartThermostat")
smart_thermostat.sensors.extend([
    Sensor("temperature_sensor", 0.95),
    Sensor("motion_sensor", 0.90)
])

smart_thermostat.add_model_rule(
    smart_cooling_condition,
    generate_smart_cooling,
    update_cooling_state
)
```

## Deliberative architectures

### Goal-based agents

Agents that work towards explicit goals through planning and deliberation:

```python
@dataclass
class Goal:
    """Represents an agent goal"""
    description: str
    priority: float
    deadline: Optional[float] = None
    success_criteria: Dict[str, Any] = field(default_factory=dict)
    current_progress: float = 0.0
    status: str = "active"  # active, completed, failed, paused
    
class Plan:
    """Represents a plan to achieve goals"""
    
    def __init__(self, goal: Goal):
        self.goal = goal
        self.steps = []
        self.current_step = 0
        self.estimated_cost = 0.0
        self.confidence = 0.0
        
    def add_step(self, action: Action, preconditions: List[str] = None, 
                 effects: List[str] = None):
        """Add a step to the plan"""
        step = {
            'action': action,
            'preconditions': preconditions or [],
            'effects': effects or [],
            'step_number': len(self.steps)
        }
        self.steps.append(step)
    
    def get_next_action(self) -> Optional[Action]:
        """Get the next action in the plan"""
        if self.current_step < len(self.steps):
            return self.steps[self.current_step]['action']
        return None
    
    def advance_step(self):
        """Move to next step in plan"""
        if self.current_step < len(self.steps):
            self.current_step += 1
    
    def is_complete(self) -> bool:
        """Check if plan is complete"""
        return self.current_step >= len(self.steps)

class GoalBasedAgent(AgentArchitecture):
    """Goal-based agent with planning capabilities"""
    
    def __init__(self, name: str = "GoalBasedAgent"):
        super().__init__(name)
        self.goals = []
        self.current_plan = None
        self.planning_knowledge = {}  # Domain knowledge for planning
        self.world_model = {}
        
    def add_goal(self, goal: Goal):
        """Add a goal to the agent"""
        self.goals.append(goal)
        self.goals.sort(key=lambda g: g.priority, reverse=True)
    
    def perceive(self, environment: Any) -> List[Observation]:
        """Perceive environment for goal-relevant information"""
        observations = []
        
        # Mock perception focused on goal-relevant aspects
        for goal in self.goals:
            if goal.status == "active":
                # Generate observations relevant to this goal
                obs = Observation(
                    data=f"goal_progress_{goal.description}_{np.random.random():.2f}",
                    timestamp=time.time(),
                    source="goal_monitor",
                    metadata={'goal_id': id(goal)}
                )
                observations.append(obs)
        
        return observations
    
    def reason(self, observations: List[Observation]) -> Dict[str, Any]:
        """Reason about goals and plan actions"""
        
        # Update world model
        self.update_world_model(observations)
        
        # Update goal progress
        self.update_goal_progress(observations)
        
        # Select current goal if none selected
        current_goal = self.select_current_goal()
        
        # Generate or update plan for current goal
        if current_goal and (not self.current_plan or 
                           self.current_plan.goal != current_goal):
            self.current_plan = self.generate_plan(current_goal)
        
        return {
            'current_goal': current_goal,
            'current_plan': self.current_plan,
            'world_state': self.world_model,
            'active_goals': [g for g in self.goals if g.status == "active"]
        }
    
    def act(self, reasoning_output: Dict[str, Any]) -> List[Action]:
        """Execute planned actions"""
        actions = []
        
        if reasoning_output['current_plan']:
            plan = reasoning_output['current_plan']
            next_action = plan.get_next_action()
            
            if next_action:
                actions.append(next_action)
                plan.advance_step()
                
                # Check if plan is complete
                if plan.is_complete():
                    self.evaluate_goal_completion(plan.goal)
                    self.current_plan = None
        
        return actions
    
    def select_current_goal(self) -> Optional[Goal]:
        """Select the most important active goal"""
        active_goals = [g for g in self.goals if g.status == "active"]
        
        if not active_goals:
            return None
        
        # Consider priority, deadline, and progress
        scored_goals = []
        current_time = time.time()
        
        for goal in active_goals:
            score = goal.priority
            
            # Increase urgency based on deadline
            if goal.deadline:
                time_remaining = goal.deadline - current_time
                if time_remaining > 0:
                    urgency = 1.0 / (time_remaining / 3600)  # Inverse of hours remaining
                    score += urgency
                else:
                    score += 10.0  # Very urgent if past deadline
            
            # Consider progress (might want to continue current goal)
            if goal.current_progress > 0:
                score += goal.current_progress * 0.5
            
            scored_goals.append((goal, score))
        
        scored_goals.sort(key=lambda x: x[1], reverse=True)
        return scored_goals[0][0]
    
    def generate_plan(self, goal: Goal) -> Plan:
        """Generate a plan to achieve the goal"""
        plan = Plan(goal)
        
        # Simple planning based on goal type (mock implementation)
        if "research" in goal.description.lower():
            plan.add_step(
                Action("search", {"query": goal.description, "sources": ["academic"]}),
                preconditions=["internet_access"],
                effects=["research_data_available"]
            )
            plan.add_step(
                Action("analyze", {"data_source": "research_data"}),
                preconditions=["research_data_available"],
                effects=["analysis_complete"]
            )
            plan.add_step(
                Action("synthesize", {"analysis_results": True}),
                preconditions=["analysis_complete"],
                effects=["goal_achieved"]
            )
        
        elif "optimize" in goal.description.lower():
            plan.add_step(
                Action("measure_baseline", {"metrics": goal.success_criteria}),
                effects=["baseline_established"]
            )
            plan.add_step(
                Action("identify_bottlenecks", {"baseline_data": True}),
                preconditions=["baseline_established"],
                effects=["bottlenecks_identified"]
            )
            plan.add_step(
                Action("implement_improvements", {"bottlenecks": True}),
                preconditions=["bottlenecks_identified"],
                effects=["improvements_implemented"]
            )
            plan.add_step(
                Action("validate_improvements", {"before_after": True}),
                preconditions=["improvements_implemented"],
                effects=["goal_achieved"]
            )
        
        plan.confidence = 0.8  # Mock confidence score
        return plan
    
    def update_world_model(self, observations: List[Observation]):
        """Update world model based on observations"""
        for obs in observations:
            self.world_model[obs.source] = {
                'data': obs.data,
                'timestamp': obs.timestamp,
                'confidence': obs.confidence
            }
    
    def update_goal_progress(self, observations: List[Observation]):
        """Update goal progress based on observations"""
        for obs in observations:
            if obs.source == "goal_monitor":
                goal_id = obs.metadata.get('goal_id')
                for goal in self.goals:
                    if id(goal) == goal_id:
                        # Extract progress from observation data
                        try:
                            progress = float(obs.data.split("_")[-1])
                            goal.current_progress = min(1.0, progress)
                        except:
                            pass
    
    def evaluate_goal_completion(self, goal: Goal):
        """Evaluate if goal has been completed"""
        if goal.current_progress >= 1.0:
            goal.status = "completed"
        elif goal.deadline and time.time() > goal.deadline:
            goal.status = "failed"
        # Otherwise keep as active

# Example usage
research_goal = Goal(
    description="research latest developments in AI safety",
    priority=0.9,
    deadline=time.time() + 7200,  # 2 hours from now
    success_criteria={"papers_reviewed": 10, "summary_created": True}
)

optimization_goal = Goal(
    description="optimize system performance",
    priority=0.7,
    success_criteria={"performance_increase": 0.2}
)

goal_agent = GoalBasedAgent("ResearchAssistant")
goal_agent.add_goal(research_goal)
goal_agent.add_goal(optimization_goal)
```

## Reasoning architectures

### ReAct (Reasoning and Acting) agents

Advanced agents that interleave reasoning and acting in iterative cycles:

```python
class ReActAgent(AgentArchitecture):
    """ReAct agent that combines reasoning and acting"""
    
    def __init__(self, name: str = "ReActAgent", llm_model=None):
        super().__init__(name)
        self.llm_model = llm_model  # Language model for reasoning
        self.tools = {}  # Available tools
        self.reasoning_trace = []
        self.max_iterations = 10
        self.current_iteration = 0
        
    def add_tool(self, tool_name: str, tool_function: Callable, 
                 description: str, parameters: Dict[str, str] = None):
        """Add a tool that the agent can use"""
        self.tools[tool_name] = {
            'function': tool_function,
            'description': description,
            'parameters': parameters or {}
        }
    
    def perceive(self, environment: Any) -> List[Observation]:
        """Perceive current task or query"""
        # In ReAct, perception often means receiving a new task/query
        if hasattr(environment, 'get_current_task'):
            task = environment.get_current_task()
            obs = Observation(
                data=task,
                timestamp=time.time(),
                source="task_input",
                metadata={'task_type': 'query'}
            )
            return [obs]
        
        return []
    
    def reason(self, observations: List[Observation]) -> Dict[str, Any]:
        """Reason about the current situation and plan next action"""
        
        if not observations:
            return {'action_type': 'wait', 'reasoning': 'No observations to process'}
        
        current_task = observations[-1].data if observations else ""
        
        # Build reasoning prompt
        reasoning_prompt = self._build_reasoning_prompt(current_task)
        
        # Generate reasoning (mock LLM call)
        reasoning_output = self._generate_reasoning(reasoning_prompt)
        
        # Parse reasoning to extract thought and action
        thought, action_plan = self._parse_reasoning(reasoning_output)
        
        self.reasoning_trace.append({
            'iteration': self.current_iteration,
            'thought': thought,
            'action_plan': action_plan,
            'timestamp': time.time()
        })
        
        return {
            'thought': thought,
            'action_plan': action_plan,
            'available_tools': list(self.tools.keys()),
            'iteration': self.current_iteration
        }
    
    def act(self, reasoning_output: Dict[str, Any]) -> List[Action]:
        """Execute the planned action"""
        
        action_plan = reasoning_output['action_plan']
        actions = []
        
        if action_plan['type'] == 'use_tool':
            tool_name = action_plan.get('tool')
            tool_args = action_plan.get('arguments', {})
            
            if tool_name in self.tools:
                action = Action(
                    type="tool_use",
                    parameters={
                        'tool_name': tool_name,
                        'arguments': tool_args
                    },
                    expected_outcome=f"tool_result_{tool_name}"
                )
                actions.append(action)
        
        elif action_plan['type'] == 'final_answer':
            action = Action(
                type="provide_answer",
                parameters={'answer': action_plan.get('content', '')},
                expected_outcome="task_completed"
            )
            actions.append(action)
        
        elif action_plan['type'] == 'continue_reasoning':
            # Continue to next iteration
            self.current_iteration += 1
            action = Action(
                type="continue",
                parameters={'reasoning': reasoning_output['thought']},
                expected_outcome="continue_cycle"
            )
            actions.append(action)
        
        return actions
    
    def _build_reasoning_prompt(self, task: str) -> str:
        """Build prompt for reasoning step"""
        
        prompt = f"""Task: {task}

Available tools:
"""
        for tool_name, tool_info in self.tools.items():
            prompt += f"- {tool_name}: {tool_info['description']}\n"
        
        prompt += f"""
Previous reasoning trace:
"""
        
        for trace in self.reasoning_trace[-3:]:  # Last 3 iterations
            prompt += f"Thought {trace['iteration']}: {trace['thought']}\n"
            prompt += f"Action {trace['iteration']}: {trace['action_plan']}\n"
        
        prompt += """
Think step by step about what to do next. You can:
1. Use a tool to gather information
2. Provide a final answer if you have sufficient information
3. Continue reasoning if you need to think more

Your response should be in the format:
Thought: [your reasoning]
Action: [tool_name with arguments OR final_answer OR continue_reasoning]
"""
        
        return prompt
    
    def _generate_reasoning(self, prompt: str) -> str:
        """Generate reasoning using language model (mock implementation)"""
        
        # Mock reasoning based on current iteration and available tools
        mock_responses = [
            "Thought: I need to search for information about this topic.\nAction: search_tool(query='relevant_information')",
            "Thought: Let me analyze the search results to understand the key points.\nAction: analyze_tool(data='search_results')",
            "Thought: Based on my analysis, I can now provide a comprehensive answer.\nAction: final_answer('Based on the available information...')"
        ]
        
        if self.current_iteration < len(mock_responses):
            return mock_responses[self.current_iteration]
        else:
            return "Thought: I have sufficient information to provide an answer.\nAction: final_answer('Complete response based on reasoning')"
    
    def _parse_reasoning(self, reasoning_output: str) -> Tuple[str, Dict[str, Any]]:
        """Parse reasoning output to extract thought and action"""
        
        lines = reasoning_output.strip().split('\n')
        thought = ""
        action_plan = {}
        
        for line in lines:
            if line.startswith("Thought:"):
                thought = line.replace("Thought:", "").strip()
            elif line.startswith("Action:"):
                action_text = line.replace("Action:", "").strip()
                action_plan = self._parse_action(action_text)
        
        return thought, action_plan
    
    def _parse_action(self, action_text: str) -> Dict[str, Any]:
        """Parse action text to extract structured action plan"""
        
        if "final_answer" in action_text.lower():
            return {
                'type': 'final_answer',
                'content': action_text
            }
        elif "continue" in action_text.lower():
            return {
                'type': 'continue_reasoning'
            }
        else:
            # Try to parse tool usage
            for tool_name in self.tools.keys():
                if tool_name in action_text:
                    return {
                        'type': 'use_tool',
                        'tool': tool_name,
                        'arguments': {'query': action_text}  # Simplified parsing
                    }
        
        return {'type': 'continue_reasoning'}
    
    def reset(self):
        """Reset agent for new task"""
        self.reasoning_trace.clear()
        self.current_iteration = 0
        self.state = AgentState.IDLE

# Example tools for ReAct agent
def search_tool(query: str) -> str:
    """Mock search tool"""
    return f"Search results for '{query}': [Mock search results with relevant information]"

def analyze_tool(data: str) -> str:
    """Mock analysis tool"""
    return f"Analysis of {data}: [Mock analysis with key insights]"

def calculate_tool(expression: str) -> str:
    """Mock calculation tool"""
    try:
        result = eval(expression)  # Dangerous in production - use safe eval
        return f"Calculation result: {result}"
    except:
        return "Error in calculation"

# Create ReAct agent
react_agent = ReActAgent("ResearchAgent")
react_agent.add_tool("search_tool", search_tool, "Search for information on the web")
react_agent.add_tool("analyze_tool", analyze_tool, "Analyze data and extract insights")
react_agent.add_tool("calculate_tool", calculate_tool, "Perform mathematical calculations")
```

### Chain-of-Thought agents

Agents that use structured reasoning chains for problem-solving:

```python
class ChainOfThoughtAgent(AgentArchitecture):
    """Agent that uses chain-of-thought reasoning"""
    
    def __init__(self, name: str = "CoTAgent", llm_model=None):
        super().__init__(name)
        self.llm_model = llm_model
        self.reasoning_chain = []
        self.problem_decomposition = []
        self.solution_steps = []
        
    def perceive(self, environment: Any) -> List[Observation]:
        """Perceive problem or query"""
        if hasattr(environment, 'get_problem'):
            problem = environment.get_problem()
            obs = Observation(
                data=problem,
                timestamp=time.time(),
                source="problem_input",
                metadata={'problem_type': self._classify_problem(problem)}
            )
            return [obs]
        return []
    
    def reason(self, observations: List[Observation]) -> Dict[str, Any]:
        """Apply chain-of-thought reasoning"""
        
        if not observations:
            return {'reasoning_chain': [], 'solution_steps': []}
        
        problem = observations[-1].data
        problem_type = observations[-1].metadata.get('problem_type', 'general')
        
        # Step 1: Problem decomposition
        decomposition = self._decompose_problem(problem, problem_type)
        
        # Step 2: Generate reasoning chain
        reasoning_chain = self._generate_reasoning_chain(decomposition)
        
        # Step 3: Generate solution steps
        solution_steps = self._generate_solution_steps(reasoning_chain)
        
        self.problem_decomposition = decomposition
        self.reasoning_chain = reasoning_chain
        self.solution_steps = solution_steps
        
        return {
            'problem_decomposition': decomposition,
            'reasoning_chain': reasoning_chain,
            'solution_steps': solution_steps,
            'confidence': self._calculate_confidence()
        }
    
    def act(self, reasoning_output: Dict[str, Any]) -> List[Action]:
        """Execute solution steps"""
        actions = []
        
        for step in reasoning_output['solution_steps']:
            action = Action(
                type="reasoning_step",
                parameters={
                    'step_description': step['description'],
                    'step_type': step['type'],
                    'inputs': step.get('inputs', {}),
                    'expected_output': step.get('expected_output', '')
                },
                expected_outcome=f"step_{step['step_number']}_completed"
            )
            actions.append(action)
        
        # Final synthesis action
        synthesis_action = Action(
            type="synthesize_solution",
            parameters={
                'reasoning_chain': reasoning_output['reasoning_chain'],
                'solution_steps': reasoning_output['solution_steps']
            },
            expected_outcome="problem_solved"
        )
        actions.append(synthesis_action)
        
        return actions
    
    def _classify_problem(self, problem: str) -> str:
        """Classify the type of problem"""
        problem_lower = problem.lower()
        
        if any(keyword in problem_lower for keyword in ['calculate', 'compute', 'solve', 'equation']):
            return 'mathematical'
        elif any(keyword in problem_lower for keyword in ['analyze', 'compare', 'evaluate']):
            return 'analytical'
        elif any(keyword in problem_lower for keyword in ['plan', 'strategy', 'approach']):
            return 'planning'
        elif any(keyword in problem_lower for keyword in ['explain', 'describe', 'what', 'how', 'why']):
            return 'explanatory'
        else:
            return 'general'
    
    def _decompose_problem(self, problem: str, problem_type: str) -> List[Dict[str, Any]]:
        """Decompose problem into sub-problems"""
        
        decomposition = []
        
        if problem_type == 'mathematical':
            decomposition = [
                {'component': 'identify_variables', 'description': 'Identify all variables and given information'},
                {'component': 'identify_relationships', 'description': 'Identify mathematical relationships'},
                {'component': 'select_method', 'description': 'Select appropriate solution method'},
                {'component': 'solve_step_by_step', 'description': 'Solve using selected method'},
                {'component': 'verify_solution', 'description': 'Verify the solution makes sense'}
            ]
        
        elif problem_type == 'analytical':
            decomposition = [
                {'component': 'gather_information', 'description': 'Collect relevant information'},
                {'component': 'identify_patterns', 'description': 'Identify patterns and relationships'},
                {'component': 'apply_frameworks', 'description': 'Apply analytical frameworks'},
                {'component': 'draw_conclusions', 'description': 'Draw evidence-based conclusions'},
                {'component': 'consider_alternatives', 'description': 'Consider alternative interpretations'}
            ]
        
        elif problem_type == 'planning':
            decomposition = [
                {'component': 'define_objectives', 'description': 'Clearly define objectives and constraints'},
                {'component': 'identify_resources', 'description': 'Identify available resources'},
                {'component': 'generate_alternatives', 'description': 'Generate alternative approaches'},
                {'component': 'evaluate_options', 'description': 'Evaluate options against criteria'},
                {'component': 'select_plan', 'description': 'Select and detail the best plan'}
            ]
        
        else:  # general or explanatory
            decomposition = [
                {'component': 'understand_question', 'description': 'Understand what is being asked'},
                {'component': 'gather_knowledge', 'description': 'Gather relevant knowledge'},
                {'component': 'organize_information', 'description': 'Organize information logically'},
                {'component': 'construct_response', 'description': 'Construct comprehensive response'},
                {'component': 'review_completeness', 'description': 'Review for completeness and accuracy'}
            ]
        
        return decomposition
    
    def _generate_reasoning_chain(self, decomposition: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate detailed reasoning chain"""
        
        reasoning_chain = []
        
        for i, component in enumerate(decomposition):
            reasoning_step = {
                'step_number': i + 1,
                'component': component['component'],
                'description': component['description'],
                'reasoning': self._generate_step_reasoning(component),
                'dependencies': [j + 1 for j in range(i)],  # Depends on all previous steps
                'confidence': np.random.uniform(0.7, 0.95)  # Mock confidence
            }
            reasoning_chain.append(reasoning_step)
        
        return reasoning_chain
    
    def _generate_step_reasoning(self, component: Dict[str, Any]) -> str:
        """Generate reasoning for a specific step"""
        
        component_type = component['component']
        
        reasoning_templates = {
            'identify_variables': 'I need to carefully read the problem and list all given information and unknowns.',
            'identify_relationships': 'I should look for mathematical relationships between the variables.',
            'select_method': 'Based on the problem structure, I need to choose the most appropriate solution method.',
            'gather_information': 'I need to collect all relevant facts and data related to this topic.',
            'identify_patterns': 'I should analyze the information to find patterns and connections.',
            'define_objectives': 'I must clearly state what I want to achieve and any constraints.',
            'understand_question': 'I need to carefully parse what is being asked and identify key components.',
        }
        
        return reasoning_templates.get(component_type, 
                                    f'I need to work on {component["description"].lower()}.')
    
    def _generate_solution_steps(self, reasoning_chain: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate concrete solution steps from reasoning chain"""
        
        solution_steps = []
        
        for reasoning_step in reasoning_chain:
            solution_step = {
                'step_number': reasoning_step['step_number'],
                'description': reasoning_step['description'],
                'type': reasoning_step['component'],
                'reasoning': reasoning_step['reasoning'],
                'inputs': self._determine_step_inputs(reasoning_step),
                'expected_output': self._determine_expected_output(reasoning_step),
                'verification_criteria': self._determine_verification_criteria(reasoning_step)
            }
            solution_steps.append(solution_step)
        
        return solution_steps
    
    def _determine_step_inputs(self, reasoning_step: Dict[str, Any]) -> Dict[str, Any]:
        """Determine inputs needed for a reasoning step"""
        component = reasoning_step['component']
        
        input_mapping = {
            'identify_variables': {'problem_statement': 'original_problem'},
            'gather_information': {'topic': 'problem_domain', 'sources': 'knowledge_base'},
            'define_objectives': {'requirements': 'problem_requirements'},
            'understand_question': {'question': 'original_question'}
        }
        
        return input_mapping.get(component, {'context': 'previous_steps'})
    
    def _determine_expected_output(self, reasoning_step: Dict[str, Any]) -> str:
        """Determine expected output for a reasoning step"""
        component = reasoning_step['component']
        
        output_mapping = {
            'identify_variables': 'list_of_variables_and_given_values',
            'identify_relationships': 'mathematical_equations_or_relationships',
            'select_method': 'chosen_solution_method_with_justification',
            'gather_information': 'relevant_facts_and_data',
            'define_objectives': 'clear_objectives_and_constraints'
        }
        
        return output_mapping.get(component, 'step_results')
    
    def _determine_verification_criteria(self, reasoning_step: Dict[str, Any]) -> List[str]:
        """Determine how to verify the step was completed correctly"""
        component = reasoning_step['component']
        
        criteria_mapping = {
            'identify_variables': ['all_variables_listed', 'units_specified', 'knowns_vs_unknowns_clear'],
            'solve_step_by_step': ['each_step_mathematically_valid', 'units_consistent', 'intermediate_results_reasonable'],
            'draw_conclusions': ['conclusions_supported_by_evidence', 'alternative_explanations_considered'],
            'select_plan': ['plan_addresses_objectives', 'resource_constraints_considered', 'risks_identified']
        }
        
        return criteria_mapping.get(component, ['output_quality_adequate', 'step_completed'])
    
    def _calculate_confidence(self) -> float:
        """Calculate overall confidence in the reasoning"""
        if not self.reasoning_chain:
            return 0.0
        
        step_confidences = [step.get('confidence', 0.5) for step in self.reasoning_chain]
        
        # Overall confidence is affected by the weakest link
        min_confidence = min(step_confidences)
        avg_confidence = np.mean(step_confidences)
        
        # Weighted average favoring the minimum (weakest link principle)
        overall_confidence = 0.3 * min_confidence + 0.7 * avg_confidence
        
        return overall_confidence

# Example usage
class MockEnvironment:
    def __init__(self):
        self.current_problem = None
    
    def set_problem(self, problem: str):
        self.current_problem = problem
    
    def get_problem(self):
        return self.current_problem

# Create Chain-of-Thought agent
cot_agent = ChainOfThoughtAgent("MathTutor")

# Set up problem
env = MockEnvironment()
env.set_problem("A train travels 120 miles in 2 hours. If it maintains the same speed, how far will it travel in 5 hours?")

# Run agent
observations = cot_agent.perceive(env)
reasoning_output = cot_agent.reason(observations)
actions = cot_agent.act(reasoning_output)

print(f"Problem decomposition: {len(reasoning_output['problem_decomposition'])} components")
print(f"Reasoning chain: {len(reasoning_output['reasoning_chain'])} steps")
print(f"Confidence: {reasoning_output['confidence']:.2f}")
```

## Agent integration with EderSpark

### Freiya-powered research agents

```python
class FreiyaResearchAgent(AgentArchitecture):
    """Research agent powered by EderSpark's Freiya platform"""
    
    def __init__(self, name: str = "FreiyaResearcher", api_key: str = ""):
        super().__init__(name)
        self.api_key = api_key
        self.freiya_client = None  # Would initialize Freiya API client
        self.research_context = {}
        self.knowledge_graph = {}
        
    def perceive(self, environment: Any) -> List[Observation]:
        """Perceive research queries and scientific context"""
        observations = []
        
        if hasattr(environment, 'get_research_query'):
            query = environment.get_research_query()
            obs = Observation(
                data=query,
                timestamp=time.time(),
                source="research_query",
                metadata={
                    'query_type': self._classify_research_query(query),
                    'domain': self._identify_research_domain(query)
                }
            )
            observations.append(obs)
        
        return observations
    
    def reason(self, observations: List[Observation]) -> Dict[str, Any]:
        """Reason about research strategy using scientific knowledge"""
        
        if not observations:
            return {}
        
        query = observations[-1].data
        query_metadata = observations[-1].metadata
        
        # Generate research strategy
        research_strategy = self._generate_research_strategy(query, query_metadata)
        
        # Query Freiya for relevant papers
        relevant_papers = self._query_freiya_papers(query, query_metadata['domain'])
        
        # Build knowledge context
        knowledge_context = self._build_knowledge_context(relevant_papers)
        
        return {
            'research_query': query,
            'research_strategy': research_strategy,
            'relevant_papers': relevant_papers,
            'knowledge_context': knowledge_context,
            'next_actions': self._plan_research_actions(research_strategy)
        }
    
    def act(self, reasoning_output: Dict[str, Any]) -> List[Action]:
        """Execute research actions"""
        actions = []
        
        for planned_action in reasoning_output.get('next_actions', []):
            if planned_action['type'] == 'literature_search':
                action = Action(
                    type="freiya_search",
                    parameters={
                        'query': planned_action['query'],
                        'domain': planned_action.get('domain', ''),
                        'max_papers': planned_action.get('max_papers', 20)
                    },
                    expected_outcome="relevant_papers_retrieved"
                )
                actions.append(action)
                
            elif planned_action['type'] == 'knowledge_synthesis':
                action = Action(
                    type="synthesize_knowledge",
                    parameters={
                        'papers': reasoning_output.get('relevant_papers', []),
                        'synthesis_type': planned_action.get('synthesis_type', 'summary')
                    },
                    expected_outcome="knowledge_synthesized"
                )
                actions.append(action)
                
            elif planned_action['type'] == 'generate_insights':
                action = Action(
                    type="generate_research_insights",
                    parameters={
                        'knowledge_context': reasoning_output.get('knowledge_context', {}),
                        'research_question': reasoning_output['research_query']
                    },
                    expected_outcome="research_insights_generated"
                )
                actions.append(action)
        
        return actions
    
    def _classify_research_query(self, query: str) -> str:
        """Classify the type of research query"""
        query_lower = query.lower()
        
        if any(term in query_lower for term in ['what is', 'define', 'definition']):
            return 'definition'
        elif any(term in query_lower for term in ['how does', 'mechanism', 'process']):
            return 'mechanism'
        elif any(term in query_lower for term in ['compare', 'versus', 'difference']):
            return 'comparison'
        elif any(term in query_lower for term in ['review', 'survey', 'overview']):
            return 'literature_review'
        elif any(term in query_lower for term in ['future', 'trends', 'emerging']):
            return 'trend_analysis'
        else:
            return 'general_inquiry'
    
    def _identify_research_domain(self, query: str) -> str:
        """Identify the research domain of the query"""
        query_lower = query.lower()
        
        domain_keywords = {
            'machine_learning': ['machine learning', 'neural network', 'deep learning', 'ai'],
            'biology': ['protein', 'dna', 'gene', 'cell', 'organism'],
            'physics': ['quantum', 'particle', 'energy', 'wave', 'relativity'],
            'chemistry': ['molecule', 'reaction', 'compound', 'catalyst'],
            'medicine': ['drug', 'disease', 'treatment', 'therapy', 'clinical'],
            'materials': ['material', 'crystal', 'polymer', 'nanotechnology']
        }
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                return domain
        
        return 'general'
    
    def _generate_research_strategy(self, query: str, metadata: Dict) -> Dict[str, Any]:
        """Generate a research strategy based on query analysis"""
        
        query_type = metadata.get('query_type', 'general_inquiry')
        domain = metadata.get('domain', 'general')
        
        strategy = {
            'approach': self._select_research_approach(query_type),
            'search_terms': self._generate_search_terms(query),
            'paper_priorities': self._determine_paper_priorities(query_type),
            'synthesis_method': self._select_synthesis_method(query_type),
            'expected_timeline': self._estimate_timeline(query_type)
        }
        
        return strategy
    
    def _select_research_approach(self, query_type: str) -> str:
        """Select appropriate research approach"""
        approach_mapping = {
            'definition': 'comprehensive_search',
            'mechanism': 'mechanistic_analysis',
            'comparison': 'comparative_analysis',
            'literature_review': 'systematic_review',
            'trend_analysis': 'temporal_analysis'
        }
        
        return approach_mapping.get(query_type, 'general_exploration')
    
    def _generate_search_terms(self, query: str) -> List[str]:
        """Generate effective search terms from the query"""
        # Mock implementation - would use NLP techniques
        base_terms = query.lower().split()
        search_terms = [term for term in base_terms if len(term) > 3]
        
        # Add domain-specific synonyms and variations
        expanded_terms = search_terms.copy()
        
        return list(set(expanded_terms))
    
    def _query_freiya_papers(self, query: str, domain: str) -> List[Dict[str, Any]]:
        """Query Freiya platform for relevant papers (mock implementation)"""
        
        # Mock paper results
        mock_papers = [
            {
                'title': f'Recent Advances in {domain.replace("_", " ").title()}',
                'authors': ['Smith, J.', 'Doe, A.'],
                'journal': 'Nature',
                'year': 2024,
                'doi': '10.1038/nature.2024.001',
                'abstract': f'This paper discusses recent developments in {domain}...',
                'relevance_score': 0.95,
                'citation_count': 150
            },
            {
                'title': f'Computational Methods in {domain.replace("_", " ").title()}',
                'authors': ['Johnson, K.', 'Lee, M.'],
                'journal': 'Science',
                'year': 2023,
                'doi': '10.1126/science.2023.002',
                'abstract': f'We present novel computational approaches for {domain}...',
                'relevance_score': 0.89,
                'citation_count': 89
            }
        ]
        
        return mock_papers
    
    def _build_knowledge_context(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Build knowledge context from retrieved papers"""
        
        context = {
            'key_concepts': [],
            'methodologies': [],
            'recent_findings': [],
            'research_gaps': [],
            'future_directions': []
        }
        
        for paper in papers:
            # Mock knowledge extraction
            context['key_concepts'].extend([
                f"concept_from_{paper['title'][:20]}",
                f"method_from_{paper['authors'][0]}"
            ])
            
            context['recent_findings'].append({
                'finding': f"Key finding from {paper['title']}",
                'source': paper['title'],
                'year': paper['year'],
                'confidence': paper['relevance_score']
            })
        
        return context
    
    def _plan_research_actions(self, strategy: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Plan specific research actions based on strategy"""
        
        actions = []
        
        if strategy['approach'] == 'comprehensive_search':
            actions.extend([
                {
                    'type': 'literature_search',
                    'query': ' '.join(strategy['search_terms']),
                    'max_papers': 50
                },
                {
                    'type': 'knowledge_synthesis',
                    'synthesis_type': 'comprehensive_summary'
                }
            ])
            
        elif strategy['approach'] == 'comparative_analysis':
            actions.extend([
                {
                    'type': 'literature_search',
                    'query': strategy['search_terms'][0],
                    'max_papers': 30
                },
                {
                    'type': 'literature_search',
                    'query': strategy['search_terms'][1] if len(strategy['search_terms']) > 1 else '',
                    'max_papers': 30
                },
                {
                    'type': 'knowledge_synthesis',
                    'synthesis_type': 'comparative_analysis'
                }
            ])
        
        # Always end with insight generation
        actions.append({
            'type': 'generate_insights'
        })
        
        return actions
    
    def _determine_paper_priorities(self, query_type: str) -> Dict[str, float]:
        """Determine how to prioritize papers"""
        
        priority_schemes = {
            'definition': {'recency': 0.3, 'citations': 0.4, 'relevance': 0.3},
            'mechanism': {'recency': 0.4, 'citations': 0.3, 'relevance': 0.3},
            'comparison': {'recency': 0.2, 'citations': 0.5, 'relevance': 0.3},
            'literature_review': {'recency': 0.2, 'citations': 0.6, 'relevance': 0.2},
            'trend_analysis': {'recency': 0.7, 'citations': 0.2, 'relevance': 0.1}
        }
        
        return priority_schemes.get(query_type, {'recency': 0.33, 'citations': 0.33, 'relevance': 0.34})
    
    def _select_synthesis_method(self, query_type: str) -> str:
        """Select method for synthesizing research findings"""
        
        synthesis_methods = {
            'definition': 'concept_consolidation',
            'mechanism': 'process_mapping',
            'comparison': 'comparative_matrix',
            'literature_review': 'systematic_synthesis',
            'trend_analysis': 'temporal_synthesis'
        }
        
        return synthesis_methods.get(query_type, 'general_synthesis')
    
    def _estimate_timeline(self, query_type: str) -> Dict[str, int]:
        """Estimate timeline for research completion"""
        
        timeline_estimates = {
            'definition': {'search_minutes': 5, 'synthesis_minutes': 10, 'total_minutes': 15},
            'mechanism': {'search_minutes': 10, 'synthesis_minutes': 20, 'total_minutes': 30},
            'comparison': {'search_minutes': 15, 'synthesis_minutes': 25, 'total_minutes': 40},
            'literature_review': {'search_minutes': 30, 'synthesis_minutes': 45, 'total_minutes': 75},
            'trend_analysis': {'search_minutes': 20, 'synthesis_minutes': 30, 'total_minutes': 50}
        }
        
        return timeline_estimates.get(query_type, {'search_minutes': 10, 'synthesis_minutes': 15, 'total_minutes': 25})

# Example usage
def main():
    """Example of using different agent architectures"""
    
    # Initialize taxonomy
    taxonomy = ArchitectureTaxonomy()
    
    # Get architecture recommendations
    requirements = {
        'complexity': ComplexityLevel.ADVANCED,
        'capabilities': ['reasoning_action_cycles', 'tool_usage'],
        'use_cases': ['research_assistance']
    }
    
    recommendations = taxonomy.recommend_architecture(requirements)
    print("Architecture Recommendations:")
    for name, score in recommendations:
        print(f"- {name}: {score:.2f}")
    
    # Create and test ReAct agent
    react_agent = ReActAgent("TestReActAgent")
    react_agent.add_tool("search", search_tool, "Search for information")
    
    # Create mock environment
    class MockTaskEnvironment:
        def __init__(self):
            self.task = "Find information about recent advances in quantum computing"
        
        def get_current_task(self):
            return self.task
    
    env = MockTaskEnvironment()
    
    # Run ReAct agent
    observations = react_agent.perceive(env)
    reasoning = react_agent.reason(observations)
    actions = react_agent.act(reasoning)
    
    print(f"\nReAct Agent Results:")
    print(f"Thought: {reasoning.get('thought', 'N/A')}")
    print(f"Actions planned: {len(actions)}")
    
    # Create and test Freiya research agent
    freiya_agent = FreiyaResearchAgent("FreiyaResearcher", "mock_api_key")
    
    class MockResearchEnvironment:
        def __init__(self):
            self.query = "What are the latest developments in protein folding prediction using AI?"
        
        def get_research_query(self):
            return self.query
    
    research_env = MockResearchEnvironment()
    
    # Run Freiya agent
    research_observations = freiya_agent.perceive(research_env)
    research_reasoning = freiya_agent.reason(research_observations)
    research_actions = freiya_agent.act(research_reasoning)
    
    print(f"\nFreiya Research Agent Results:")
    print(f"Research strategy: {research_reasoning.get('research_strategy', {}).get('approach', 'N/A')}")
    print(f"Relevant papers found: {len(research_reasoning.get('relevant_papers', []))}")
    print(f"Research actions: {len(research_actions)}")
    
    return taxonomy, react_agent, freiya_agent

if __name__ == "__main__":
    taxonomy, react_agent, freiya_agent = main()
```

This comprehensive guide to agent architectures provides the foundation for building sophisticated AI agents capable of autonomous reasoning, planning, and action. From simple reactive systems to advanced reasoning agents integrated with scientific knowledge platforms like EderSpark's Freiya, these architectures enable the development of intelligent systems that can tackle complex real-world problems.

The modular design patterns and integration examples shown here provide practical frameworks for implementing agent-based solutions across diverse domains, from basic automation to advanced research assistance and scientific discovery.