---
title: Pre-training large language models
description: Comprehensive guide to pre-training LLMs from scratch, covering data preparation, training objectives, infrastructure requirements, and optimization strategies
---

# Pre-training large language models

Pre-training is the foundational phase of large language model development where models learn general language understanding and generation capabilities from vast amounts of text data. This process transforms a randomly initialized neural network into a sophisticated language model that can understand context, generate coherent text, and perform various downstream tasks. The pre-training phase is computationally intensive but crucial for developing the broad linguistic knowledge that makes modern LLMs so capable.

## Understanding pre-training objectives

### Next token prediction

The primary objective in LLM pre-training is causal language modeling, where the model learns to predict the next token in a sequence given all previous tokens. This seemingly simple task requires the model to learn complex patterns in language, including syntax, semantics, factual knowledge, and reasoning capabilities.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
import math
from typing import List, Dict, Optional, Tuple

class CausalLanguageModelingLoss(nn.Module):
    """Causal language modeling loss for pre-training"""
    
    def __init__(self, ignore_index=-100):
        super().__init__()
        self.ignore_index = ignore_index
        
    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Compute cross-entropy loss for next token prediction
        
        Args:
            logits: Model predictions [batch_size, seq_len, vocab_size]
            targets: Target token ids [batch_size, seq_len]
        """
        # Shift targets: predict token i+1 given tokens 0..i
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = targets[..., 1:].contiguous()
        
        # Flatten for cross-entropy computation
        batch_size, seq_len, vocab_size = shift_logits.shape
        shift_logits = shift_logits.view(-1, vocab_size)
        shift_labels = shift_labels.view(-1)
        
        # Compute cross-entropy loss
        loss = F.cross_entropy(
            shift_logits, 
            shift_labels, 
            ignore_index=self.ignore_index,
            reduction='mean'
        )
        
        return loss

class PretrainingDataset(Dataset):
    """Dataset for pre-training with tokenized text sequences"""
    
    def __init__(self, tokenized_data: List[List[int]], max_length: int = 2048):
        self.data = tokenized_data
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sequence = self.data[idx]
        
        # Truncate or pad sequence
        if len(sequence) > self.max_length:
            sequence = sequence[:self.max_length]
        else:
            sequence = sequence + [0] * (self.max_length - len(sequence))  # 0 = PAD token
        
        # Convert to tensor
        tokens = torch.tensor(sequence, dtype=torch.long)
        
        # For causal LM, input and target are the same sequence
        return {
            'input_ids': tokens,
            'labels': tokens.clone()
        }

# Example training loop for pre-training
def pretrain_language_model(model, train_dataloader, optimizer, scheduler, 
                           device, num_epochs=1, gradient_accumulation_steps=8,
                           save_steps=10000, eval_steps=5000):
    """
    Pre-training loop for language models
    """
    model.train()
    criterion = CausalLanguageModelingLoss()
    
    global_step = 0
    accumulated_loss = 0.0
    
    for epoch in range(num_epochs):
        for batch_idx, batch in enumerate(train_dataloader):
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            
            # Forward pass
            outputs = model(input_ids)
            logits = outputs.logits if hasattr(outputs, 'logits') else outputs
            
            # Compute loss
            loss = criterion(logits, labels)
            loss = loss / gradient_accumulation_steps  # Scale loss for gradient accumulation
            
            # Backward pass
            loss.backward()
            accumulated_loss += loss.item()
            
            # Gradient accumulation
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                # Clip gradients
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # Update parameters
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                
                global_step += 1
                
                # Logging
                if global_step % 100 == 0:
                    avg_loss = accumulated_loss / 100
                    perplexity = torch.exp(torch.tensor(avg_loss))
                    
                    print(f"Step {global_step}: Loss = {avg_loss:.4f}, Perplexity = {perplexity:.2f}")
                    accumulated_loss = 0.0
                
                # Evaluation
                if global_step % eval_steps == 0:
                    eval_loss = evaluate_model(model, eval_dataloader, criterion, device)
                    print(f"Evaluation at step {global_step}: Loss = {eval_loss:.4f}")
                    model.train()  # Switch back to training mode
                
                # Save checkpoint
                if global_step % save_steps == 0:
                    checkpoint = {
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'global_step': global_step,
                        'loss': avg_loss
                    }
                    torch.save(checkpoint, f'checkpoint_step_{global_step}.pt')
                    print(f"Saved checkpoint at step {global_step}")

def evaluate_model(model, eval_dataloader, criterion, device, max_eval_batches=100):
    """Evaluate model on validation set"""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(eval_dataloader):
            if batch_idx >= max_eval_batches:
                break
                
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids)
            logits = outputs.logits if hasattr(outputs, 'logits') else outputs
            
            loss = criterion(logits, labels)
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches if num_batches > 0 else 0.0
```

### Alternative pre-training objectives

While next-token prediction is the dominant approach, researchers have explored various alternative objectives:

```python
class MaskedLanguageModelingLoss(nn.Module):
    """Masked language modeling loss (BERT-style)"""
    
    def __init__(self, vocab_size: int, ignore_index=-100):
        super().__init__()
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
        
    def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        Compute MLM loss only on masked tokens
        
        Args:
            logits: Model predictions [batch_size, seq_len, vocab_size]
            targets: Target token ids [batch_size, seq_len]
            mask: Boolean mask indicating which tokens to predict [batch_size, seq_len]
        """
        # Only compute loss on masked tokens
        masked_logits = logits[mask]
        masked_targets = targets[mask]
        
        loss = F.cross_entropy(
            masked_logits,
            masked_targets,
            ignore_index=self.ignore_index,
            reduction='mean'
        )
        
        return loss

class PrefixLanguageModelingLoss(nn.Module):
    """Prefix LM loss (T5-style)"""
    
    def __init__(self, ignore_index=-100):
        super().__init__()
        self.ignore_index = ignore_index
        
    def forward(self, logits: torch.Tensor, targets: torch.Tensor, 
               prefix_length: int) -> torch.Tensor:
        """
        Compute loss only on target portion (after prefix)
        
        Args:
            logits: Model predictions [batch_size, seq_len, vocab_size]
            targets: Target token ids [batch_size, seq_len]  
            prefix_length: Length of prefix (input) portion
        """
        # Only predict tokens after the prefix
        target_logits = logits[:, prefix_length-1:-1, :]  # Shift by 1 for prediction
        target_labels = targets[:, prefix_length:]
        
        batch_size, seq_len, vocab_size = target_logits.shape
        target_logits = target_logits.contiguous().view(-1, vocab_size)
        target_labels = target_labels.contiguous().view(-1)
        
        loss = F.cross_entropy(
            target_logits,
            target_labels,
            ignore_index=self.ignore_index,
            reduction='mean'
        )
        
        return loss

# Contrastive learning objectives
class ContrastiveLearningObjective(nn.Module):
    """Contrastive learning for representation learning"""
    
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
        
    def forward(self, embeddings: torch.Tensor, positive_pairs: torch.Tensor) -> torch.Tensor:
        """
        Compute contrastive loss for learning better representations
        
        Args:
            embeddings: Sentence/document embeddings [batch_size, hidden_dim]
            positive_pairs: Indices of positive pairs [num_pairs, 2]
        """
        # Normalize embeddings
        embeddings = F.normalize(embeddings, dim=-1)
        
        # Compute similarity matrix
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature
        
        # Create labels for positive pairs
        batch_size = embeddings.size(0)
        labels = torch.zeros(batch_size, batch_size, device=embeddings.device)
        
        for pair in positive_pairs:
            i, j = pair
            labels[i, j] = 1
            labels[j, i] = 1
        
        # Mask out self-similarities
        mask = torch.eye(batch_size, device=embeddings.device).bool()
        similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))
        
        # Compute contrastive loss
        exp_sim = torch.exp(similarity_matrix)
        
        loss = 0.0
        num_positives = 0
        
        for i in range(batch_size):
            positive_mask = labels[i] == 1
            if positive_mask.sum() > 0:
                positive_similarities = exp_sim[i][positive_mask]
                all_similarities = exp_sim[i].sum()
                
                # Negative log likelihood for positive examples
                loss -= torch.log(positive_similarities / all_similarities).sum()
                num_positives += positive_mask.sum()
        
        return loss / max(num_positives, 1)
```

## Data preparation and preprocessing

### Dataset construction

Building high-quality pre-training datasets is crucial for model performance. Modern LLMs are trained on diverse text sources including web pages, books, academic papers, code repositories, and more.

```python
import json
import gzip
import random
from pathlib import Path
import multiprocessing as mp
from collections import defaultdict, Counter
import re
from typing import Iterator, List, Dict, Any
import hashlib

class PretrainingDataProcessor:
    """Process and prepare data for LLM pre-training"""
    
    def __init__(self, tokenizer, min_length: int = 10, max_length: int = 512000, 
                 quality_threshold: float = 0.7):
        self.tokenizer = tokenizer
        self.min_length = min_length
        self.max_length = max_length
        self.quality_threshold = quality_threshold
        
        # Language detection patterns (simplified)
        self.language_patterns = {
            'english': re.compile(r'\b(the|and|or|but|in|on|at|to|for|of|with|by)\b', re.IGNORECASE),
            'code': re.compile(r'(def|class|import|function|var|let|const|if|else|for|while)\s'),
            'math': re.compile(r'[\\\$][\\\(].*?[\\\)][\\\$]|[\\\$].*?[\\\$]')  # LaTeX math
        }
    
    def process_dataset(self, input_files: List[str], output_dir: str, 
                       num_workers: int = 8, chunk_size: int = 1000) -> Dict[str, Any]:
        """
        Process raw text files into pre-training format
        
        Args:
            input_files: List of input file paths
            output_dir: Directory to save processed data
            num_workers: Number of worker processes
            chunk_size: Documents per chunk for parallel processing
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Statistics tracking
        stats = {
            'total_documents': 0,
            'filtered_documents': 0,
            'total_tokens': 0,
            'language_distribution': defaultdict(int),
            'length_distribution': defaultdict(int),
            'quality_scores': []
        }
        
        processed_chunks = []
        
        for input_file in input_files:
            print(f"Processing {input_file}...")
            
            # Read and chunk documents
            documents = self._read_documents(input_file)
            document_chunks = self._chunk_documents(documents, chunk_size)
            
            # Process chunks in parallel
            with mp.Pool(num_workers) as pool:
                chunk_results = pool.map(self._process_chunk, document_chunks)
            
            # Aggregate results
            for chunk_result in chunk_results:
                if chunk_result:
                    processed_chunks.append(chunk_result)
                    self._update_stats(stats, chunk_result['stats'])
        
        # Save processed data
        output_files = self._save_processed_data(processed_chunks, output_path)
        
        # Save statistics
        stats_file = output_path / 'preprocessing_stats.json'
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"Processing complete. Saved {len(output_files)} files to {output_dir}")
        print(f"Total documents: {stats['total_documents']:,}")
        print(f"Filtered documents: {stats['filtered_documents']:,}")
        print(f"Total tokens: {stats['total_tokens']:,}")
        
        return {
            'output_files': output_files,
            'statistics': stats
        }
    
    def _read_documents(self, file_path: str) -> Iterator[Dict[str, Any]]:
        """Read documents from various formats"""
        if file_path.endswith('.jsonl') or file_path.endswith('.jsonl.gz'):
            opener = gzip.open if file_path.endswith('.gz') else open
            mode = 'rt' if file_path.endswith('.gz') else 'r'
            
            with opener(file_path, mode, encoding='utf-8') as f:
                for line in f:
                    try:
                        doc = json.loads(line.strip())
                        yield doc
                    except json.JSONDecodeError:
                        continue
        
        elif file_path.endswith('.txt'):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                yield {'text': content, 'source': file_path}
        
        else:
            raise ValueError(f"Unsupported file format: {file_path}")
    
    def _chunk_documents(self, documents: Iterator[Dict[str, Any]], 
                        chunk_size: int) -> List[List[Dict[str, Any]]]:
        """Split documents into chunks for parallel processing"""
        chunks = []
        current_chunk = []
        
        for doc in documents:
            current_chunk.append(doc)
            if len(current_chunk) >= chunk_size:
                chunks.append(current_chunk)
                current_chunk = []
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _process_chunk(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Process a chunk of documents"""
        processed_docs = []
        chunk_stats = {
            'total_documents': 0,
            'filtered_documents': 0,
            'total_tokens': 0,
            'language_distribution': defaultdict(int),
            'length_distribution': defaultdict(int),
            'quality_scores': []
        }
        
        for doc in documents:
            chunk_stats['total_documents'] += 1
            
            # Extract text
            text = doc.get('text', '')
            if not text:
                continue
            
            # Quality filtering
            quality_score = self._assess_quality(text)
            chunk_stats['quality_scores'].append(quality_score)
            
            if quality_score < self.quality_threshold:
                chunk_stats['filtered_documents'] += 1
                continue
            
            # Language detection
            language = self._detect_language(text)
            chunk_stats['language_distribution'][language] += 1
            
            # Tokenization
            tokens = self.tokenizer.encode(text)
            token_count = len(tokens)
            
            # Length filtering
            if token_count < self.min_length or token_count > self.max_length:
                chunk_stats['filtered_documents'] += 1
                continue
            
            chunk_stats['total_tokens'] += token_count
            
            # Length distribution tracking
            length_bucket = self._get_length_bucket(token_count)
            chunk_stats['length_distribution'][length_bucket] += 1
            
            # Store processed document
            processed_doc = {
                'text': text,
                'tokens': tokens,
                'token_count': token_count,
                'language': language,
                'quality_score': quality_score,
                'source': doc.get('source', 'unknown'),
                'id': hashlib.md5(text.encode()).hexdigest()
            }
            processed_docs.append(processed_doc)
        
        return {
            'documents': processed_docs,
            'stats': chunk_stats
        }
    
    def _assess_quality(self, text: str) -> float:
        """Assess text quality using heuristics"""
        if not text:
            return 0.0
        
        score = 1.0
        
        # Length penalties
        if len(text) < 100:
            score *= 0.5
        elif len(text) < 50:
            score *= 0.1
        
        # Character diversity
        unique_chars = len(set(text.lower()))
        char_diversity = unique_chars / len(text)
        if char_diversity < 0.05:  # Very repetitive
            score *= 0.3
        
        # Word-like token ratio
        words = text.split()
        if words:
            avg_word_length = sum(len(word) for word in words) / len(words)
            if avg_word_length < 2 or avg_word_length > 20:
                score *= 0.5
        
        # Special character ratio
        special_chars = sum(1 for char in text if not char.isalnum() and not char.isspace())
        special_ratio = special_chars / len(text)
        if special_ratio > 0.3:  # Too many special characters
            score *= 0.7
        
        # Line repetition detection
        lines = text.split('\n')
        unique_lines = len(set(lines))
        if len(lines) > 10 and unique_lines / len(lines) < 0.7:
            score *= 0.5
        
        return min(score, 1.0)
    
    def _detect_language(self, text: str) -> str:
        """Simple language detection"""
        text_lower = text.lower()
        
        # Check for code patterns
        if self.language_patterns['code'].search(text):
            return 'code'
        
        # Check for math patterns
        if self.language_patterns['math'].search(text):
            return 'math'
        
        # Check for English patterns
        english_matches = len(self.language_patterns['english'].findall(text_lower))
        if english_matches > len(text.split()) * 0.1:
            return 'english'
        
        return 'other'
    
    def _get_length_bucket(self, token_count: int) -> str:
        """Categorize documents by length"""
        if token_count < 256:
            return 'short'
        elif token_count < 1024:
            return 'medium'
        elif token_count < 4096:
            return 'long'
        else:
            return 'very_long'
    
    def _update_stats(self, global_stats: Dict, chunk_stats: Dict):
        """Update global statistics with chunk statistics"""
        global_stats['total_documents'] += chunk_stats['total_documents']
        global_stats['filtered_documents'] += chunk_stats['filtered_documents']
        global_stats['total_tokens'] += chunk_stats['total_tokens']
        
        for lang, count in chunk_stats['language_distribution'].items():
            global_stats['language_distribution'][lang] += count
        
        for length, count in chunk_stats['length_distribution'].items():
            global_stats['length_distribution'][length] += count
        
        global_stats['quality_scores'].extend(chunk_stats['quality_scores'])
    
    def _save_processed_data(self, processed_chunks: List[Dict], 
                           output_path: Path) -> List[str]:
        """Save processed data to files"""
        output_files = []
        
        for i, chunk in enumerate(processed_chunks):
            output_file = output_path / f'processed_data_{i:06d}.jsonl.gz'
            
            with gzip.open(output_file, 'wt', encoding='utf-8') as f:
                for doc in chunk['documents']:
                    json.dump(doc, f)
                    f.write('\n')
            
            output_files.append(str(output_file))
        
        return output_files

# Data mixing and sampling strategies
class DataMixer:
    """Mix different data sources according to specified ratios"""
    
    def __init__(self, mixing_ratios: Dict[str, float], temperature: float = 1.0):
        """
        Args:
            mixing_ratios: Dictionary mapping source names to mixing ratios
            temperature: Temperature for sampling (higher = more uniform)
        """
        self.mixing_ratios = mixing_ratios
        self.temperature = temperature
        
        # Normalize ratios
        total = sum(mixing_ratios.values())
        self.mixing_ratios = {k: v/total for k, v in mixing_ratios.items()}
    
    def create_mixed_dataset(self, source_datasets: Dict[str, List[Dict]], 
                           total_samples: int) -> List[Dict]:
        """Create mixed dataset according to specified ratios"""
        mixed_data = []
        
        # Apply temperature to ratios
        ratios = {k: v**(1/self.temperature) for k, v in self.mixing_ratios.items()}
        total = sum(ratios.values())
        ratios = {k: v/total for k, v in ratios.items()}
        
        # Calculate samples per source
        samples_per_source = {}
        for source, ratio in ratios.items():
            samples_per_source[source] = int(total_samples * ratio)
        
        # Sample from each source
        for source, num_samples in samples_per_source.items():
            if source in source_datasets:
                dataset = source_datasets[source]
                if len(dataset) >= num_samples:
                    sampled = random.sample(dataset, num_samples)
                else:
                    # Repeat sampling if not enough data
                    sampled = random.choices(dataset, k=num_samples)
                
                # Add source information
                for item in sampled:
                    item['data_source'] = source
                
                mixed_data.extend(sampled)
        
        # Shuffle final dataset
        random.shuffle(mixed_data)
        
        return mixed_data

# Example usage
class SimpleTokenizer:
    """Simple tokenizer for demonstration"""
    def __init__(self, vocab_size=50000):
        self.vocab_size = vocab_size
        # In practice, would use actual tokenizer like SentencePiece or tiktoken
        
    def encode(self, text: str) -> List[int]:
        # Simplified tokenization - split by whitespace and assign IDs
        tokens = text.split()
        return [hash(token) % self.vocab_size for token in tokens]

# Example processing pipeline
def run_preprocessing_example():
    """Example of pre-training data preprocessing"""
    
    # Initialize tokenizer and processor
    tokenizer = SimpleTokenizer()
    processor = PretrainingDataProcessor(
        tokenizer=tokenizer,
        min_length=10,
        max_length=2048,
        quality_threshold=0.5
    )
    
    # Example input files (would be real data files)
    input_files = ['example_data.jsonl', 'web_crawl.jsonl.gz']
    
    # Process data
    try:
        result = processor.process_dataset(
            input_files=input_files,
            output_dir='processed_pretraining_data',
            num_workers=4,
            chunk_size=1000
        )
        
        print("Processing completed successfully!")
        print(f"Output files: {len(result['output_files'])}")
        
        # Display statistics
        stats = result['statistics']
        print(f"\nDataset Statistics:")
        print(f"Total documents processed: {stats['total_documents']:,}")
        print(f"Documents after filtering: {stats['total_documents'] - stats['filtered_documents']:,}")
        print(f"Total tokens: {stats['total_tokens']:,}")
        print(f"Average quality score: {np.mean(stats['quality_scores']):.3f}")
        
        print(f"\nLanguage distribution:")
        for lang, count in stats['language_distribution'].items():
            percentage = count / (stats['total_documents'] - stats['filtered_documents']) * 100
            print(f"  {lang}: {count:,} ({percentage:.1f}%)")
        
        print(f"\nLength distribution:")
        for length_cat, count in stats['length_distribution'].items():
            percentage = count / (stats['total_documents'] - stats['filtered_documents']) * 100
            print(f"  {length_cat}: {count:,} ({percentage:.1f}%)")
            
    except Exception as e:
        print(f"Error during processing: {e}")
        print("This is expected in the demo as the input files don't exist")

# Run example
print("Pre-training Data Processing Example:")
print("=" * 50)
run_preprocessing_example()
```

### Curriculum learning and data ordering

The order in which data is presented during training can significantly impact model performance. Curriculum learning strategies help models learn more efficiently by gradually increasing task difficulty.

```python
class CurriculumLearningScheduler:
    """Implement curriculum learning for pre-training"""
    
    def __init__(self, difficulty_metrics: List[str] = ['length', 'complexity', 'quality']):
        self.difficulty_metrics = difficulty_metrics
        self.current_step = 0
        
    def calculate_difficulty(self, document: Dict[str, Any]) -> float:
        """Calculate difficulty score for a document"""
        scores = []
        
        # Length-based difficulty
        if 'length' in self.difficulty_metrics:
            token_count = document.get('token_count', 0)
            length_score = min(token_count / 1000, 1.0)  # Normalize to [0, 1]
            scores.append(length_score)
        
        # Complexity-based difficulty (vocabulary diversity)
        if 'complexity' in self.difficulty_metrics:
            text = document.get('text', '')
            if text:
                words = text.split()
                unique_words = len(set(words))
                complexity_score = unique_words / len(words) if words else 0
                scores.append(complexity_score)
            else:
                scores.append(0)
        
        # Quality-based difficulty (inverse relationship - higher quality = easier)
        if 'quality' in self.difficulty_metrics:
            quality_score = 1.0 - document.get('quality_score', 0.5)  # Invert quality
            scores.append(quality_score)
        
        return np.mean(scores)
    
    def create_curriculum_batches(self, documents: List[Dict], batch_size: int, 
                                num_steps: int, difficulty_increase_rate: float = 0.1) -> Iterator[List[Dict]]:
        """Create batches following curriculum learning schedule"""
        
        # Calculate difficulty for all documents
        for doc in documents:
            doc['difficulty'] = self.calculate_difficulty(doc)
        
        # Sort documents by difficulty
        sorted_docs = sorted(documents, key=lambda x: x['difficulty'])
        
        step = 0
        while step < num_steps:
            # Calculate current difficulty threshold
            progress = step / num_steps
            max_difficulty = min(progress + difficulty_increase_rate, 1.0)
            
            # Filter documents by current difficulty threshold
            available_docs = [
                doc for doc in sorted_docs 
                if doc['difficulty'] <= max_difficulty
            ]
            
            if not available_docs:
                available_docs = sorted_docs[:100]  # Fallback to easiest documents
            
            # Sample batch
            batch = random.sample(available_docs, min(batch_size, len(available_docs)))
            
            yield batch
            step += 1
            self.current_step += 1

class AdaptiveBatchSampler:
    """Adaptive sampling based on training dynamics"""
    
    def __init__(self, initial_batch_size: int = 32, max_batch_size: int = 128):
        self.initial_batch_size = initial_batch_size
        self.max_batch_size = max_batch_size
        self.current_batch_size = initial_batch_size
        self.loss_history = []
        self.gradient_norm_history = []
        
    def update_batch_size(self, current_loss: float, gradient_norm: float):
        """Update batch size based on training dynamics"""
        self.loss_history.append(current_loss)
        self.gradient_norm_history.append(gradient_norm)
        
        # Keep only recent history
        if len(self.loss_history) > 100:
            self.loss_history = self.loss_history[-100:]
            self.gradient_norm_history = self.gradient_norm_history[-100:]
        
        if len(self.loss_history) < 10:
            return  # Not enough history
        
        # Calculate recent trends
        recent_loss_trend = np.mean(self.loss_history[-10:]) - np.mean(self.loss_history[-20:-10])
        recent_grad_stability = np.std(self.gradient_norm_history[-10:])
        
        # Increase batch size if training is stable and loss is decreasing
        if recent_loss_trend < -0.01 and recent_grad_stability < 1.0:
            self.current_batch_size = min(
                int(self.current_batch_size * 1.1), 
                self.max_batch_size
            )
        
        # Decrease batch size if training is unstable
        elif recent_grad_stability > 5.0 or recent_loss_trend > 0.05:
            self.current_batch_size = max(
                int(self.current_batch_size * 0.9),
                self.initial_batch_size
            )
    
    def get_current_batch_size(self) -> int:
        return self.current_batch_size
```

## Model architecture and scaling

### Transformer architecture for pre-training

Modern LLMs are based on transformer architectures optimized for large-scale pre-training:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import LayerNorm
import math

class PretrainingTransformer(nn.Module):
    """Transformer model optimized for pre-training"""
    
    def __init__(self, vocab_size: int, max_seq_length: int = 2048, 
                 d_model: int = 768, n_heads: int = 12, n_layers: int = 12,
                 d_ff: int = 3072, dropout: float = 0.1, layer_norm_eps: float = 1e-5):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.max_seq_length = max_seq_length
        self.d_model = d_model
        
        # Token and position embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_length, d_model)
        
        # Transformer layers
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout, layer_norm_eps)
            for _ in range(n_layers)
        ])
        
        # Final layer norm
        self.final_layer_norm = LayerNorm(d_model, eps=layer_norm_eps)
        
        # Output projection
        self.output_projection = nn.Linear(d_model, vocab_size, bias=False)
        
        # Tie embeddings (common practice)
        self.output_projection.weight = self.token_embedding.weight
        
        # Initialize parameters
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """Initialize weights following GPT-style initialization"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
    
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        """
        Forward pass through the transformer
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            attention_mask: Attention mask [batch_size, seq_len]
        """
        batch_size, seq_len = input_ids.shape
        
        # Create position IDs
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding(position_ids)
        hidden_states = token_embeds + position_embeds
        
        # Create causal attention mask
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        
        # Convert to 4D attention mask for multi-head attention
        attention_mask = self._create_causal_mask(batch_size, seq_len, attention_mask, input_ids.device)
        
        # Forward through transformer layers
        for layer in self.layers:
            hidden_states = layer(hidden_states, attention_mask)
        
        # Final layer norm
        hidden_states = self.final_layer_norm(hidden_states)
        
        # Output projection
        logits = self.output_projection(hidden_states)
        
        return logits
    
    def _create_causal_mask(self, batch_size: int, seq_len: int, 
                          attention_mask: torch.Tensor, device: torch.device):
        """Create causal attention mask"""
        # Create causal mask (lower triangular)
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]
        
        # Combine with padding mask
        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]
        combined_mask = attention_mask * causal_mask
        
        # Convert to additive mask (0 for attend, -inf for don't attend)
        combined_mask = (1.0 - combined_mask) * -1e9
        
        return combined_mask

class TransformerBlock(nn.Module):
    """Single transformer block with multi-head attention and feed-forward"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, 
                 dropout: float = 0.1, layer_norm_eps: float = 1e-5):
        super().__init__()
        
        # Multi-head attention
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.attention_dropout = nn.Dropout(dropout)
        self.attention_norm = LayerNorm(d_model, eps=layer_norm_eps)
        
        # Feed-forward network
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.ff_dropout = nn.Dropout(dropout)
        self.ff_norm = LayerNorm(d_model, eps=layer_norm_eps)
    
    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor):
        """Forward pass through transformer block"""
        
        # Multi-head attention with residual connection
        attention_output = self.attention(hidden_states, attention_mask)
        attention_output = self.attention_dropout(attention_output)
        hidden_states = self.attention_norm(hidden_states + attention_output)
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(hidden_states)
        ff_output = self.ff_dropout(ff_output)
        hidden_states = self.ff_norm(hidden_states + ff_output)
        
        return hidden_states

class MultiHeadAttention(nn.Module):
    """Multi-head attention mechanism"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        # Linear projections for Q, K, V
        self.query_projection = nn.Linear(d_model, d_model, bias=False)
        self.key_projection = nn.Linear(d_model, d_model, bias=False)
        self.value_projection = nn.Linear(d_model, d_model, bias=False)
        
        # Output projection
        self.output_projection = nn.Linear(d_model, d_model, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor):
        batch_size, seq_len, d_model = hidden_states.shape
        
        # Linear projections
        queries = self.query_projection(hidden_states)
        keys = self.key_projection(hidden_states)
        values = self.value_projection(hidden_states)
        
        # Reshape for multi-head attention
        queries = queries.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)
        keys = keys.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)
        values = values.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)
        
        # Scaled dot-product attention
        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.d_head)
        
        # Apply attention mask
        if attention_mask is not None:
            attention_scores += attention_mask
        
        # Softmax and dropout
        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        
        # Apply attention to values
        context = torch.matmul(attention_probs, values)
        
        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        
        # Output projection
        output = self.output_projection(context)
        
        return output

class FeedForward(nn.Module):
    """Position-wise feed-forward network"""
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.linear_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, hidden_states: torch.Tensor):
        hidden_states = F.gelu(self.linear_1(hidden_states))
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.linear_2(hidden_states)
        return hidden_states

# Model scaling utilities
class ModelScaler:
    """Utilities for scaling transformer models"""
    
    @staticmethod
    def calculate_model_size(vocab_size: int, max_seq_length: int, d_model: int, 
                           n_heads: int, n_layers: int, d_ff: int) -> Dict[str, int]:
        """Calculate model parameter count and memory requirements"""
        
        # Embedding parameters
        token_embed_params = vocab_size * d_model
        position_embed_params = max_seq_length * d_model
        
        # Attention parameters per layer
        attention_params_per_layer = (
            d_model * d_model * 3 +  # Q, K, V projections
            d_model * d_model        # Output projection
        )
        
        # Feed-forward parameters per layer
        ff_params_per_layer = (
            d_model * d_ff +  # First linear layer
            d_ff * d_model    # Second linear layer
        )
        
        # Layer norm parameters per layer
        norm_params_per_layer = d_model * 2 * 2  # 2 layer norms, each with weight and bias
        
        # Total parameters
        total_params = (
            token_embed_params +
            position_embed_params +
            n_layers * (attention_params_per_layer + ff_params_per_layer + norm_params_per_layer) +
            d_model * 2  # Final layer norm
        )
        
        # Memory estimates (in MB, assuming float32)
        param_memory = total_params * 4 / (1024 * 1024)
        
        # Activation memory (rough estimate for batch_size=1, seq_len=2048)
        batch_size, seq_len = 1, 2048
        activation_memory = (
            batch_size * seq_len * d_model * n_layers * 8  # Rough estimate
        ) * 4 / (1024 * 1024)
        
        return {
            'total_parameters': total_params,
            'token_embedding_params': token_embed_params,
            'position_embedding_params': position_embed_params,
            'transformer_params': n_layers * (attention_params_per_layer + ff_params_per_layer + norm_params_per_layer),
            'parameter_memory_mb': param_memory,
            'activation_memory_mb': activation_memory,
            'total_memory_mb': param_memory + activation_memory
        }
    
    @staticmethod
    def suggest_scaling_configuration(target_params: int) -> Dict[str, int]:
        """Suggest model configuration for target parameter count"""
        
        # Common scaling relationships
        configurations = [
            # Small models
            {'params': 125e6, 'd_model': 768, 'n_layers': 12, 'n_heads': 12, 'd_ff': 3072},
            # Medium models  
            {'params': 350e6, 'd_model': 1024, 'n_layers': 24, 'n_heads': 16, 'd_ff': 4096},
            {'params': 760e6, 'd_model': 1536, 'n_layers': 24, 'n_heads': 16, 'd_ff': 6144},
            # Large models
            {'params': 1.5e9, 'd_model': 2048, 'n_layers': 24, 'n_heads': 16, 'd_ff': 8192},
            {'params': 2.8e9, 'd_model': 2560, 'n_layers': 32, 'n_heads': 20, 'd_ff': 10240},
            # Very large models
            {'params': 6.7e9, 'd_model': 4096, 'n_layers': 32, 'n_heads': 32, 'd_ff': 16384},
            {'params': 13e9, 'd_model': 5120, 'n_layers': 40, 'n_heads': 40, 'd_ff': 20480},
            {'params': 175e9, 'd_model': 12288, 'n_layers': 96, 'n_heads': 96, 'd_ff': 49152},
        ]
        
        # Find closest configuration
        closest_config = min(configurations, key=lambda x: abs(x['params'] - target_params))
        
        return {
            'd_model': int(closest_config['d_model']),
            'n_layers': int(closest_config['n_layers']),
            'n_heads': int(closest_config['n_heads']),
            'd_ff': int(closest_config['d_ff']),
            'estimated_params': int(closest_config['params'])
        }

# Example model creation and scaling analysis
def demonstrate_model_scaling():
    """Demonstrate model scaling calculations"""
    
    print("Model Scaling Analysis")
    print("=" * 50)
    
    # Define model configurations
    configs = [
        {'name': 'Small', 'params': 125e6},
        {'name': 'Medium', 'params': 350e6},
        {'name': 'Large', 'params': 1.5e9},
        {'name': 'Very Large', 'params': 13e9},
    ]
    
    vocab_size = 50000
    max_seq_length = 2048
    
    for config in configs:
        print(f"\n{config['name']} Model ({config['params']/1e6:.0f}M parameters):")
        
        # Get suggested configuration
        suggested = ModelScaler.suggest_scaling_configuration(config['params'])
        
        print(f"  Suggested architecture:")
        print(f"    d_model: {suggested['d_model']}")
        print(f"    n_layers: {suggested['n_layers']}")
        print(f"    n_heads: {suggested['n_heads']}")
        print(f"    d_ff: {suggested['d_ff']}")
        
        # Calculate actual model size
        model_stats = ModelScaler.calculate_model_size(
            vocab_size=vocab_size,
            max_seq_length=max_seq_length,
            **{k: v for k, v in suggested.items() if k != 'estimated_params'}
        )
        
        print(f"  Actual parameter count: {model_stats['total_parameters']/1e6:.1f}M")
        print(f"  Memory requirements:")
        print(f"    Parameters: {model_stats['parameter_memory_mb']:.0f} MB")
        print(f"    Activations: {model_stats['activation_memory_mb']:.0f} MB")
        print(f"    Total: {model_stats['total_memory_mb']:.0f} MB")

# Run scaling analysis
demonstrate_model_scaling()
```

## Optimization and training strategies

### Advanced optimizers for pre-training

Pre-training large language models requires sophisticated optimization strategies that can handle the scale and complexity of modern architectures:

```python
import torch
import torch.nn as nn
from torch.optim.optimizer import Optimizer
import math
from typing import List, Optional, Tuple, Dict, Any

class AdamW(Optimizer):
    """
    AdamW optimizer - Adam with decoupled weight decay
    Preferred optimizer for transformer pre-training
    """
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                 weight_decay=1e-2, amsgrad=False):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        defaults = dict(lr=lr, betas=betas, eps=eps,
                       weight_decay=weight_decay, amsgrad=amsgrad)
        super(AdamW, self).__init__(params, defaults)

    def step(self, closure=None):
        """Performs a single optimization step"""
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad.data
                if grad.dtype in {torch.float16, torch.bfloat16}:
                    grad = grad.float()

                if grad.is_sparse:
                    raise RuntimeError('AdamW does not support sparse gradients')

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p.data).float()
                    state['exp_avg_sq'] = torch.zeros_like(p.data).float()
                    if group['amsgrad']:
                        state['max_exp_avg_sq'] = torch.zeros_like(p.data).float()

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                if group['amsgrad']:
                    max_exp_avg_sq = state['max_exp_avg_sq']
                beta1, beta2 = group['betas']

                state['step'] += 1

                # Exponential moving average of gradient values
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                # Exponential moving average of squared gradient values
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                if group['amsgrad']:
                    # Maintains the maximum of all 2nd moment running avg. till now
                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
                    # Use the max. for normalizing running avg. of gradient
                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(1 - beta2 ** state['step'])).add_(group['eps'])
                else:
                    denom = (exp_avg_sq.sqrt() / math.sqrt(1 - beta2 ** state['step'])).add_(group['eps'])

                bias_correction1 = 1 - beta1 ** state['step']
                step_size = group['lr'] / bias_correction1

                # Apply weight decay (decoupled)
                if group['weight_decay'] != 0:
                    p.data.add_(p.data, alpha=-group['weight_decay'] * group['lr'])

                # Apply update
                p.data.addcdiv_(exp_avg, denom, value=-step_size)

        return loss

class Lion(Optimizer):
    """
    Lion optimizer - Evolved Sign Momentum
    Often more memory efficient than Adam
    """
    
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super(Lion, self).__init__(params, defaults)

    def step(self, closure=None):
        """Performs a single optimization step"""
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad.data
                if grad.dtype in {torch.float16, torch.bfloat16}:
                    grad = grad.float()

                state = self.state[p]
                
                # State initialization
                if len(state) == 0:
                    state['exp_avg'] = torch.zeros_like(p.data).float()

                exp_avg = state['exp_avg']
                beta1, beta2 = group['betas']

                # Weight decay
                if group['weight_decay'] != 0:
                    p.data.mul_(1 - group['lr'] * group['weight_decay'])

                # Update
                update = exp_avg * beta1 + grad * (1 - beta1)
                p.data.add_(torch.sign(update), alpha=-group['lr'])
                
                # Update momentum
                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)

        return loss

class LearningRateScheduler:
    """Advanced learning rate scheduling for pre-training"""
    
    def __init__(self, optimizer: Optimizer, schedule_type: str = 'cosine_with_warmup',
                 num_warmup_steps: int = 4000, num_training_steps: int = 100000,
                 min_lr_ratio: float = 0.1, **kwargs):
        self.optimizer = optimizer
        self.schedule_type = schedule_type
        self.num_warmup_steps = num_warmup_steps
        self.num_training_steps = num_training_steps
        self.min_lr_ratio = min_lr_ratio
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]
        self.current_step = 0
        
        # Store additional parameters
        self.kwargs = kwargs
        
    def step(self):
        """Update learning rate"""
        self.current_step += 1
        lrs = self._get_lr()
        
        for param_group, lr in zip(self.optimizer.param_groups, lrs):
            param_group['lr'] = lr
    
    def _get_lr(self) -> List[float]:
        """Calculate learning rates for current step"""
        if self.schedule_type == 'cosine_with_warmup':
            return self._cosine_with_warmup()
        elif self.schedule_type == 'linear_with_warmup':
            return self._linear_with_warmup()
        elif self.schedule_type == 'polynomial_decay':
            return self._polynomial_decay()
        elif self.schedule_type == 'inverse_sqrt':
            return self._inverse_sqrt()
        else:
            raise ValueError(f"Unknown schedule type: {self.schedule_type}")
    
    def _cosine_with_warmup(self) -> List[float]:
        """Cosine annealing with linear warmup"""
        if self.current_step < self.num_warmup_steps:
            # Linear warmup
            warmup_factor = self.current_step / self.num_warmup_steps
            return [base_lr * warmup_factor for base_lr in self.base_lrs]
        else:
            # Cosine annealing
            progress = (self.current_step - self.num_warmup_steps) / (self.num_training_steps - self.num_warmup_steps)
            cosine_factor = 0.5 * (1 + math.cos(math.pi * progress))
            lr_range = 1.0 - self.min_lr_ratio
            return [base_lr * (self.min_lr_ratio + lr_range * cosine_factor) for base_lr in self.base_lrs]
    
    def _linear_with_warmup(self) -> List[float]:
        """Linear decay with warmup"""
        if self.current_step < self.num_warmup_steps:
            warmup_factor = self.current_step / self.num_warmup_steps
            return [base_lr * warmup_factor for base_lr in self.base_lrs]
        else:
            progress = (self.current_step - self.num_warmup_steps) / (self.num_training_steps - self.num_warmup_steps)
            decay_factor = max(self.min_lr_ratio, 1.0 - progress)
            return [base_lr * decay_factor for base_lr in self.base_lrs]
    
    def _polynomial_decay(self) -> List[float]:
        """Polynomial decay schedule"""
        power = self.kwargs.get('power', 1.0)
        if self.current_step < self.num_warmup_steps:
            warmup_factor = self.current_step / self.num_warmup_steps
            return [base_lr * warmup_factor for base_lr in self.base_lrs]
        else:
            progress = (self.current_step - self.num_warmup_steps) / (self.num_training_steps - self.num_warmup_steps)
            decay_factor = max(self.min_lr_ratio, (1.0 - progress) ** power)
            return [base_lr * decay_factor for base_lr in self.base_lrs]
    
    def _inverse_sqrt(self) -> List[float]:
        """Inverse square root schedule (Transformer paper)"""
        if self.current_step < self.num_warmup_steps:
            warmup_factor = self.current_step / self.num_warmup_steps
            return [base_lr * warmup_factor for base_lr in self.base_lrs]
        else:
            inv_sqrt_factor = math.sqrt(self.num_warmup_steps) / math.sqrt(self.current_step)
            return [base_lr * inv_sqrt_factor for base_lr in self.base_lrs]

class GradientClipping:
    """Advanced gradient clipping techniques"""
    
    def __init__(self, clip_type: str = 'norm', max_norm: float = 1.0, 
                 adaptive: bool = False):
        self.clip_type = clip_type
        self.max_norm = max_norm
        self.adaptive = adaptive
        self.gradient_norm_history = []
        
    def clip_gradients(self, model: nn.Module) -> float:
        """Clip gradients and return gradient norm"""
        if self.clip_type == 'norm':
            return self._clip_by_norm(model)
        elif self.clip_type == 'value':
            return self._clip_by_value(model)
        elif self.clip_type == 'adaptive_norm':
            return self._adaptive_clip_by_norm(model)
        else:
            raise ValueError(f"Unknown clip type: {self.clip_type}")
    
    def _clip_by_norm(self, model: nn.Module) -> float:
        """Clip gradients by global norm"""
        parameters = [p for p in model.parameters() if p.grad is not None]
        total_norm = torch.nn.utils.clip_grad_norm_(parameters, self.max_norm)
        return total_norm.item()
    
    def _clip_by_value(self, model: nn.Module) -> float:
        """Clip gradients by value"""
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                p.grad.data.clamp_(-self.max_norm, self.max_norm)
        
        return math.sqrt(total_norm)
    
    def _adaptive_clip_by_norm(self, model: nn.Module) -> float:
        """Adaptive gradient clipping based on gradient norm history"""
        parameters = [p for p in model.parameters() if p.grad is not None]
        
        # Calculate current gradient norm
        total_norm = 0.0
        for p in parameters:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
        total_norm = math.sqrt(total_norm)
        
        # Update history
        self.gradient_norm_history.append(total_norm)
        if len(self.gradient_norm_history) > 1000:
            self.gradient_norm_history = self.gradient_norm_history[-1000:]
        
        # Calculate adaptive threshold
        if len(self.gradient_norm_history) > 10:
            mean_norm = np.mean(self.gradient_norm_history)
            std_norm = np.std(self.gradient_norm_history)
            adaptive_threshold = mean_norm + 2 * std_norm
            clip_norm = min(self.max_norm, adaptive_threshold)
        else:
            clip_norm = self.max_norm
        
        # Apply clipping
        if total_norm > clip_norm:
            clip_coef = clip_norm / total_norm
            for p in parameters:
                p.grad.data.mul_(clip_coef)
        
        return total_norm

# Advanced training utilities
class PretrainingTrainer:
    """Complete training framework for LLM pre-training"""
    
    def __init__(self, model: nn.Module, train_dataloader: DataLoader,
                 eval_dataloader: Optional[DataLoader] = None,
                 optimizer_config: Dict[str, Any] = None,
                 scheduler_config: Dict[str, Any] = None,
                 device: str = 'cuda'):
        
        self.model = model.to(device)
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader
        self.device = device
        
        # Initialize optimizer
        optimizer_config = optimizer_config or {}
        optimizer_type = optimizer_config.pop('type', 'adamw')
        
        if optimizer_type == 'adamw':
            self.optimizer = AdamW(model.parameters(), **optimizer_config)
        elif optimizer_type == 'lion':
            self.optimizer = Lion(model.parameters(), **optimizer_config)
        else:
            raise ValueError(f"Unknown optimizer type: {optimizer_type}")
        
        # Initialize scheduler
        scheduler_config = scheduler_config or {}
        self.scheduler = LearningRateScheduler(self.optimizer, **scheduler_config)
        
        # Initialize gradient clipping
        self.gradient_clipper = GradientClipping()
        
        # Training state
        self.global_step = 0
        self.epoch = 0
        self.best_eval_loss = float('inf')
        
        # Logging
        self.train_losses = []
        self.eval_losses = []
        self.learning_rates = []
        self.gradient_norms = []
    
    def train(self, num_epochs: int = 1, gradient_accumulation_steps: int = 1,
             eval_steps: int = 1000, save_steps: int = 5000,
             log_steps: int = 100, max_steps: Optional[int] = None) -> Dict[str, Any]:
        """
        Main training loop
        
        Args:
            num_epochs: Number of training epochs
            gradient_accumulation_steps: Steps to accumulate gradients
            eval_steps: Steps between evaluations
            save_steps: Steps between checkpoints
            log_steps: Steps between logging
            max_steps: Maximum training steps (overrides epochs if set)
        """
        
        self.model.train()
        criterion = CausalLanguageModelingLoss()
        
        accumulated_loss = 0.0
        steps_in_epoch = 0
        
        for epoch in range(num_epochs):
            self.epoch = epoch
            
            for batch_idx, batch in enumerate(self.train_dataloader):
                # Move batch to device
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward pass
                logits = self.model(input_ids)
                loss = criterion(logits, labels)
                loss = loss / gradient_accumulation_steps
                
                # Backward pass
                loss.backward()
                accumulated_loss += loss.item()
                steps_in_epoch += 1
                
                # Gradient accumulation
                if steps_in_epoch % gradient_accumulation_steps == 0:
                    # Clip gradients
                    grad_norm = self.gradient_clipper.clip_gradients(self.model)
                    self.gradient_norms.append(grad_norm)
                    
                    # Optimizer step
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    
                    self.global_step += 1
                    
                    # Record learning rate
                    current_lr = self.optimizer.param_groups[0]['lr']
                    self.learning_rates.append(current_lr)
                    
                    # Logging
                    if self.global_step % log_steps == 0:
                        avg_loss = accumulated_loss * gradient_accumulation_steps / log_steps
                        perplexity = math.exp(avg_loss)
                        
                        print(f"Step {self.global_step}: Loss = {avg_loss:.4f}, "
                             f"PPL = {perplexity:.2f}, LR = {current_lr:.2e}, "
                             f"Grad Norm = {grad_norm:.3f}")
                        
                        self.train_losses.append(avg_loss)
                        accumulated_loss = 0.0
                    
                    # Evaluation
                    if self.eval_dataloader and self.global_step % eval_steps == 0:
                        eval_loss = self._evaluate()
                        self.eval_losses.append(eval_loss)
                        
                        if eval_loss < self.best_eval_loss:
                            self.best_eval_loss = eval_loss
                            self._save_checkpoint('best_model.pt')
                        
                        self.model.train()  # Back to training mode
                    
                    # Save checkpoint
                    if self.global_step % save_steps == 0:
                        self._save_checkpoint(f'checkpoint_step_{self.global_step}.pt')
                    
                    # Check max steps
                    if max_steps and self.global_step >= max_steps:
                        break
            
            if max_steps and self.global_step >= max_steps:
                break
        
        return {
            'final_loss': self.train_losses[-1] if self.train_losses else None,
            'best_eval_loss': self.best_eval_loss,
            'total_steps': self.global_step,
            'train_losses': self.train_losses,
            'eval_losses': self.eval_losses,
            'learning_rates': self.learning_rates,
            'gradient_norms': self.gradient_norms
        }
    
    def _evaluate(self) -> float:
        """Evaluate model on validation set"""
        self.model.eval()
        criterion = CausalLanguageModelingLoss()
        
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.eval_dataloader:
                if num_batches >= 100:  # Limit eval batches
                    break
                
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                logits = self.model(input_ids)
                loss = criterion(logits, labels)
                
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / max(num_batches, 1)
        print(f"Evaluation: Loss = {avg_loss:.4f}, PPL = {math.exp(avg_loss):.2f}")
        
        return avg_loss
    
    def _save_checkpoint(self, filename: str):
        """Save training checkpoint"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state': {
                'current_step': self.scheduler.current_step,
                'base_lrs': self.scheduler.base_lrs
            },
            'global_step': self.global_step,
            'epoch': self.epoch,
            'best_eval_loss': self.best_eval_loss,
            'train_losses': self.train_losses,
            'eval_losses': self.eval_losses
        }
        torch.save(checkpoint, filename)
        print(f"Saved checkpoint: {filename}")

# Example training configuration
def create_training_example():
    """Example of setting up pre-training"""
    
    print("Pre-training Setup Example")
    print("=" * 50)
    
    # Model configuration
    model_config = {
        'vocab_size': 50000,
        'max_seq_length': 2048,
        'd_model': 768,
        'n_heads': 12,
        'n_layers': 12,
        'd_ff': 3072,
        'dropout': 0.1
    }
    
    print(f"Model Configuration:")
    for key, value in model_config.items():
        print(f"  {key}: {value}")
    
    # Calculate model size
    model_stats = ModelScaler.calculate_model_size(**model_config)
    print(f"\nModel Statistics:")
    print(f"  Parameters: {model_stats['total_parameters']/1e6:.1f}M")
    print(f"  Memory: {model_stats['total_memory_mb']:.0f}MB")
    
    # Optimizer configuration
    optimizer_config = {
        'type': 'adamw',
        'lr': 6e-4,
        'betas': (0.9, 0.95),
        'weight_decay': 0.1,
        'eps': 1e-8
    }
    
    # Scheduler configuration
    scheduler_config = {
        'schedule_type': 'cosine_with_warmup',
        'num_warmup_steps': 2000,
        'num_training_steps': 100000,
        'min_lr_ratio': 0.1
    }
    
    print(f"\nTraining Configuration:")
    print(f"  Optimizer: {optimizer_config}")
    print(f"  Scheduler: {scheduler_config}")
    
    print(f"\nEstimated Training Requirements:")
    print(f"  - High-end GPU with 40+ GB memory")
    print(f"  - Gradient accumulation or model/data parallelism")
    print(f"  - Efficient data pipeline with preprocessing")
    print(f"  - Regular checkpointing and monitoring")

# Run example
create_training_example()
```

## Challenges and considerations

### Computational requirements and scaling

Pre-training large language models requires massive computational resources and careful scaling strategies:

```python
class ComputeEstimator:
    """Estimate compute requirements for LLM pre-training"""
    
    @staticmethod
    def estimate_training_compute(num_parameters: int, num_tokens: int, 
                                 utilization_factor: float = 0.5) -> Dict[str, float]:
        """
        Estimate compute requirements using Kaplan et al. scaling laws
        
        Args:
            num_parameters: Model parameter count
            num_tokens: Number of training tokens
            utilization_factor: Hardware utilization efficiency
        """
        
        # Compute estimate: ~6 FLOPs per parameter per token (forward + backward)
        total_flops = 6 * num_parameters * num_tokens
        
        # Convert to different units
        petaflops_days = total_flops / (1e15 * 24 * 3600)
        
        # Estimate training time on different hardware
        hardware_specs = {
            'V100': {'peak_flops': 125e12, 'memory': 32},      # GB
            'A100': {'peak_flops': 312e12, 'memory': 80},      # GB  
            'H100': {'peak_flops': 989e12, 'memory': 80},      # GB
        }
        
        training_estimates = {}
        for hw_name, specs in hardware_specs.items():
            effective_flops = specs['peak_flops'] * utilization_factor
            training_days = total_flops / (effective_flops * 24 * 3600)
            
            training_estimates[hw_name] = {
                'training_days': training_days,
                'num_gpus_needed': max(1, num_parameters * 4 / (specs['memory'] * 1e9)),  # Rough estimate
                'total_gpu_days': training_days * training_estimates.get(hw_name, {}).get('num_gpus_needed', 1)
            }
        
        return {
            'total_flops': total_flops,
            'petaflops_days': petaflops_days,
            'training_estimates': training_estimates
        }
    
    @staticmethod
    def estimate_costs(compute_estimates: Dict, cost_per_gpu_hour: Dict[str, float]) -> Dict[str, float]:
        """Estimate training costs"""
        cost_estimates = {}
        
        for hw_name, estimates in compute_estimates['training_estimates'].items():
            if hw_name in cost_per_gpu_hour:
                total_gpu_hours = estimates['total_gpu_days'] * 24
                total_cost = total_gpu_hours * cost_per_gpu_hour[hw_name]
                cost_estimates[hw_name] = total_cost
        
        return cost_estimates

# Example compute estimation
def demonstrate_compute_estimation():
    """Demonstrate compute requirement estimation"""
    
    print("Pre-training Compute Requirements")
    print("=" * 50)
    
    models = [
        {'name': 'Small (125M)', 'params': 125e6, 'tokens': 300e9},
        {'name': 'Medium (1.5B)', 'params': 1.5e9, 'tokens': 300e9},
        {'name': 'Large (7B)', 'params': 7e9, 'tokens': 1e12},
        {'name': 'Very Large (70B)', 'params': 70e9, 'tokens': 1.4e12},
    ]
    
    # GPU costs (approximate, per hour)
    gpu_costs = {
        'V100': 3.0,   # USD per hour
        'A100': 4.0,   # USD per hour  
        'H100': 8.0,   # USD per hour
    }
    
    for model in models:
        print(f"\n{model['name']} Parameters:")
        print(f"  Parameters: {model['params']/1e9:.1f}B")
        print(f"  Training tokens: {model['tokens']/1e9:.0f}B")
        
        # Compute estimates
        compute_est = ComputeEstimator.estimate_training_compute(
            model['params'], model['tokens']
        )
        
        print(f"  Total FLOPs: {compute_est['total_flops']:.2e}")
        print(f"  PetaFLOP-days: {compute_est['petaflops_days']:.1f}")
        
        # Cost estimates
        cost_est = ComputeEstimator.estimate_costs(compute_est, gpu_costs)
        
        print(f"  Training estimates:")
        for hw, est in compute_est['training_estimates'].items():
            cost = cost_est.get(hw, 0)
            print(f"    {hw}: {est['training_days']:.0f} days, "
                 f"{est['num_gpus_needed']:.0f} GPUs, "
                 f"${cost:,.0f}")

demonstrate_compute_estimation()
```

Pre-training large language models represents one of the most computationally intensive tasks in modern AI. Success requires careful orchestration of data preparation, model architecture, optimization strategies, and distributed training infrastructure. As models continue to scale, innovations in efficiency, training methods, and computational resources will be crucial for advancing the capabilities of language models while making them more accessible to researchers and practitioners.

The techniques covered in this comprehensive guide provide the foundation for understanding and implementing pre-training systems, from small experimental models to large-scale production systems. As the field evolves, these core principles will continue to underpin the development of increasingly capable and efficient language models.