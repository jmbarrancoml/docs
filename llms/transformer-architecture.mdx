---
title: "Transformer Architecture"
description: "Deep dive into the architecture that revolutionized natural language processing"
---

## Introduction to Transformers

The Transformer architecture, introduced in "Attention Is All You Need" (Vaswani et al., 2017), revolutionized natural language processing and became the foundation for modern large language models. Unlike previous architectures that processed sequences sequentially, Transformers process all positions in parallel using self-attention mechanisms.

## Core Architecture Overview

### High-Level Structure
The original Transformer consists of two main components:
1. **Encoder**: Processes input sequences into representations
2. **Decoder**: Generates output sequences autoregressively

```
Input → Encoder → Decoder → Output
```

However, many modern LLMs use decoder-only architectures (like GPT) or encoder-only architectures (like BERT).

### Key Components
1. **Multi-Head Self-Attention**
2. **Feed-Forward Networks**
3. **Layer Normalization**
4. **Residual Connections**
5. **Positional Encoding**

## Self-Attention Mechanism

### The Attention Function
Attention maps queries to key-value pairs:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q$ = Queries matrix
- $K$ = Keys matrix  
- $V$ = Values matrix
- $d_k$ = Dimension of keys

### Mathematical Foundation

#### Step 1: Compute Attention Scores
$$\text{scores} = \frac{QK^T}{\sqrt{d_k}}$$

The scaling by $\sqrt{d_k}$ prevents the softmax function from having extremely small gradients.

#### Step 2: Apply Softmax
$$\text{attention\_weights} = \text{softmax}(\text{scores})$$

This ensures attention weights sum to 1 across the sequence length dimension.

#### Step 3: Apply to Values
$$\text{output} = \text{attention\_weights} \times V$$

### Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_model, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len, d_model = query.size()
        
        # Compute attention scores
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_model)
        
        # Apply mask if provided (for padding or causality)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        output = torch.matmul(attention_weights, value)
        
        return output, attention_weights
```

### Intuitive Understanding
Self-attention allows each position to "look at" all other positions in the sequence:
- **Query**: "What am I looking for?"
- **Key**: "What do I have to offer?"
- **Value**: "What information do I actually provide?"

For the sentence "The cat sat on the mat":
- The word "sat" might attend strongly to "cat" (subject-verb relationship)
- "on" might attend to both "sat" and "mat" (prepositional relationships)

## Multi-Head Attention

### Concept
Instead of using a single attention function, Multi-Head Attention runs multiple attention mechanisms in parallel, each focusing on different aspects of the relationships.

### Mathematical Formulation
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

Where each head is:
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

### Implementation

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear projections for Q, K, V
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k, dropout)
        
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len, d_model = query.size()
        
        # Linear transformations and split into heads
        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # Apply attention to each head
        attention_output, attention_weights = self.attention(Q, K, V, mask)
        
        # Concatenate heads
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        # Final linear projection
        output = self.w_o(attention_output)
        
        return output, attention_weights
```

### Why Multiple Heads?
Different attention heads can capture different types of relationships:
- **Head 1**: Syntactic relationships (subject-verb, adjective-noun)
- **Head 2**: Semantic relationships (cat-animal, happy-emotion)
- **Head 3**: Positional relationships (first-second, beginning-end)

## Positional Encoding

### The Problem
Transformers have no inherent notion of sequence order since attention is applied to all positions simultaneously. Positional encoding addresses this limitation.

### Sinusoidal Positional Encoding
The original paper uses sinusoidal functions:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

Where:
- $pos$ = position in sequence
- $i$ = dimension index
- $d_{model}$ = model dimension

### Implementation

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length=5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

### Alternative Approaches
- **Learned Positional Embeddings**: Trainable position vectors
- **Relative Positional Encoding**: Focus on relative distances
- **Rotary Position Embedding (RoPE)**: Used in modern models like GPT-NeoX

## Feed-Forward Networks

### Structure
Each transformer layer includes a position-wise feed-forward network:

$$\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$$

### Implementation

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        return self.linear2(self.dropout(self.relu(self.linear1(x))))
```

### Modern Variations
- **SwiGLU**: Used in LLaMA models
- **GeGLU**: Gated activation functions
- **MoE (Mixture of Experts)**: Conditional computation

## Layer Normalization and Residual Connections

### Layer Normalization
Normalizes activations across the feature dimension:

$$\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sigma} + \beta$$

Where:
- $\mu$ = mean across features
- $\sigma$ = standard deviation across features
- $\gamma$, $\beta$ = learned parameters

### Residual Connections
Enable training of very deep networks:

$$\text{output} = \text{LayerNorm}(x + \text{Sublayer}(x))$$

### Implementation

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Self-attention with residual connection and layer norm
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward with residual connection and layer norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

## Complete Transformer Architecture

### Decoder-Only Model (GPT-style)

```python
class TransformerLM(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
        
        self.ln_f = nn.LayerNorm(d_model)  # Final layer norm
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
    def forward(self, input_ids):
        batch_size, seq_len = input_ids.size()
        
        # Create causal mask
        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
        
        # Token embeddings + positional encoding
        x = self.embedding(input_ids) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        
        # Apply transformer blocks
        for block in self.transformer_blocks:
            x = block(x, mask)
        
        # Final layer norm and projection to vocabulary
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        return logits
```

## Architecture Variants

### Encoder-Only (BERT-style)
- **Use Case**: Understanding tasks (classification, NER)
- **Training**: Masked Language Modeling
- **Attention**: Bidirectional (can see future tokens)

### Decoder-Only (GPT-style)
- **Use Case**: Generation tasks
- **Training**: Autoregressive Language Modeling
- **Attention**: Causal (can only see past tokens)

### Encoder-Decoder (T5-style)
- **Use Case**: Sequence-to-sequence tasks
- **Training**: Various objectives (span corruption, etc.)
- **Attention**: Bidirectional encoder, causal decoder

## Modern Improvements

### Architectural Innovations

#### 1. Pre-Layer Normalization
Move layer normalization before the sub-layers:

```python
# Pre-norm (modern approach)
x = x + attention(layernorm(x))
x = x + feedforward(layernorm(x))
```

#### 2. RMSNorm
Replace LayerNorm with RMSNorm for better efficiency:

$$\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \gamma$$

Where $\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2}$

#### 3. Rotary Position Embedding (RoPE)
Encode positional information through rotation:

$$f(x_m, m) = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix} \begin{pmatrix} x_m^{(1)} \\ x_m^{(2)} \end{pmatrix}$$

### Efficiency Improvements

#### 1. Flash Attention
Reduce memory complexity from $O(n^2)$ to $O(n)$ through clever tiling:

```python
# Conceptual Flash Attention
def flash_attention(Q, K, V, block_size):
    # Tile computation to reduce memory usage
    # Process attention in blocks rather than all at once
    pass
```

#### 2. Sparse Attention
Only attend to a subset of positions:
- **Local Attention**: Attend to nearby positions
- **Strided Attention**: Attend to every k-th position
- **Random Attention**: Attend to random positions

## Training Considerations

### Initialization
Proper weight initialization is crucial:

```python
def init_weights(module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

### Scaling Laws
Transformer performance scales predictably with:
- **Model size** (number of parameters)
- **Dataset size** (tokens of training data)
- **Compute budget** (FLOPs during training)

### Memory and Compute
Training large transformers requires:
- **Memory**: Store activations and gradients
- **Compute**: Matrix multiplications in attention and FFN
- **Communication**: Distribute across multiple GPUs

## Common Issues and Solutions

### Training Instability
- **Gradient Clipping**: Prevent exploding gradients
- **Learning Rate Scheduling**: Warm-up and decay
- **Mixed Precision**: Use FP16 for efficiency

### Attention Collapse
- **Layer Normalization**: Stabilize training
- **Residual Connections**: Maintain gradient flow
- **Proper Initialization**: Prevent saturation

### Memory Issues
- **Gradient Checkpointing**: Trade compute for memory
- **Model Parallelism**: Split model across GPUs
- **Sequence Parallelism**: Split sequences across GPUs

## Impact and Applications

The transformer architecture has enabled:
- **Large Language Models**: GPT, BERT, T5, PaLM
- **Computer Vision**: Vision Transformer (ViT)
- **Multimodal Models**: CLIP, DALL-E, GPT-4V
- **Scientific Computing**: AlphaFold, protein design
- **Code Generation**: GitHub Copilot, CodeT5

## Future Directions

### Architectural Research
- **Longer Context**: Handling much longer sequences
- **Efficient Attention**: Reducing quadratic complexity
- **Multimodal Integration**: Better fusion of modalities
- **Specialized Architectures**: Domain-specific designs

### Hardware Co-design
- **Custom Chips**: TPUs, IPUs designed for transformers
- **Memory Hierarchies**: Optimizing data movement
- **Sparse Computation**: Leveraging sparsity patterns

The transformer architecture continues to be the foundation for most state-of-the-art language models. Understanding its components and variations is essential for working with modern AI systems.

## Next Steps

<Card
  title="Training Process"
  icon="graduation-cap"
  href="/llms/pretraining"
>
  Learn how transformers are trained at scale.
</Card>

<Card
  title="Model Families"
  icon="sitemap"
  href="/llms/model-families"
>
  Compare different transformer-based architectures.
</Card>

<Card
  title="Scaling Laws"
  icon="trending-up"
  href="/llms/scaling-laws"
>
  Understand how performance scales with model size and data.
</Card>