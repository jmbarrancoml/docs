---
title: "Scaling laws"
description: "How model performance improves predictably with compute, data, and parameters"
---

## Introduction to scaling laws

Scaling laws describe the predictable relationships between the size of neural networks, the amount of training data, computational resources, and model performance. These empirical relationships have become fundamental to understanding how to build better AI systems and have driven the development of increasingly large language models.

## Historical context and discovery

### Early observations
Before formal scaling laws, researchers observed that larger models generally performed better, but the relationships weren't precisely quantified.

### Breakthrough research
- **OpenAI (2020)**: First systematic study of scaling laws for language models
- **DeepMind (2021)**: Optimal allocation of compute between model size and training data  
- **Anthropic (2022)**: Constitutional AI and scaling supervision
- **Meta (2022)**: Chinchilla paper revealing optimal compute allocation

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

def demonstrate_scaling_law():
    """Demonstrate basic scaling law relationship"""
    
    # Synthetic data following power law: L = A * N^(-α)
    # Where L is loss, N is number of parameters, A is constant, α is scaling exponent
    
    model_sizes = np.logspace(6, 11, 20)  # 1M to 100B parameters
    alpha = 0.076  # Empirically observed scaling exponent
    A = 100
    
    # Perfect scaling law
    losses_perfect = A * model_sizes**(-alpha)
    
    # Add realistic noise
    np.random.seed(42)
    losses_noisy = losses_perfect * (1 + 0.05 * np.random.randn(len(model_sizes)))
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.loglog(model_sizes, losses_perfect, 'r-', label=f'Perfect scaling law: L ∝ N^(-{alpha})')
    plt.loglog(model_sizes, losses_noisy, 'bo', alpha=0.7, label='Observed performance')
    
    plt.xlabel('Number of parameters')
    plt.ylabel('Test loss')
    plt.title('Language model scaling law')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return model_sizes, losses_noisy

# demonstrate_scaling_law()
```

## Core scaling laws

### Parameter scaling
The fundamental relationship between model size and performance:

$$L(N) = A \cdot N^{-\alpha} + B$$

Where:
- $L$ = test loss
- $N$ = number of parameters  
- $A$ = scale constant
- $\alpha$ = scaling exponent (≈ 0.076 for language models)
- $B$ = irreducible loss (performance ceiling)

```python
class ParameterScalingLaw:
    def __init__(self, A=100, alpha=0.076, B=1.8):
        self.A = A
        self.alpha = alpha
        self.B = B
    
    def predict_loss(self, num_parameters):
        """Predict test loss given number of parameters"""
        return self.A * (num_parameters ** (-self.alpha)) + self.B
    
    def parameters_for_loss(self, target_loss):
        """Calculate parameters needed to achieve target loss"""
        if target_loss <= self.B:
            return float('inf')  # Cannot achieve better than irreducible loss
        
        return ((target_loss - self.B) / self.A) ** (-1 / self.alpha)
    
    def improvement_ratio(self, n1, n2):
        """Loss improvement ratio when scaling from n1 to n2 parameters"""
        loss1 = self.predict_loss(n1)
        loss2 = self.predict_loss(n2)
        return loss1 / loss2

# Example usage
scaling_law = ParameterScalingLaw()

models = [
    ("GPT-1", 117e6),
    ("GPT-2", 1.5e9),
    ("GPT-3", 175e9),
    ("GPT-4", 1.8e12)  # Estimated
]

print("Model scaling predictions:")
for name, params in models:
    predicted_loss = scaling_law.predict_loss(params)
    print(f"{name}: {params:.0e} parameters → loss: {predicted_loss:.3f}")
```

### Data scaling
Performance improves with more training data:

$$L(D) = A \cdot D^{-\beta} + B$$

Where:
- $D$ = dataset size (number of tokens)
- $\beta$ = data scaling exponent (≈ 0.095)

```python
class DataScalingLaw:
    def __init__(self, A=50, beta=0.095, B=1.8):
        self.A = A
        self.beta = beta
        self.B = B
    
    def predict_loss(self, dataset_size):
        """Predict test loss given dataset size"""
        return self.A * (dataset_size ** (-self.beta)) + self.B
    
    def data_for_loss(self, target_loss):
        """Calculate data needed to achieve target loss"""
        if target_loss <= self.B:
            return float('inf')
        
        return ((target_loss - self.B) / self.A) ** (-1 / self.beta)

def plot_data_scaling():
    """Visualize data scaling law"""
    data_scaling = DataScalingLaw()
    
    # Dataset sizes from millions to trillions of tokens
    dataset_sizes = np.logspace(6, 13, 30)
    losses = [data_scaling.predict_loss(size) for size in dataset_sizes]
    
    plt.figure(figsize=(10, 6))
    plt.loglog(dataset_sizes, losses, 'g-', linewidth=2)
    plt.xlabel('Dataset size (tokens)')
    plt.ylabel('Test loss')
    plt.title('Data scaling law for language models')
    plt.grid(True, alpha=0.3)
    
    # Annotate key dataset sizes
    milestones = [
        (1e9, "1B tokens"),
        (1e11, "100B tokens"),
        (1e12, "1T tokens")
    ]
    
    for size, label in milestones:
        loss = data_scaling.predict_loss(size)
        plt.annotate(label, (size, loss), xytext=(10, 10), 
                    textcoords='offset points', fontsize=9)
    
    plt.show()
```

### Compute scaling
Total compute budget determines achievable performance:

$$L(C) = A \cdot C^{-\gamma} + B$$

Where:
- $C$ = compute (FLOPs)
- $\gamma$ = compute scaling exponent (≈ 0.056)

```python
class ComputeScalingLaw:
    def __init__(self, A=200, gamma=0.056, B=1.8):
        self.A = A
        self.gamma = gamma
        self.B = B
    
    def predict_loss(self, compute_flops):
        """Predict test loss given compute budget"""
        return self.A * (compute_flops ** (-self.gamma)) + self.B
    
    def compute_for_loss(self, target_loss):
        """Calculate compute needed to achieve target loss"""
        if target_loss <= self.B:
            return float('inf')
        
        return ((target_loss - self.B) / self.A) ** (-1 / self.gamma)

def calculate_training_compute(num_parameters, dataset_size, efficiency=0.5):
    """Calculate FLOPs needed to train a model
    
    Args:
        num_parameters: Number of model parameters
        dataset_size: Number of training tokens
        efficiency: Hardware efficiency factor
    """
    # Rough estimate: 6 * N * D FLOPs
    # Factor of 6 accounts for forward pass (1x) + backward pass (2x) + parameter updates
    theoretical_flops = 6 * num_parameters * dataset_size
    
    # Account for hardware efficiency
    actual_flops = theoretical_flops / efficiency
    
    return actual_flops

# Example: GPT-3 training compute
gpt3_params = 175e9
gpt3_tokens = 300e9  # Estimated training tokens
gpt3_compute = calculate_training_compute(gpt3_params, gpt3_tokens)

print(f"GPT-3 training compute: {gpt3_compute:.2e} FLOPs")
print(f"That's approximately {gpt3_compute / 1e23:.1f} × 10^23 FLOPs")
```

## Chinchilla scaling laws

### Optimal compute allocation
The Chinchilla paper revealed that most large models were undertrained:

```python
class ChinchillaScaling:
    def __init__(self):
        # Optimal allocation constants from Chinchilla paper
        self.a = 407.7
        self.b = 410.7
        self.alpha = 0.34
        self.beta = 0.28
    
    def optimal_model_size(self, compute_budget):
        """Calculate optimal number of parameters for given compute"""
        return self.a * (compute_budget ** self.alpha)
    
    def optimal_dataset_size(self, compute_budget):
        """Calculate optimal dataset size for given compute"""  
        return self.b * (compute_budget ** self.beta)
    
    def optimal_allocation(self, compute_budget):
        """Return optimal N and D for given compute budget"""
        optimal_N = self.optimal_model_size(compute_budget)
        optimal_D = self.optimal_dataset_size(compute_budget)
        
        return optimal_N, optimal_D
    
    def is_compute_optimal(self, num_parameters, dataset_size, compute_budget):
        """Check if a model uses compute optimally"""
        optimal_N, optimal_D = self.optimal_allocation(compute_budget)
        
        n_ratio = num_parameters / optimal_N
        d_ratio = dataset_size / optimal_D
        
        return {
            'parameter_ratio': n_ratio,
            'data_ratio': d_ratio,
            'is_optimal': 0.5 < n_ratio < 2.0 and 0.5 < d_ratio < 2.0
        }

def analyze_model_efficiency():
    """Analyze whether famous models used compute optimally"""
    chinchilla = ChinchillaScaling()
    
    models = [
        ("GPT-3", 175e9, 300e9, 3.14e23),
        ("Gopher", 280e9, 300e9, 5.76e23),
        ("Chinchilla", 70e9, 1.4e12, 5.76e23),  # Same compute as Gopher
        ("PaLM", 540e9, 780e9, 2.56e24)
    ]
    
    print("Model efficiency analysis:")
    print("=" * 60)
    
    for name, params, data, compute in models:
        analysis = chinchilla.is_compute_optimal(params, data, compute)
        optimal_N, optimal_D = chinchilla.optimal_allocation(compute)
        
        print(f"\n{name}:")
        print(f"  Parameters: {params:.1e} (optimal: {optimal_N:.1e})")
        print(f"  Data: {data:.1e} tokens (optimal: {optimal_D:.1e})")
        print(f"  Parameter ratio: {analysis['parameter_ratio']:.2f}")
        print(f"  Data ratio: {analysis['data_ratio']:.2f}")
        print(f"  Compute optimal: {analysis['is_optimal']}")

# analyze_model_efficiency()
```

### Training time scaling
How training time relates to model size and dataset:

```python
def estimate_training_time(num_parameters, dataset_size, 
                         gpu_memory=80e9,  # 80GB A100
                         gpu_throughput=312e12,  # TFLOPS
                         num_gpus=1024):
    """Estimate training time for a language model"""
    
    # Memory requirements (rough estimates)
    model_memory = num_parameters * 4  # 4 bytes per parameter (FP32)
    optimizer_memory = num_parameters * 12  # Adam optimizer states
    activation_memory = 1e9  # Batch-dependent, rough estimate
    
    total_memory_per_gpu = model_memory + optimizer_memory + activation_memory
    
    # Check if model fits
    if total_memory_per_gpu > gpu_memory:
        required_gpus_memory = int(np.ceil(total_memory_per_gpu / gpu_memory))
        print(f"Warning: Model requires {required_gpus_memory} GPUs just for memory")
    
    # Compute requirements
    compute_flops = calculate_training_compute(num_parameters, dataset_size)
    
    # Total system throughput
    system_throughput = num_gpus * gpu_throughput
    
    # Training time
    training_time_seconds = compute_flops / system_throughput
    training_time_days = training_time_seconds / (24 * 3600)
    
    return {
        'training_time_days': training_time_days,
        'compute_flops': compute_flops,
        'memory_per_gpu_gb': total_memory_per_gpu / 1e9,
        'requires_gpus': max(num_gpus, int(np.ceil(total_memory_per_gpu / gpu_memory)))
    }

# Example: Estimate training time for various models
models = [
    ("GPT-2", 1.5e9, 40e9),
    ("GPT-3", 175e9, 300e9),
    ("GPT-4", 1.8e12, 13e12),  # Estimated
]

print("Training time estimates (1024 A100 GPUs):")
print("=" * 50)

for name, params, data in models:
    estimates = estimate_training_time(params, data)
    print(f"\n{name}:")
    print(f"  Training time: {estimates['training_time_days']:.1f} days")
    print(f"  Compute: {estimates['compute_flops']:.2e} FLOPs")
    print(f"  Memory per GPU: {estimates['memory_per_gpu_gb']:.1f} GB")
    print(f"  Required GPUs: {estimates['requires_gpus']}")
```

## Emergence and phase transitions

### Emergent abilities
Certain capabilities appear suddenly at specific scales:

```python
def model_emergent_abilities():
    """Model emergence of capabilities with scale"""
    
    abilities = {
        'arithmetic': {'threshold': 1e8, 'steepness': 2.0},
        'few_shot_learning': {'threshold': 1e9, 'steepness': 1.5},
        'chain_of_thought': {'threshold': 1e11, 'steepness': 3.0},
        'code_generation': {'threshold': 2e10, 'steepness': 2.5},
        'instruction_following': {'threshold': 5e10, 'steepness': 2.0}
    }
    
    def emergence_curve(num_parameters, threshold, steepness):
        """Sigmoid-like emergence curve"""
        x = np.log10(num_parameters / threshold)
        return 1 / (1 + np.exp(-steepness * x))
    
    # Model sizes to evaluate
    model_sizes = np.logspace(6, 12, 100)
    
    plt.figure(figsize=(12, 8))
    
    for ability, params in abilities.items():
        capabilities = [emergence_curve(size, params['threshold'], params['steepness']) 
                       for size in model_sizes]
        plt.semilogx(model_sizes, capabilities, label=ability, linewidth=2)
    
    plt.xlabel('Number of parameters')
    plt.ylabel('Capability score')
    plt.title('Emergent abilities in language models')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Mark key model sizes
    key_models = [
        (1.5e9, "GPT-2"),
        (1.75e11, "GPT-3"),
        (5.4e11, "PaLM"),
    ]
    
    for size, name in key_models:
        plt.axvline(size, color='red', linestyle='--', alpha=0.5)
        plt.text(size, 0.9, name, rotation=90, ha='right')
    
    plt.show()

def analyze_capability_thresholds():
    """Analyze at what scale different capabilities emerge"""
    
    # Data from various papers and benchmarks
    capability_data = {
        'Basic language modeling': {'min_params': 1e6, 'good_params': 1e8},
        'Few-shot in-context learning': {'min_params': 1e9, 'good_params': 1e11},
        'Chain-of-thought reasoning': {'min_params': 1e11, 'good_params': 5e11},
        'Code generation': {'min_params': 2e10, 'good_params': 2e11},
        'Mathematical reasoning': {'min_params': 5e10, 'good_params': 5e11},
        'Complex instruction following': {'min_params': 1e11, 'good_params': 1e12},
    }
    
    print("Capability emergence thresholds:")
    print("=" * 50)
    
    for capability, thresholds in capability_data.items():
        print(f"\n{capability}:")
        print(f"  Minimal performance: {thresholds['min_params']:.0e} parameters")
        print(f"  Good performance: {thresholds['good_params']:.0e} parameters")
        print(f"  Scaling factor: {thresholds['good_params']/thresholds['min_params']:.1f}x")

# analyze_capability_thresholds()
```

## Limitations and exceptions

### Where scaling laws break down

```python
def scaling_law_limitations():
    """Demonstrate limitations of scaling laws"""
    
    limitations = {
        'data_quality': {
            'description': 'Low-quality data provides diminishing returns',
            'impact': 'Reduces effective data scaling exponent'
        },
        'task_saturation': {
            'description': 'Some tasks saturate at human performance',
            'impact': 'Creates performance ceiling below theoretical limit'
        },
        'architectural_constraints': {
            'description': 'Model architecture may become bottleneck',
            'impact': 'Scaling exponent decreases for very large models'
        },
        'compute_efficiency': {
            'description': 'Hardware limitations affect scaling',
            'impact': 'Actual training time scales worse than theoretical'
        },
        'sample_efficiency': {
            'description': 'Models may not efficiently use all data',
            'impact': 'Data scaling becomes sublinear'
        }
    }
    
    return limitations

def modified_scaling_with_saturation(num_parameters, saturation_point=1.5):
    """Scaling law with performance saturation"""
    base_scaling = ParameterScalingLaw()
    base_loss = base_scaling.predict_loss(num_parameters)
    
    # Add saturation effect
    saturation_factor = 1 - np.exp(-(base_loss - saturation_point))
    if base_loss < saturation_point:
        adjusted_loss = saturation_point + (base_loss - saturation_point) * saturation_factor
    else:
        adjusted_loss = base_loss
    
    return adjusted_loss

def plot_scaling_limitations():
    """Plot ideal vs realistic scaling curves"""
    model_sizes = np.logspace(6, 13, 100)
    
    # Ideal scaling
    scaling_law = ParameterScalingLaw()
    ideal_losses = [scaling_law.predict_loss(size) for size in model_sizes]
    
    # With saturation
    saturated_losses = [modified_scaling_with_saturation(size) for size in model_sizes]
    
    plt.figure(figsize=(10, 6))
    plt.loglog(model_sizes, ideal_losses, 'b-', label='Ideal scaling law')
    plt.loglog(model_sizes, saturated_losses, 'r--', label='With saturation effects')
    
    plt.xlabel('Number of parameters')
    plt.ylabel('Test loss')
    plt.title('Scaling laws: ideal vs realistic')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# plot_scaling_limitations()
```

## Practical implications

### Model design decisions
How scaling laws inform model architecture choices:

```python
class ScalingGuidedDesign:
    def __init__(self):
        self.param_scaling = ParameterScalingLaw()
        self.data_scaling = DataScalingLaw()
        self.chinchilla = ChinchillaScaling()
    
    def recommend_model_config(self, compute_budget, target_loss=None):
        """Recommend optimal model configuration"""
        
        # Get Chinchilla-optimal allocation
        optimal_params, optimal_data = self.chinchilla.optimal_allocation(compute_budget)
        
        # Predict performance
        predicted_loss = self.param_scaling.predict_loss(optimal_params)
        
        # Architecture recommendations based on size
        if optimal_params < 1e9:
            arch_rec = "Dense transformer, standard attention"
        elif optimal_params < 1e11:
            arch_rec = "Dense transformer, consider sparse attention for efficiency"
        elif optimal_params < 1e12:
            arch_rec = "Mixture of experts (MoE) for efficiency"
        else:
            arch_rec = "MoE with optimized routing and sparse patterns"
        
        config = {
            'parameters': optimal_params,
            'dataset_tokens': optimal_data,
            'predicted_loss': predicted_loss,
            'architecture_rec': arch_rec,
            'compute_budget': compute_budget
        }
        
        if target_loss:
            needed_params = self.param_scaling.parameters_for_loss(target_loss)
            needed_compute = self.chinchilla.compute_for_params(needed_params)
            config['target_analysis'] = {
                'needed_parameters': needed_params,
                'needed_compute': needed_compute,
                'feasible': needed_compute <= compute_budget * 2  # Some tolerance
            }
        
        return config
    
    def scaling_roadmap(self, current_model_size, target_improvements):
        """Create scaling roadmap for model improvements"""
        roadmap = []
        
        current_loss = self.param_scaling.predict_loss(current_model_size)
        
        for improvement_factor in target_improvements:
            target_loss = current_loss / improvement_factor
            needed_params = self.param_scaling.parameters_for_loss(target_loss)
            needed_compute = self.estimate_compute(needed_params)
            
            roadmap.append({
                'improvement': f"{improvement_factor}x",
                'target_loss': target_loss,
                'needed_parameters': needed_params,
                'parameter_scale_up': needed_params / current_model_size,
                'compute_requirements': needed_compute
            })
        
        return roadmap
    
    def estimate_compute(self, num_parameters):
        """Estimate compute needed for given model size"""
        # Use Chinchilla-optimal data allocation
        optimal_data = self.chinchilla.optimal_dataset_size(num_parameters)
        return calculate_training_compute(num_parameters, optimal_data)

# Example usage
design_guide = ScalingGuidedDesign()

# Design a model with specific compute budget
compute_budget = 1e24  # 1e24 FLOPs
config = design_guide.recommend_model_config(compute_budget)

print("Recommended model configuration:")
print(f"Parameters: {config['parameters']:.2e}")
print(f"Dataset size: {config['dataset_tokens']:.2e} tokens")
print(f"Predicted loss: {config['predicted_loss']:.3f}")
print(f"Architecture: {config['architecture_rec']}")

# Create scaling roadmap
current_size = 175e9  # GPT-3 scale
improvements = [2, 5, 10, 20]
roadmap = design_guide.scaling_roadmap(current_size, improvements)

print("\nScaling roadmap from GPT-3 baseline:")
for milestone in roadmap:
    print(f"{milestone['improvement']} improvement:")
    print(f"  Parameters: {milestone['needed_parameters']:.2e}")
    print(f"  Scale-up: {milestone['parameter_scale_up']:.1f}x")
    print(f"  Compute: {milestone['compute_requirements']:.2e} FLOPs")
```

### Cost-performance optimization

```python
def cost_optimization_analysis():
    """Analyze cost-performance trade-offs"""
    
    # Hardware costs (approximate)
    hardware_costs = {
        'A100_80GB': {'cost_per_hour': 3.2, 'tflops': 312e12, 'memory_gb': 80},
        'H100_80GB': {'cost_per_hour': 4.5, 'tflops': 500e12, 'memory_gb': 80},
        'TPU_v4': {'cost_per_hour': 2.8, 'tflops': 275e12, 'memory_gb': 32}
    }
    
    def training_cost(num_parameters, dataset_size, hardware_type='A100_80GB'):
        """Calculate training cost for given model"""
        compute_flops = calculate_training_compute(num_parameters, dataset_size)
        hardware = hardware_costs[hardware_type]
        
        # Training time in hours
        training_hours = compute_flops / hardware['tflops'] / 3600
        
        # Number of GPUs needed (memory constrained)
        model_memory_gb = (num_parameters * 16) / 1e9  # Mixed precision
        num_gpus = max(1, int(np.ceil(model_memory_gb / hardware['memory_gb'])))
        
        total_cost = training_hours * num_gpus * hardware['cost_per_hour']
        
        return {
            'total_cost': total_cost,
            'training_hours': training_hours,
            'num_gpus': num_gpus,
            'cost_per_gpu_hour': hardware['cost_per_hour']
        }
    
    # Analyze cost-performance for different model sizes
    model_configs = [
        (1e9, 20e9, "1B parameter model"),
        (7e9, 140e9, "7B parameter model (LLaMA-7B scale)"),
        (13e9, 260e9, "13B parameter model"),
        (65e9, 1.3e12, "65B parameter model"),
        (175e9, 3.5e12, "175B parameter model (GPT-3 scale)")
    ]
    
    print("Training cost analysis:")
    print("=" * 70)
    
    scaling_law = ParameterScalingLaw()
    
    for params, data, description in model_configs:
        cost_info = training_cost(params, data)
        predicted_loss = scaling_law.predict_loss(params)
        
        print(f"\n{description}:")
        print(f"  Parameters: {params:.0e}")
        print(f"  Training data: {data:.0e} tokens")
        print(f"  Predicted loss: {predicted_loss:.3f}")
        print(f"  Training cost: ${cost_info['total_cost']:,.0f}")
        print(f"  Training time: {cost_info['training_hours']:,.0f} hours ({cost_info['training_hours']/24:.1f} days)")
        print(f"  Required GPUs: {cost_info['num_gpus']}")
        print(f"  Cost per loss point: ${cost_info['total_cost']/predicted_loss:,.0f}")

# cost_optimization_analysis()
```

## Future scaling predictions

### Extrapolating scaling laws

```python
def predict_future_capabilities():
    """Predict future model capabilities based on scaling trends"""
    
    # Historical data points
    historical_models = {
        2018: {'name': 'GPT-1', 'params': 117e6, 'compute': 1e19},
        2019: {'name': 'GPT-2', 'params': 1.5e9, 'compute': 1e21},
        2020: {'name': 'GPT-3', 'params': 175e9, 'compute': 3.14e23},
        2022: {'name': 'PaLM', 'params': 540e9, 'compute': 2.5e24},
        2023: {'name': 'GPT-4', 'params': 1.8e12, 'compute': 2e25}  # Estimated
    }
    
    # Fit exponential growth trends
    years = list(historical_models.keys())
    params = [historical_models[year]['params'] for year in years]
    compute = [historical_models[year]['compute'] for year in years]
    
    # Exponential fit: y = a * exp(b * (x - x0))
    def exp_growth(x, a, b):
        return a * np.exp(b * (x - 2018))
    
    from scipy.optimize import curve_fit
    
    # Fit parameter growth
    param_fit, _ = curve_fit(exp_growth, years, params)
    compute_fit, _ = curve_fit(exp_growth, years, compute)
    
    # Predict future years
    future_years = range(2024, 2031)
    
    print("Scaling law extrapolations:")
    print("=" * 50)
    
    scaling_law = ParameterScalingLaw()
    
    for year in future_years:
        predicted_params = exp_growth(year, *param_fit)
        predicted_compute = exp_growth(year, *compute_fit)
        predicted_loss = scaling_law.predict_loss(predicted_params)
        
        print(f"\n{year} predictions:")
        print(f"  Parameters: {predicted_params:.2e}")
        print(f"  Compute: {predicted_compute:.2e} FLOPs")
        print(f"  Predicted loss: {predicted_loss:.3f}")
        
        # Capability predictions based on thresholds
        if predicted_params > 1e13:
            print("  Expected capabilities: Human-level reasoning, complex problem solving")
        elif predicted_params > 1e12:
            print("  Expected capabilities: Advanced reasoning, expert-level knowledge")
        elif predicted_params > 5e11:
            print("  Expected capabilities: Strong reasoning, reliable instruction following")

def scaling_bottlenecks():
    """Identify potential bottlenecks to continued scaling"""
    
    bottlenecks = {
        'data_availability': {
            'description': 'High-quality text data is finite',
            'timeline': '2026-2028',
            'impact': 'May require synthetic data or multimodal training',
            'solutions': ['Data synthesis', 'Multimodal training', 'Better data filtering']
        },
        'compute_costs': {
            'description': 'Training costs growing exponentially',
            'timeline': '2025-2027',
            'impact': 'May limit model sizes to well-funded organizations',
            'solutions': ['Hardware efficiency improvements', 'Algorithmic advances', 'Model compression']
        },
        'hardware_limits': {
            'description': 'Physical limits of semiconductor scaling',
            'timeline': '2028-2030',
            'impact': 'Diminishing returns from hardware improvements',
            'solutions': ['New computing paradigms', 'Specialized AI chips', 'Quantum computing']
        },
        'energy_consumption': {
            'description': 'Environmental and infrastructure constraints',
            'timeline': '2026-2029',
            'impact': 'Regulatory and practical limits on power usage',
            'solutions': ['Efficient architectures', 'Renewable energy', 'Better cooling']
        },
        'algorithmic_efficiency': {
            'description': 'Need for more sample-efficient algorithms',
            'timeline': 'Ongoing',
            'impact': 'Current scaling may become suboptimal',
            'solutions': ['Better architectures', 'Meta-learning', 'Neurosymbolic approaches']
        }
    }
    
    print("Potential scaling bottlenecks:")
    print("=" * 60)
    
    for bottleneck, info in bottlenecks.items():
        print(f"\n{bottleneck.replace('_', ' ').title()}:")
        print(f"  Description: {info['description']}")
        print(f"  Timeline: {info['timeline']}")
        print(f"  Impact: {info['impact']}")
        print(f"  Potential solutions: {', '.join(info['solutions'])}")

# predict_future_capabilities()
# scaling_bottlenecks()
```

## Scaling laws in practice

### Model selection and training

```python
class PracticalScalingGuide:
    """Practical guide for applying scaling laws"""
    
    def __init__(self):
        self.param_scaling = ParameterScalingLaw()
        self.chinchilla = ChinchillaScaling()
    
    def budget_allocation(self, total_budget, cost_per_gpu_hour=3.2):
        """Allocate budget between model size and training time"""
        
        # Convert budget to compute hours
        available_gpu_hours = total_budget / cost_per_gpu_hour
        
        # Estimate compute FLOPs (assuming A100 GPUs)
        gpu_tflops = 312e12
        available_compute = available_gpu_hours * gpu_tflops * 3600
        
        # Get optimal allocation
        optimal_params, optimal_data = self.chinchilla.optimal_allocation(available_compute)
        
        # Calculate actual resource requirements
        training_time = calculate_training_compute(optimal_params, optimal_data) / (gpu_tflops * 3600)
        memory_per_gpu = (optimal_params * 16) / 1e9  # Mixed precision
        
        return {
            'recommended_parameters': optimal_params,
            'recommended_data_tokens': optimal_data,
            'training_gpu_hours': training_time,
            'estimated_cost': training_time * cost_per_gpu_hour,
            'memory_per_gpu_gb': memory_per_gpu,
            'predicted_performance': self.param_scaling.predict_loss(optimal_params)
        }
    
    def incremental_scaling_strategy(self, current_params, improvement_targets):
        """Design incremental scaling strategy"""
        
        current_loss = self.param_scaling.predict_loss(current_params)
        strategy = []
        
        for target_improvement in improvement_targets:
            target_loss = current_loss / target_improvement
            needed_params = self.param_scaling.parameters_for_loss(target_loss)
            
            # Check if this is achievable
            if needed_params == float('inf'):
                continue
            
            scale_factor = needed_params / current_params
            optimal_data = self.chinchilla.optimal_dataset_size(needed_params)
            
            strategy.append({
                'improvement_factor': target_improvement,
                'target_loss': target_loss,
                'required_parameters': needed_params,
                'scaling_factor': scale_factor,
                'optimal_data': optimal_data,
                'feasible': scale_factor < 100  # Practical scaling limit
            })
        
        return strategy
    
    def compare_scaling_approaches(self, compute_budget):
        """Compare different approaches to using compute budget"""
        
        approaches = {}
        
        # 1. Chinchilla optimal
        opt_params, opt_data = self.chinchilla.optimal_allocation(compute_budget)
        approaches['chinchilla_optimal'] = {
            'parameters': opt_params,
            'data': opt_data,
            'loss': self.param_scaling.predict_loss(opt_params),
            'description': 'Optimal parameter-data balance'
        }
        
        # 2. Parameter-heavy (like GPT-3)
        param_heavy_params = opt_params * 4
        param_heavy_data = opt_data / 4
        approaches['parameter_heavy'] = {
            'parameters': param_heavy_params,
            'data': param_heavy_data,
            'loss': self.param_scaling.predict_loss(param_heavy_params),
            'description': 'Larger model, less training data'
        }
        
        # 3. Data-heavy 
        data_heavy_params = opt_params / 2
        data_heavy_data = opt_data * 2
        approaches['data_heavy'] = {
            'parameters': data_heavy_params,
            'data': data_heavy_data,
            'loss': self.param_scaling.predict_loss(data_heavy_params),
            'description': 'Smaller model, more training data'
        }
        
        return approaches

# Example usage
guide = PracticalScalingGuide()

# Budget allocation example
budget = 1_000_000  # $1M budget
allocation = guide.budget_allocation(budget)

print("Budget allocation for $1M:")
print(f"Recommended model size: {allocation['recommended_parameters']:.2e} parameters")
print(f"Training data: {allocation['recommended_data_tokens']:.2e} tokens")
print(f"Predicted loss: {allocation['predicted_performance']:.3f}")
print(f"Estimated cost: ${allocation['estimated_cost']:,.0f}")

# Compare approaches
compute_budget = 1e24
approaches = guide.compare_scaling_approaches(compute_budget)

print(f"\nComparison of scaling approaches (compute budget: {compute_budget:.1e}):")
for name, approach in approaches.items():
    print(f"\n{name.replace('_', ' ').title()}:")
    print(f"  Parameters: {approach['parameters']:.2e}")
    print(f"  Data: {approach['data']:.2e} tokens")
    print(f"  Predicted loss: {approach['loss']:.3f}")
    print(f"  Description: {approach['description']}")
```

Scaling laws have fundamentally changed how we approach building AI systems. They provide a mathematical framework for understanding the relationships between model size, data, compute, and performance, enabling more informed decisions about resource allocation and model design.

Understanding these laws is crucial for anyone working with large language models, as they inform everything from research directions to commercial product decisions. While scaling laws have limitations and may eventually break down, they currently provide our best framework for predicting and planning AI development.

## Next steps

<Card
  title="Model families"
  icon="sitemap"
  href="/llms/model-families"
>
  Explore how different model architectures implement scaling strategies.
</Card>

<Card
  title="Training infrastructure"
  icon="server"
  href="/training/distributed-training"
>
  Learn about the infrastructure needed to scale model training.
</Card>

<Card
  title="Pre-training process"
  icon="graduation-cap"
  href="/llms/pretraining"
>
  Understand how scaling laws apply to the training process.
</Card>