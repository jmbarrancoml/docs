---
title: "Model families"
description: "Comparing different large language model architectures and their design philosophies"
---

## Introduction to model families

Large language models come in distinct architectural families, each with different design philosophies, strengths, and applications. Understanding these families helps in choosing the right model for specific tasks and provides insight into the evolution of language model architectures.

## Major architectural paradigms

### Decoder-only models (GPT family)
Generate text autoregressively, predicting the next token given previous context.

### Encoder-only models (BERT family)  
Process bidirectional context to understand and classify text.

### Encoder-decoder models (T5 family)
Transform input sequences to output sequences, ideal for translation and summarization.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ModelArchitectureComparison:
    """Compare different model architectures"""
    
    @staticmethod
    def decoder_only_forward(input_ids, model):
        """GPT-style autoregressive generation"""
        # Causal mask - each position only sees previous positions
        seq_len = input_ids.size(1)
        causal_mask = torch.tril(torch.ones(seq_len, seq_len))
        
        # Forward pass with causal attention
        hidden_states = model.embed_tokens(input_ids)
        
        for layer in model.layers:
            hidden_states = layer(hidden_states, attention_mask=causal_mask)
        
        # Language modeling head
        logits = model.lm_head(hidden_states)
        return logits
    
    @staticmethod
    def encoder_only_forward(input_ids, model):
        """BERT-style bidirectional encoding"""
        # Bidirectional attention - all positions can see all positions
        attention_mask = torch.ones_like(input_ids)
        
        # Forward pass with bidirectional attention
        hidden_states = model.embed_tokens(input_ids)
        
        for layer in model.layers:
            hidden_states = layer(hidden_states, attention_mask=attention_mask)
        
        # Classification head (for [CLS] token)
        cls_output = hidden_states[:, 0]  # First token
        logits = model.classifier(cls_output)
        return logits
    
    @staticmethod
    def encoder_decoder_forward(input_ids, decoder_input_ids, model):
        """T5-style sequence-to-sequence"""
        # Encoder: bidirectional attention
        encoder_outputs = model.encoder(input_ids)
        
        # Decoder: causal attention + cross-attention to encoder
        decoder_outputs = model.decoder(
            decoder_input_ids, 
            encoder_hidden_states=encoder_outputs
        )
        
        logits = model.lm_head(decoder_outputs)
        return logits
```

## GPT family: decoder-only models

### GPT-1 (2018): proof of concept
- **Parameters**: 117M
- **Innovation**: Demonstrated unsupervised pre-training + supervised fine-tuning
- **Architecture**: 12 layers, 768 hidden dimensions, 12 attention heads

```python
class GPT1Architecture(nn.Module):
    def __init__(self, vocab_size=40478, n_positions=512, n_ctx=512, n_embd=768, n_layer=12, n_head=12):
        super().__init__()
        self.n_embd = n_embd
        
        # Token and position embeddings
        self.wte = nn.Embedding(vocab_size, n_embd)  # Token embeddings
        self.wpe = nn.Embedding(n_positions, n_embd)  # Position embeddings
        
        # Transformer blocks
        self.h = nn.ModuleList([
            TransformerBlock(n_embd, n_head) for _ in range(n_layer)
        ])
        
        # Layer normalization and language modeling head
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
        
    def forward(self, input_ids):
        batch_size, seq_len = input_ids.shape
        
        # Create position IDs
        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)
        
        # Embeddings
        token_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = token_embeds + position_embeds
        
        # Transformer blocks
        for block in self.h:
            hidden_states = block(hidden_states)
        
        # Final layer norm and projection
        hidden_states = self.ln_f(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return logits

class TransformerBlock(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        self.attn = MultiHeadAttention(n_embd, n_head)
        self.mlp = MLP(n_embd)
        self.ln_1 = nn.LayerNorm(n_embd)
        self.ln_2 = nn.LayerNorm(n_embd)
        
    def forward(self, x):
        # Attention with residual connection
        attn_output = self.attn(self.ln_1(x))
        x = x + attn_output
        
        # MLP with residual connection
        mlp_output = self.mlp(self.ln_2(x))
        x = x + mlp_output
        
        return x
```

### GPT-2 (2019): scaling breakthrough
- **Parameters**: 124M, 355M, 774M, 1.5B variants
- **Innovation**: Showed that larger models could generate coherent long-form text
- **Key insight**: Scale alone dramatically improves performance

```python
class GPT2Improvements:
    """Key improvements in GPT-2 over GPT-1"""
    
    @staticmethod
    def modified_initialization():
        """GPT-2 used modified weight initialization"""
        def init_weights(module):
            if isinstance(module, (nn.Linear, nn.Embedding)):
                # Modified initialization with smaller standard deviation
                module.weight.data.normal_(mean=0.0, std=0.02)
                if isinstance(module, nn.Linear) and module.bias is not None:
                    module.bias.data.zero_()
        return init_weights
    
    @staticmethod
    def layer_norm_positioning():
        """GPT-2 moved layer norm to the beginning of each sub-block"""
        # Pre-norm architecture (more stable training)
        # x = x + attention(layer_norm(x))
        # x = x + mlp(layer_norm(x))
        pass
    
    @staticmethod
    def vocabulary_improvements():
        """GPT-2 used byte-pair encoding (BPE) tokenization"""
        # Handles any text without unknown tokens
        # More efficient than word-level tokenization
        pass

def analyze_gpt2_scaling():
    """Analyze how GPT-2 performance scaled with model size"""
    gpt2_variants = {
        'GPT-2 Small': {'params': 124e6, 'layers': 12, 'hidden': 768, 'heads': 12},
        'GPT-2 Medium': {'params': 355e6, 'layers': 24, 'hidden': 1024, 'heads': 16},
        'GPT-2 Large': {'params': 774e6, 'layers': 36, 'hidden': 1280, 'heads': 20},
        'GPT-2 XL': {'params': 1.5e9, 'layers': 48, 'hidden': 1600, 'heads': 25}
    }
    
    # Approximate performance improvements (perplexity on WebText)
    performance = {
        'GPT-2 Small': 35.0,
        'GPT-2 Medium': 26.5,
        'GPT-2 Large': 22.1,
        'GPT-2 XL': 18.5
    }
    
    print("GPT-2 scaling analysis:")
    print("=" * 50)
    
    for model, config in gpt2_variants.items():
        perf = performance[model]
        print(f"\n{model}:")
        print(f"  Parameters: {config['params']:.0e}")
        print(f"  Architecture: {config['layers']} layers, {config['hidden']} hidden, {config['heads']} heads")
        print(f"  Perplexity: {perf:.1f}")
        
        if model != 'GPT-2 Small':
            base_perf = performance['GPT-2 Small']
            improvement = base_perf / perf
            param_ratio = config['params'] / gpt2_variants['GPT-2 Small']['params']
            print(f"  Performance improvement: {improvement:.2f}x")
            print(f"  Parameter increase: {param_ratio:.1f}x")

# analyze_gpt2_scaling()
```

### GPT-3 (2020): emergent capabilities
- **Parameters**: 175B
- **Innovation**: In-context learning without parameter updates
- **Architecture**: 96 layers, 12,288 hidden dimensions, 96 attention heads

```python
class GPT3Architecture(nn.Module):
    """GPT-3 architecture with key improvements"""
    
    def __init__(self, vocab_size=50257, n_positions=2048, n_embd=12288, n_layer=96, n_head=96):
        super().__init__()
        
        # Massive scale compared to GPT-2
        self.wte = nn.Embedding(vocab_size, n_embd)
        self.wpe = nn.Embedding(n_positions, n_embd)
        
        # 96 transformer blocks
        self.h = nn.ModuleList([
            GPT3Block(n_embd, n_head) for _ in range(n_layer)
        ])
        
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)
        
    def forward(self, input_ids, past_key_values=None):
        # Support for key-value caching for efficient generation
        batch_size, seq_len = input_ids.shape
        
        if past_key_values is None:
            past_length = 0
            past_key_values = [None] * len(self.h)
        else:
            past_length = past_key_values[0][0].size(-2)
        
        position_ids = torch.arange(past_length, seq_len + past_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)
        
        # Embeddings
        hidden_states = self.wte(input_ids) + self.wpe(position_ids)
        
        # Transformer blocks with caching
        presents = []
        for i, (block, past_kv) in enumerate(zip(self.h, past_key_values)):
            hidden_states, present = block(hidden_states, past_key_value=past_kv)
            presents.append(present)
        
        hidden_states = self.ln_f(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return logits, presents

class GPT3Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        self.attn = MultiHeadAttentionWithCache(n_embd, n_head)
        self.mlp = GPT3MLP(n_embd)
        self.ln_1 = nn.LayerNorm(n_embd)
        self.ln_2 = nn.LayerNorm(n_embd)
        
    def forward(self, x, past_key_value=None):
        attn_output, present = self.attn(self.ln_1(x), past_key_value=past_key_value)
        x = x + attn_output
        
        mlp_output = self.mlp(self.ln_2(x))
        x = x + mlp_output
        
        return x, present

class GPT3MLP(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        # GPT-3 uses 4x expansion in MLP
        self.c_fc = nn.Linear(n_embd, 4 * n_embd)
        self.c_proj = nn.Linear(4 * n_embd, n_embd)
        self.act = nn.GELU()  # GELU activation instead of ReLU
        
    def forward(self, x):
        x = self.c_fc(x)
        x = self.act(x)
        x = self.c_proj(x)
        return x

def demonstrate_incontext_learning():
    """Demonstrate GPT-3's in-context learning capabilities"""
    
    # Few-shot prompting examples
    examples = {
        'translation': {
            'prompt': '''English: Hello, how are you?
French: Bonjour, comment allez-vous?

English: What is your name?
French: Comment vous appelez-vous?

English: I love programming.
French:''',
            'capability': 'Translation without training on translation data'
        },
        
        'arithmetic': {
            'prompt': '''Q: What is 15 + 25?
A: 40

Q: What is 33 - 17?
A: 16

Q: What is 12 + 28?
A:''',
            'capability': 'Mathematical reasoning from examples'
        },
        
        'classification': {
            'prompt': '''Tweet: "I love this movie! Amazing plot and great acting."
Sentiment: Positive

Tweet: "Terrible service, would not recommend."
Sentiment: Negative

Tweet: "The weather is nice today, perfect for a walk."
Sentiment:''',
            'capability': 'Text classification from examples'
        }
    }
    
    print("GPT-3 in-context learning examples:")
    print("=" * 50)
    
    for task, example in examples.items():
        print(f"\n{task.title()}:")
        print(f"Capability: {example['capability']}")
        print(f"Example prompt:\n{example['prompt']}")
        print("---")
```

### GPT-4 (2023): multimodal reasoning
- **Parameters**: ~1.8T (estimated, mixture of experts)
- **Innovation**: Multimodal capabilities, improved reasoning
- **Architecture**: Rumored to use mixture of experts (MoE)

```python
class MixtureOfExperts(nn.Module):
    """Simplified MoE implementation (GPT-4 style)"""
    
    def __init__(self, n_embd, num_experts=8, top_k=2, expert_capacity=None):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.expert_capacity = expert_capacity
        
        # Gating network
        self.gate = nn.Linear(n_embd, num_experts)
        
        # Expert networks
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(n_embd, 4 * n_embd),
                nn.GELU(),
                nn.Linear(4 * n_embd, n_embd)
            ) for _ in range(num_experts)
        ])
        
    def forward(self, x):
        batch_size, seq_len, n_embd = x.shape
        x_flat = x.view(-1, n_embd)
        
        # Compute gating scores
        gate_logits = self.gate(x_flat)
        gate_probs = F.softmax(gate_logits, dim=-1)
        
        # Select top-k experts
        top_k_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)
        
        # Initialize output
        output = torch.zeros_like(x_flat)
        
        # Route tokens to experts
        for i in range(self.num_experts):
            # Find tokens assigned to this expert
            expert_mask = (top_k_indices == i).any(dim=-1)
            
            if expert_mask.any():
                expert_input = x_flat[expert_mask]
                expert_output = self.experts[i](expert_input)
                
                # Weight by gating probability
                expert_weights = gate_probs[expert_mask, i].unsqueeze(-1)
                output[expert_mask] += expert_weights * expert_output
        
        return output.view(batch_size, seq_len, n_embd)

def analyze_gpt4_capabilities():
    """Analyze GPT-4's advanced capabilities"""
    
    capabilities = {
        'multimodal_reasoning': {
            'description': 'Process and reason about images and text together',
            'examples': ['Image captioning', 'Visual question answering', 'Chart analysis']
        },
        'improved_reasoning': {
            'description': 'Better logical reasoning and problem-solving',
            'examples': ['Mathematical proofs', 'Legal reasoning', 'Scientific analysis']
        },
        'code_generation': {
            'description': 'Generate and debug complex code',
            'examples': ['Full applications', 'Algorithm implementation', 'Code explanation']
        },
        'instruction_following': {
            'description': 'More reliable adherence to complex instructions',
            'examples': ['Multi-step tasks', 'Role-playing', 'Constraint satisfaction']
        },
        'reduced_hallucination': {
            'description': 'More factually accurate responses',
            'examples': ['Admits uncertainty', 'Cites sources', 'Avoids false claims']
        }
    }
    
    print("GPT-4 advanced capabilities:")
    print("=" * 50)
    
    for capability, info in capabilities.items():
        print(f"\n{capability.replace('_', ' ').title()}:")
        print(f"Description: {info['description']}")
        print(f"Examples: {', '.join(info['examples'])}")
```

## BERT family: encoder-only models

### BERT (2018): bidirectional revolution
- **Innovation**: Bidirectional training with masked language modeling
- **Architecture**: Encoder-only transformer
- **Training**: Masked LM + Next Sentence Prediction

```python
class BERTArchitecture(nn.Module):
    """BERT encoder-only architecture"""
    
    def __init__(self, vocab_size=30522, hidden_size=768, num_layers=12, num_heads=12, 
                 max_position_embeddings=512, type_vocab_size=2):
        super().__init__()
        
        # Embeddings
        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)
        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)
        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)
        self.LayerNorm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(0.1)
        
        # Transformer encoder layers
        self.encoder_layers = nn.ModuleList([
            BERTLayer(hidden_size, num_heads) for _ in range(num_layers)
        ])
        
        # Task-specific heads
        self.pooler = nn.Linear(hidden_size, hidden_size)  # For [CLS] token
        self.mlm_head = nn.Linear(hidden_size, vocab_size)  # Masked LM
        self.nsp_head = nn.Linear(hidden_size, 2)  # Next sentence prediction
        
    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):
        seq_length = input_ids.size(1)
        
        if position_ids is None:
            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
            
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        
        # Embeddings
        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        
        embeddings = words_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        
        # Encoder layers
        hidden_states = embeddings
        for layer in self.encoder_layers:
            hidden_states = layer(hidden_states, attention_mask)
        
        # Pooled output for classification (CLS token)
        pooled_output = torch.tanh(self.pooler(hidden_states[:, 0]))
        
        return {
            'last_hidden_state': hidden_states,
            'pooler_output': pooled_output
        }

class BERTLayer(nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.attention = MultiHeadAttention(hidden_size, num_heads)
        self.intermediate = nn.Linear(hidden_size, hidden_size * 4)
        self.output = nn.Linear(hidden_size * 4, hidden_size)
        self.LayerNorm1 = nn.LayerNorm(hidden_size)
        self.LayerNorm2 = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, hidden_states, attention_mask=None):
        # Self-attention
        attention_output = self.attention(hidden_states, hidden_states, hidden_states, attention_mask)
        attention_output = self.dropout(attention_output)
        attention_output = self.LayerNorm1(hidden_states + attention_output)
        
        # Feed-forward
        intermediate_output = F.gelu(self.intermediate(attention_output))
        layer_output = self.output(intermediate_output)
        layer_output = self.dropout(layer_output)
        layer_output = self.LayerNorm2(attention_output + layer_output)
        
        return layer_output

def bert_pretraining_tasks():
    """Demonstrate BERT's pre-training objectives"""
    
    class MaskedLanguageModel:
        def __init__(self, mask_prob=0.15):
            self.mask_prob = mask_prob
            
        def mask_tokens(self, tokens, vocab_size, mask_token_id, special_tokens):
            """Apply MLM masking strategy"""
            labels = tokens.clone()
            
            # Create random masking probability
            probability_matrix = torch.full(tokens.shape, self.mask_prob)
            
            # Don't mask special tokens
            special_tokens_mask = torch.isin(tokens, torch.tensor(special_tokens))
            probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
            
            masked_indices = torch.bernoulli(probability_matrix).bool()
            labels[~masked_indices] = -100  # Only compute loss on masked tokens
            
            # 80% of the time, replace with [MASK]
            indices_replaced = torch.bernoulli(torch.full(tokens.shape, 0.8)).bool() & masked_indices
            tokens[indices_replaced] = mask_token_id
            
            # 10% of the time, replace with random token
            indices_random = torch.bernoulli(torch.full(tokens.shape, 0.5)).bool() & masked_indices & ~indices_replaced
            random_words = torch.randint(vocab_size, tokens.shape, dtype=torch.long)
            tokens[indices_random] = random_words[indices_random]
            
            # 10% of the time, keep original token
            
            return tokens, labels
    
    class NextSentencePrediction:
        def __init__(self):
            pass
            
        def create_nsp_examples(self, sentences):
            """Create next sentence prediction examples"""
            examples = []
            
            for i in range(len(sentences) - 1):
                # 50% of the time, use actual next sentence (label = 1)
                if torch.rand(1) > 0.5:
                    sentence_a = sentences[i]
                    sentence_b = sentences[i + 1]
                    label = 1
                else:
                    # 50% of the time, use random sentence (label = 0)
                    sentence_a = sentences[i]
                    random_idx = torch.randint(0, len(sentences), (1,)).item()
                    sentence_b = sentences[random_idx]
                    label = 0
                
                examples.append((sentence_a, sentence_b, label))
            
            return examples
    
    return MaskedLanguageModel(), NextSentencePrediction()
```

### RoBERTa (2019): optimized BERT
- **Innovation**: Removed NSP, dynamic masking, larger batches
- **Key insight**: BERT was undertrained

```python
class RoBERTaImprovements:
    """Key improvements in RoBERTa over BERT"""
    
    @staticmethod
    def dynamic_masking():
        """RoBERTa uses dynamic masking instead of static"""
        # BERT: mask tokens once during preprocessing
        # RoBERTa: generate new masks for each epoch
        pass
    
    @staticmethod
    def remove_nsp():
        """RoBERTa removed Next Sentence Prediction"""
        # Found NSP task didn't help downstream performance
        # Simplified to just Masked Language Modeling
        pass
    
    @staticmethod
    def training_improvements():
        """RoBERTa training optimizations"""
        improvements = {
            'batch_size': '8x larger batches (8K vs 1K)',
            'training_data': '10x more data (160GB vs 16GB)',
            'training_steps': 'Longer training (500K vs 1M steps)',
            'learning_rate': 'Different learning rate schedule',
            'text_encoding': 'Byte-level BPE instead of character-level'
        }
        return improvements

def compare_bert_variants():
    """Compare different BERT family models"""
    
    bert_family = {
        'BERT-Base': {
            'params': 110e6,
            'layers': 12,
            'hidden': 768,
            'heads': 12,
            'innovations': ['Bidirectional training', 'MLM + NSP']
        },
        'RoBERTa-Base': {
            'params': 125e6,
            'layers': 12,
            'hidden': 768,
            'heads': 12,
            'innovations': ['Dynamic masking', 'No NSP', 'Longer training']
        },
        'DeBERTa-Base': {
            'params': 139e6,
            'layers': 12,
            'hidden': 768,
            'heads': 12,
            'innovations': ['Disentangled attention', 'Enhanced mask decoder']
        },
        'ALBERT-Base': {
            'params': 12e6,  # Parameter sharing
            'layers': 12,
            'hidden': 768,
            'heads': 12,
            'innovations': ['Parameter sharing', 'Factorized embeddings', 'SOP task']
        }
    }
    
    print("BERT family comparison:")
    print("=" * 60)
    
    for model, config in bert_family.items():
        print(f"\n{model}:")
        print(f"  Parameters: {config['params']:.0e}")
        print(f"  Architecture: {config['layers']} layers, {config['hidden']} hidden, {config['heads']} heads")
        print(f"  Key innovations: {', '.join(config['innovations'])}")
```

## T5 family: encoder-decoder models

### T5 (2019): text-to-text transfer transformer
- **Innovation**: All tasks framed as text-to-text problems
- **Architecture**: Encoder-decoder transformer
- **Training**: Span corruption objective

```python
class T5Architecture(nn.Module):
    """T5 encoder-decoder architecture"""
    
    def __init__(self, vocab_size=32128, d_model=512, d_ff=2048, num_layers=6, num_heads=8):
        super().__init__()
        
        # Shared embeddings
        self.shared = nn.Embedding(vocab_size, d_model)
        
        # Encoder
        self.encoder = T5Stack(d_model, d_ff, num_layers, num_heads, is_decoder=False)
        
        # Decoder
        self.decoder = T5Stack(d_model, d_ff, num_layers, num_heads, is_decoder=True)
        
        # Language modeling head
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
    def forward(self, input_ids, decoder_input_ids, attention_mask=None, decoder_attention_mask=None):
        # Encoder
        encoder_outputs = self.encoder(
            self.shared(input_ids),
            attention_mask=attention_mask
        )
        
        # Decoder
        decoder_outputs = self.decoder(
            self.shared(decoder_input_ids),
            encoder_hidden_states=encoder_outputs,
            attention_mask=decoder_attention_mask,
            encoder_attention_mask=attention_mask
        )
        
        # Language modeling head
        logits = self.lm_head(decoder_outputs)
        
        return logits

class T5Stack(nn.Module):
    def __init__(self, d_model, d_ff, num_layers, num_heads, is_decoder=False):
        super().__init__()
        self.is_decoder = is_decoder
        
        self.layers = nn.ModuleList([
            T5Block(d_model, d_ff, num_heads, is_decoder) for _ in range(num_layers)
        ])
        
        self.final_layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None):
        for layer in self.layers:
            hidden_states = layer(
                hidden_states,
                attention_mask=attention_mask,
                encoder_hidden_states=encoder_hidden_states,
                encoder_attention_mask=encoder_attention_mask
            )
        
        hidden_states = self.final_layer_norm(hidden_states)
        return hidden_states

class T5Block(nn.Module):
    def __init__(self, d_model, d_ff, num_heads, is_decoder=False):
        super().__init__()
        self.is_decoder = is_decoder
        
        # Self-attention
        self.layer = nn.ModuleList([T5LayerSelfAttention(d_model, num_heads)])
        
        # Cross-attention (decoder only)
        if is_decoder:
            self.layer.append(T5LayerCrossAttention(d_model, num_heads))
        
        # Feed-forward
        self.layer.append(T5LayerFF(d_model, d_ff))
        
    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None):
        # Self-attention
        hidden_states = self.layer[0](hidden_states, attention_mask)
        
        # Cross-attention (decoder only)
        if self.is_decoder and encoder_hidden_states is not None:
            hidden_states = self.layer[1](hidden_states, encoder_hidden_states, encoder_attention_mask)
        
        # Feed-forward
        layer_idx = 2 if self.is_decoder else 1
        hidden_states = self.layer[layer_idx](hidden_states)
        
        return hidden_states

def t5_pretraining_objective():
    """Demonstrate T5's span corruption objective"""
    
    class SpanCorruption:
        def __init__(self, corruption_rate=0.15, mean_span_length=3):
            self.corruption_rate = corruption_rate
            self.mean_span_length = mean_span_length
            
        def corrupt_spans(self, tokens):
            """Apply span corruption to tokens"""
            seq_len = len(tokens)
            num_corrupt = int(seq_len * self.corruption_rate)
            
            # Randomly select spans to corrupt
            corrupt_spans = []
            start_positions = torch.randperm(seq_len)[:num_corrupt]
            
            for start in start_positions:
                # Random span length (Poisson distribution)
                span_length = max(1, torch.poisson(torch.tensor(float(self.mean_span_length))).item())
                end = min(start + span_length, seq_len)
                corrupt_spans.append((start, end))
            
            # Create input and target
            input_tokens = tokens.clone()
            target_tokens = []
            sentinel_id = 32000  # Special sentinel token
            
            for i, (start, end) in enumerate(corrupt_spans):
                # Replace span with sentinel token
                input_tokens[start:end] = sentinel_id + i
                
                # Add to target
                target_tokens.extend([sentinel_id + i] + tokens[start:end].tolist())
            
            return input_tokens, torch.tensor(target_tokens)
    
    # Example
    original_text = "The quick brown fox jumps over the lazy dog"
    tokens = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Simplified tokenization
    
    span_corruptor = SpanCorruption()
    input_tokens, target_tokens = span_corruptor.corrupt_spans(tokens)
    
    print("T5 span corruption example:")
    print(f"Original: {tokens}")
    print(f"Input: {input_tokens}")
    print(f"Target: {target_tokens}")

def t5_task_formatting():
    """Show how T5 formats different tasks"""
    
    task_examples = {
        'translation': {
            'input': 'translate English to German: That is good.',
            'target': 'Das ist gut.'
        },
        'summarization': {
            'input': 'summarize: ' + 'Long article text...',
            'target': 'Brief summary of the article.'
        },
        'classification': {
            'input': 'cola sentence: The course is jumping well.',
            'target': 'unacceptable'
        },
        'question_answering': {
            'input': 'question: What is the capital of France? context: Paris is the capital...',
            'target': 'Paris'
        }
    }
    
    print("T5 task formatting:")
    print("=" * 40)
    
    for task, example in task_examples.items():
        print(f"\n{task.title()}:")
        print(f"Input: {example['input']}")
        print(f"Target: {example['target']}")
```

## Specialized model families

### Code generation models

```python
class CodeGenerationModels:
    """Analysis of code-specialized language models"""
    
    @staticmethod
    def codex_architecture():
        """Codex (GitHub Copilot) - GPT-3 fine-tuned on code"""
        return {
            'base_model': 'GPT-3',
            'specialization': 'Fine-tuned on GitHub repositories',
            'capabilities': ['Code completion', 'Code generation', 'Code explanation'],
            'training_data': 'GitHub public repositories, documentation'
        }
    
    @staticmethod
    def code_t5():
        """CodeT5 - T5 adapted for code understanding and generation"""
        return {
            'base_model': 'T5',
            'specialization': 'Code-aware tokenization and objectives',
            'capabilities': ['Code summarization', 'Bug fixing', 'Code translation'],
            'innovations': ['Identifier-aware attention', 'Bimodal dual generation']
        }
    
    @staticmethod
    def incoder():
        """InCoder - bidirectional code generation"""
        return {
            'base_model': 'GPT-style decoder',
            'specialization': 'Left-to-right and right-to-left generation',
            'capabilities': ['Code infilling', 'Bidirectional completion'],
            'innovations': ['Causal masking with bidirectional context']
        }

class CodeT5Example(nn.Module):
    """Simplified CodeT5 with code-aware features"""
    
    def __init__(self, vocab_size, d_model, num_layers, num_heads):
        super().__init__()
        self.base_t5 = T5Architecture(vocab_size, d_model, num_layers, num_heads)
        
        # Code-specific components
        self.identifier_attention = IdentifierAwareAttention(d_model, num_heads)
        self.syntax_embeddings = nn.Embedding(100, d_model)  # For syntax tokens
        
    def forward(self, input_ids, syntax_ids=None, **kwargs):
        # Add syntax embeddings if available
        if syntax_ids is not None:
            syntax_embeds = self.syntax_embeddings(syntax_ids)
            # Incorporate into base model (simplified)
        
        return self.base_t5(input_ids, **kwargs)

class IdentifierAwareAttention(nn.Module):
    """Attention that gives special treatment to code identifiers"""
    
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.identifier_gate = nn.Linear(d_model, 1)
        
    def forward(self, hidden_states, identifier_mask=None):
        # Standard attention
        attn_output = self.attention(hidden_states, hidden_states, hidden_states)
        
        # Identifier-aware gating
        if identifier_mask is not None:
            gate_scores = torch.sigmoid(self.identifier_gate(hidden_states))
            attn_output = attn_output * gate_scores
        
        return attn_output
```

### Scientific models

```python
class ScientificLanguageModels:
    """Models specialized for scientific domains"""
    
    @staticmethod
    def sciberta():
        """SciBERT - BERT trained on scientific papers"""
        return {
            'base_model': 'BERT',
            'training_data': 'Scientific papers from Semantic Scholar',
            'vocabulary': 'Scientific vocabulary (chemistry, biology, etc.)',
            'applications': ['Paper classification', 'Entity extraction', 'Citation analysis']
        }
    
    @staticmethod
    def biogpt():
        """BioGPT - GPT for biomedical domain"""
        return {
            'base_model': 'GPT',
            'training_data': 'Biomedical literature (PubMed abstracts)',
            'capabilities': ['Biomedical text generation', 'QA', 'Relation extraction'],
            'performance': 'State-of-the-art on biomedical NLP benchmarks'
        }
    
    @staticmethod
    def galactica():
        """Galactica - model for scientific knowledge"""
        return {
            'base_model': 'Transformer decoder',
            'training_data': 'Scientific papers, reference materials, knowledge bases',
            'capabilities': ['Scientific reasoning', 'Citation generation', 'Formula generation'],
            'innovations': ['Working memory tokens', 'Multi-step reasoning']
        }

class ScientificTransformer(nn.Module):
    """Transformer adapted for scientific text (EderSpark style)"""
    
    def __init__(self, vocab_size, d_model, num_layers, num_heads, num_scientific_entities=10000):
        super().__init__()
        
        # Base transformer
        self.transformer = GPT3Architecture(vocab_size, d_model, num_layers, num_heads)
        
        # Scientific entity embeddings
        self.entity_embeddings = nn.Embedding(num_scientific_entities, d_model)
        
        # Citation-aware attention
        self.citation_attention = CitationAwareAttention(d_model, num_heads)
        
        # Scientific reasoning head
        self.reasoning_head = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, 3)  # Entailment, contradiction, neutral
        )
        
    def forward(self, input_ids, entity_ids=None, citation_mask=None):
        # Base transformer forward
        hidden_states = self.transformer(input_ids)
        
        # Incorporate scientific entities
        if entity_ids is not None:
            entity_embeds = self.entity_embeddings(entity_ids)
            # Combine with hidden states (simplified)
            hidden_states = hidden_states + entity_embeds
        
        # Citation-aware processing
        if citation_mask is not None:
            hidden_states = self.citation_attention(hidden_states, citation_mask)
        
        # Scientific reasoning
        reasoning_logits = self.reasoning_head(hidden_states[:, 0])  # CLS token
        
        return hidden_states, reasoning_logits

class CitationAwareAttention(nn.Module):
    """Attention mechanism that handles citation relationships"""
    
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.citation_projection = nn.Linear(d_model, d_model)
        
    def forward(self, hidden_states, citation_mask=None):
        # Standard self-attention
        attn_output = self.attention(hidden_states, hidden_states, hidden_states)
        
        # Citation relationship modeling
        if citation_mask is not None:
            citation_features = self.citation_projection(hidden_states)
            # Apply citation mask to focus on relevant citations
            citation_attended = attn_output * citation_mask.unsqueeze(-1)
            attn_output = attn_output + citation_attended
        
        return attn_output
```

## Model comparison and selection

### Performance characteristics

```python
def compare_model_families():
    """Compare different model families across various dimensions"""
    
    comparison_matrix = {
        'GPT Family (Decoder-only)': {
            'strengths': ['Text generation', 'Few-shot learning', 'Creative writing'],
            'weaknesses': ['Understanding tasks', 'Bidirectional reasoning'],
            'best_for': ['Content generation', 'Conversational AI', 'Code completion'],
            'training_efficiency': 'High',
            'inference_speed': 'Fast (autoregressive)',
            'memory_usage': 'Moderate'
        },
        
        'BERT Family (Encoder-only)': {
            'strengths': ['Text understanding', 'Classification', 'Named entity recognition'],
            'weaknesses': ['Text generation', 'Long-form content'],
            'best_for': ['Classification', 'Information extraction', 'Search'],
            'training_efficiency': 'Moderate',
            'inference_speed': 'Very fast (parallel)',
            'memory_usage': 'Low'
        },
        
        'T5 Family (Encoder-decoder)': {
            'strengths': ['Versatility', 'Translation', 'Summarization'],
            'weaknesses': ['Complexity', 'Memory usage'],
            'best_for': ['Translation', 'Summarization', 'Multi-task learning'],
            'training_efficiency': 'Moderate',
            'inference_speed': 'Moderate',
            'memory_usage': 'High'
        }
    }
    
    print("Model family comparison:")
    print("=" * 80)
    
    for family, characteristics in comparison_matrix.items():
        print(f"\n{family}:")
        for aspect, details in characteristics.items():
            if isinstance(details, list):
                print(f"  {aspect.replace('_', ' ').title()}: {', '.join(details)}")
            else:
                print(f"  {aspect.replace('_', ' ').title()}: {details}")

def model_selection_guide():
    """Guide for selecting the right model family"""
    
    use_cases = {
        'content_generation': {
            'recommended': 'GPT family',
            'reasoning': 'Autoregressive generation, strong few-shot capabilities',
            'examples': ['Blog writing', 'Creative stories', 'Code generation']
        },
        
        'text_classification': {
            'recommended': 'BERT family',
            'reasoning': 'Bidirectional context, efficient for understanding tasks',
            'examples': ['Sentiment analysis', 'Spam detection', 'Topic classification']
        },
        
        'translation': {
            'recommended': 'T5 family',
            'reasoning': 'Encoder-decoder architecture ideal for seq2seq tasks',
            'examples': ['Language translation', 'Paraphrasing', 'Style transfer']
        },
        
        'question_answering': {
            'recommended': 'BERT or T5',
            'reasoning': 'BERT for extractive QA, T5 for generative QA',
            'examples': ['Reading comprehension', 'Knowledge base QA', 'Conversational QA']
        },
        
        'scientific_research': {
            'recommended': 'Domain-specific models',
            'reasoning': 'Specialized vocabulary and reasoning patterns',
            'examples': ['Paper analysis', 'Citation recommendation', 'Hypothesis generation']
        }
    }
    
    print("Model selection guide:")
    print("=" * 60)
    
    for use_case, recommendation in use_cases.items():
        print(f"\n{use_case.replace('_', ' ').title()}:")
        print(f"  Recommended: {recommendation['recommended']}")
        print(f"  Reasoning: {recommendation['reasoning']}")
        print(f"  Examples: {', '.join(recommendation['examples'])}")

# compare_model_families()
# model_selection_guide()
```

## Future directions

### Emerging architectures

```python
def emerging_architectures():
    """Survey of new architectural directions"""
    
    emerging_trends = {
        'mixture_of_experts': {
            'description': 'Conditional computation with expert routing',
            'benefits': ['Increased capacity without proportional compute increase'],
            'challenges': ['Load balancing', 'Training instability'],
            'examples': ['Switch Transformer', 'GLaM', 'PaLM-2']
        },
        
        'retrieval_augmented': {
            'description': 'Combine parametric and non-parametric memory',
            'benefits': ['Access to external knowledge', 'Factual accuracy'],
            'challenges': ['Retrieval quality', 'Integration complexity'],
            'examples': ['RAG', 'FiD', 'Atlas']
        },
        
        'multimodal_integration': {
            'description': 'Unified models for text, vision, audio',
            'benefits': ['Cross-modal reasoning', 'Unified interface'],
            'challenges': ['Modality alignment', 'Training complexity'],
            'examples': ['CLIP', 'DALL-E', 'GPT-4V']
        },
        
        'efficient_architectures': {
            'description': 'Reduce computational requirements',
            'benefits': ['Lower costs', 'Faster inference', 'Mobile deployment'],
            'challenges': ['Performance trade-offs', 'Optimization complexity'],
            'examples': ['MobileBERT', 'DistilBERT', 'Linformer']
        }
    }
    
    print("Emerging architectural trends:")
    print("=" * 60)
    
    for trend, info in emerging_trends.items():
        print(f"\n{trend.replace('_', ' ').title()}:")
        print(f"  Description: {info['description']}")
        print(f"  Benefits: {', '.join(info['benefits'])}")
        print(f"  Challenges: {', '.join(info['challenges'])}")
        print(f"  Examples: {', '.join(info['examples'])}")

def predict_future_developments():
    """Predictions for future model family evolution"""
    
    predictions = {
        '2024-2025': [
            'Multimodal models become standard',
            'Mixture of experts scales to trillion parameters',
            'Retrieval-augmented generation improves factuality'
        ],
        
        '2025-2027': [
            'Unified architectures for all modalities',
            'Efficient long-context models (1M+ tokens)',
            'Scientific reasoning models achieve expert-level performance'
        ],
        
        '2027-2030': [
            'Neurosymbolic architectures combine neural and symbolic reasoning',
            'Self-improving models that can modify their own architecture',
            'Quantum-neural hybrid models for specific domains'
        ]
    }
    
    print("Future model family predictions:")
    print("=" * 50)
    
    for timeframe, developments in predictions.items():
        print(f"\n{timeframe}:")
        for i, development in enumerate(developments, 1):
            print(f"  {i}. {development}")

# emerging_architectures()
# predict_future_developments()
```

Understanding different model families is crucial for selecting the right architecture for specific applications. Each family has distinct strengths that make them suitable for different types of tasks, from text generation to understanding to scientific reasoning.

The evolution of these families continues, with new architectural innovations emerging to address specific challenges like efficiency, multimodality, and specialized domain knowledge. For scientific applications like those at EderSpark, domain-specific models and retrieval-augmented approaches show particular promise for advancing research capabilities.

## Next steps

<Card
  title="Pre-training process"
  icon="graduation-cap"
  href="/llms/pretraining"
>
  Learn how these different model families are trained from scratch.
</Card>

<Card
  title="Evaluation methods"
  icon="chart-bar"
  href="/llms/evaluation-methods"
>
  Understand how to compare and evaluate different model families.
</Card>

<Card
  title="Scientific research applications"
  icon="flask"
  href="/applications/scientific-research"
>
  Explore how different model families excel in scientific domains.
</Card>