---
title: "Neural Networks"
description: "Understanding the building blocks of modern AI systems"
---

## Introduction to Neural Networks

Neural networks are computational models inspired by the human brain's structure and function. They form the foundation of modern deep learning systems and are responsible for breakthroughs in computer vision, natural language processing, and many other AI applications.

## The Biological Inspiration

### Neurons in the Brain
- **Structure**: Cell body, dendrites, axon, synapses
- **Function**: Receive signals, process information, transmit outputs
- **Learning**: Synaptic strength changes based on experience
- **Networks**: ~86 billion neurons with ~100 trillion connections

### From Biology to Mathematics
The artificial neuron abstracts key principles:
- **Input aggregation**: Combining multiple signals
- **Threshold activation**: Firing when stimulation exceeds a threshold  
- **Weighted connections**: Variable connection strengths
- **Parallel processing**: Multiple neurons operating simultaneously

## The Artificial Neuron (Perceptron)

### Mathematical Model
A single artificial neuron performs these operations:

1. **Weighted Sum**: Multiply inputs by weights and sum
2. **Bias Addition**: Add a bias term for flexibility
3. **Activation Function**: Apply non-linear transformation

```python
import numpy as np

def perceptron(inputs, weights, bias, activation_func):
    # Weighted sum
    weighted_sum = np.dot(inputs, weights) + bias
    
    # Apply activation function
    output = activation_func(weighted_sum)
    
    return output

# Example activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)
```

### Mathematical Notation
For a neuron with inputs $x_1, x_2, ..., x_n$:

$$y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)$$

Where:
- $w_i$ are the weights
- $b$ is the bias
- $f$ is the activation function
- $y$ is the output

## Activation Functions

### Why Non-linearity Matters
Without activation functions, neural networks would be limited to linear transformations. Non-linear activation functions enable:
- Complex pattern recognition
- Universal approximation capabilities
- Deep network architectures

### Common Activation Functions

#### 1. Sigmoid
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**Properties:**
- Output range: (0, 1)
- Smooth, differentiable
- Problems: Vanishing gradients, not zero-centered

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow
```

#### 2. Hyperbolic Tangent (tanh)
$$\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$

**Properties:**
- Output range: (-1, 1)
- Zero-centered
- Still suffers from vanishing gradients

#### 3. ReLU (Rectified Linear Unit)
$$\text{ReLU}(x) = \max(0, x)$$

**Properties:**
- Output range: [0, âˆž)
- Computationally efficient
- Helps with vanishing gradient problem
- Can suffer from "dying ReLU" problem

```python
def relu(x):
    return np.maximum(0, x)
```

#### 4. Leaky ReLU
$$\text{LeakyReLU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}$$

Where $\alpha$ is a small positive constant (typically 0.01).

#### 5. Swish/SiLU
$$\text{Swish}(x) = x \cdot \sigma(x)$$

**Properties:**
- Self-gated activation
- Smooth and non-monotonic
- Performs well in deep networks

## Multi-Layer Perceptrons (MLPs)

### Architecture
MLPs consist of:
- **Input Layer**: Receives data
- **Hidden Layer(s)**: Process information
- **Output Layer**: Produces final results

```python
class MLP:
    def __init__(self, layer_sizes):
        self.weights = []
        self.biases = []
        
        # Initialize weights and biases for each layer
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, x):
        activation = x
        
        # Forward pass through all layers
        for w, b in zip(self.weights, self.biases):
            z = np.dot(activation, w) + b
            activation = relu(z)  # Apply ReLU to hidden layers
        
        return activation
```

### Universal Approximation Theorem
**Theorem**: A neural network with:
- One hidden layer
- Finite number of neurons
- Non-linear activation functions

Can approximate any continuous function on a compact set to arbitrary accuracy.

**Implications:**
- Neural networks are theoretically powerful
- Practical considerations: depth vs. width trade-offs
- Deep networks often more efficient than wide networks

## Learning in Neural Networks

### The Learning Problem
Given training data $(x^{(i)}, y^{(i)})$ for $i = 1, 2, ..., m$:
- Find weights $W$ and biases $b$
- Minimize prediction error
- Generalize to unseen data

### Loss Functions

#### Mean Squared Error (Regression)
$$L = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2$$

#### Cross-Entropy (Classification)
For binary classification:
$$L = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)})]$$

### Gradient Descent
Minimize loss by iteratively updating parameters:

$$w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$$

Where $\alpha$ is the learning rate.

```python
def gradient_descent_step(weights, gradients, learning_rate):
    return weights - learning_rate * gradients
```

## Backpropagation Algorithm

### Chain Rule Foundation
Backpropagation applies the chain rule to compute gradients:

$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w_1}$$

### Forward Pass
1. Compute activations layer by layer
2. Store intermediate values for backward pass

### Backward Pass
1. Compute output layer gradients
2. Propagate gradients backward through network
3. Update weights using computed gradients

```python
def backpropagation(network, x, y_true):
    # Forward pass
    activations = [x]
    z_values = []
    
    for w, b in zip(network.weights, network.biases):
        z = np.dot(activations[-1], w) + b
        a = sigmoid(z)  # activation function
        z_values.append(z)
        activations.append(a)
    
    # Backward pass
    gradients_w = []
    gradients_b = []
    
    # Output layer error
    delta = (activations[-1] - y_true) * sigmoid_derivative(z_values[-1])
    
    # Propagate error backward
    for i in range(len(network.weights) - 1, -1, -1):
        # Compute gradients
        dw = np.dot(activations[i].T, delta)
        db = np.sum(delta, axis=0, keepdims=True)
        
        gradients_w.insert(0, dw)
        gradients_b.insert(0, db)
        
        # Propagate error to previous layer
        if i > 0:
            delta = np.dot(delta, network.weights[i].T) * sigmoid_derivative(z_values[i-1])
    
    return gradients_w, gradients_b
```

## Training Challenges and Solutions

### Vanishing Gradients
**Problem**: Gradients become very small in deep networks
**Solutions**:
- Better activation functions (ReLU, Leaky ReLU)
- Proper weight initialization
- Batch normalization
- Residual connections

### Exploding Gradients
**Problem**: Gradients become very large
**Solutions**:
- Gradient clipping
- Proper weight initialization
- Batch normalization

### Overfitting
**Problem**: Model memorizes training data, poor generalization
**Solutions**:
- Regularization (L1, L2)
- Dropout
- Early stopping
- Data augmentation

```python
# L2 regularization example
def l2_regularization(weights, lambda_reg):
    return lambda_reg * sum(np.sum(w**2) for w in weights)

# Dropout implementation
def dropout(x, keep_prob, training=True):
    if training:
        mask = np.random.rand(*x.shape) < keep_prob
        return x * mask / keep_prob
    return x
```

### Weight Initialization

#### Xavier/Glorot Initialization
For layers with $n_{in}$ inputs and $n_{out}$ outputs:

$$w \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in} + n_{out}}}\right)$$

#### He Initialization (for ReLU)
$$w \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in}}}\right)$$

```python
def xavier_init(n_in, n_out):
    limit = np.sqrt(6 / (n_in + n_out))
    return np.random.uniform(-limit, limit, (n_in, n_out))

def he_init(n_in, n_out):
    return np.random.randn(n_in, n_out) * np.sqrt(2 / n_in)
```

## Types of Neural Networks

### Feedforward Networks
- Information flows in one direction
- No cycles or loops
- Most common type for basic tasks

### Convolutional Neural Networks (CNNs)
- Designed for spatial data (images)
- Local connectivity and weight sharing
- Translation invariance

### Recurrent Neural Networks (RNNs)
- Process sequential data
- Have memory through hidden states
- Can handle variable-length inputs

### Transformer Networks
- Attention-based architecture
- Highly parallelizable
- State-of-the-art for language tasks

## Practical Implementation Tips

### Data Preprocessing
```python
def preprocess_data(X, y):
    # Normalize features
    X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    # One-hot encode labels if classification
    if len(y.shape) == 1:
        num_classes = len(np.unique(y))
        y_onehot = np.eye(num_classes)[y]
        return X_norm, y_onehot
    
    return X_norm, y
```

### Training Loop
```python
def train_network(network, X_train, y_train, epochs, learning_rate):
    for epoch in range(epochs):
        # Forward pass
        predictions = network.forward(X_train)
        
        # Compute loss
        loss = compute_loss(predictions, y_train)
        
        # Backward pass
        gradients_w, gradients_b = backpropagation(network, X_train, y_train)
        
        # Update weights
        for i in range(len(network.weights)):
            network.weights[i] -= learning_rate * gradients_w[i]
            network.biases[i] -= learning_rate * gradients_b[i]
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
```

## Modern Deep Networks

Neural networks have evolved far beyond the basic perceptron:

- **Depth**: Networks with hundreds of layers
- **Architecture**: Sophisticated designs (ResNet, Transformer, etc.)
- **Scale**: Billions of parameters
- **Applications**: Vision, language, speech, games, science

The principles covered here form the foundation for understanding these advanced architectures. The next step is exploring how these concepts scale to create the powerful AI systems we see today.

## Next Steps

<Card
  title="Deep Learning"
  icon="layers"
  href="/fundamentals/deep-learning"
>
  Explore advanced neural network architectures and training techniques.
</Card>

<Card
  title="Transformers"
  icon="shuffle"
  href="/fundamentals/transformers"
>
  Learn about the architecture powering modern language models.
</Card>

<Card
  title="Mathematical Foundations"
  icon="calculator"
  href="/fundamentals/mathematical-foundations"
>
  Deepen your understanding of the mathematics behind neural networks.
</Card>