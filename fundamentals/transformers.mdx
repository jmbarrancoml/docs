---
title: "Transformers"
description: "The architecture that revolutionized AI, powering modern language models and beyond"
---

## Introduction to Transformers

The Transformer architecture, introduced in "Attention Is All You Need" (Vaswani et al., 2017), represents one of the most significant breakthroughs in artificial intelligence. By relying entirely on attention mechanisms and eliminating recurrence and convolution, Transformers enabled the scaling that led to modern large language models and revolutionized not just natural language processing, but computer vision, multimodal AI, and scientific computing.

## The Revolution: Why Transformers Matter

### Before Transformers: Sequential Processing
Traditional sequence models processed data step by step:

```python
# RNN/LSTM sequential processing (simplified)
def rnn_process(sequence):
    hidden_state = initialize_hidden()
    outputs = []
    
    for token in sequence:  # Must process sequentially
        hidden_state = update_hidden(hidden_state, token)
        output = compute_output(hidden_state)
        outputs.append(output)
    
    return outputs
```

**Limitations:**
- **Sequential Dependency**: Each step depends on the previous
- **Limited Parallelization**: Cannot process all positions simultaneously
- **Vanishing Gradients**: Information loss over long sequences
- **Computational Inefficiency**: Slow training and inference

### Transformer Innovation: Parallel Processing
Transformers process all positions simultaneously:

```python
# Transformer parallel processing (conceptual)
def transformer_process(sequence):
    # All positions processed in parallel
    attention_output = self_attention(sequence, sequence, sequence)
    feed_forward_output = feed_forward(attention_output)
    return feed_forward_output
```

**Advantages:**
- **Parallelization**: All positions computed simultaneously
- **Long-Range Dependencies**: Direct connections between any positions
- **Scalability**: Efficient on modern hardware (GPUs/TPUs)
- **Interpretability**: Attention patterns reveal learned relationships

## Core Architecture Deep Dive

### Self-Attention: The Heart of Transformers

#### Intuitive Understanding
Self-attention allows each position to "look at" and gather information from all other positions in the sequence. Think of it as each word in a sentence simultaneously considering every other word to understand its meaning in context.

#### Mathematical Foundation
For input sequence $X \in \mathbb{R}^{n \times d}$:

1. **Create Query, Key, Value matrices:**
   $$Q = XW^Q, \quad K = XW^K, \quad V = XW^V$$

2. **Compute attention scores:**
   $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    def __init__(self, d_model, num_heads=8):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear projections for Q, K, V
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # 1. Create Q, K, V matrices
        Q = self.w_q(x)  # [batch_size, seq_len, d_model]
        K = self.w_k(x)
        V = self.w_v(x)
        
        # 2. Reshape for multi-head attention
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        # 3. Scaled dot-product attention
        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 4. Concatenate heads and project
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        return self.w_o(attention_output)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        
        # Apply attention to values
        output = torch.matmul(attention_weights, V)
        
        return output
```

#### Why Scaling by √d_k?
The scaling factor prevents the softmax from saturating:

```python
def demonstrate_scaling_importance():
    """Show why scaling matters in attention"""
    d_k = 64
    seq_len = 10
    
    # Generate random Q, K matrices
    Q = torch.randn(seq_len, d_k)
    K = torch.randn(seq_len, d_k)
    
    # Compute attention scores with and without scaling
    scores_unscaled = Q @ K.T
    scores_scaled = (Q @ K.T) / math.sqrt(d_k)
    
    print(f"Unscaled scores - Mean: {scores_unscaled.mean():.2f}, Std: {scores_unscaled.std():.2f}")
    print(f"Scaled scores - Mean: {scores_scaled.mean():.2f}, Std: {scores_scaled.std():.2f}")
    
    # After softmax
    softmax_unscaled = F.softmax(scores_unscaled, dim=-1)
    softmax_scaled = F.softmax(scores_scaled, dim=-1)
    
    print(f"Unscaled softmax entropy: {-(softmax_unscaled * torch.log(softmax_unscaled + 1e-10)).sum(-1).mean():.3f}")
    print(f"Scaled softmax entropy: {-(softmax_scaled * torch.log(softmax_scaled + 1e-10)).sum(-1).mean():.3f}")

demonstrate_scaling_importance()
```

### Multi-Head Attention: Multiple Perspectives

#### Concept
Instead of using a single attention function, multi-head attention runs multiple attention mechanisms in parallel, each learning different types of relationships.

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

Where: $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

#### What Different Heads Learn
Research has shown that different attention heads specialize in different patterns:

```python
def analyze_attention_patterns(model, text):
    """Analyze what different attention heads learn"""
    
    # Example patterns observed in real models:
    patterns = {
        "syntactic": "Subject-verb relationships, noun-adjective pairs",
        "semantic": "Word meanings, synonyms, antonyms",
        "positional": "First word, last word, relative positions", 
        "coreference": "Pronoun resolution, entity linking",
        "discourse": "Sentence boundaries, paragraph structure"
    }
    
    # In practice, you would:
    # 1. Run the model with attention output
    # 2. Analyze attention weights for each head
    # 3. Correlate patterns with linguistic/semantic structures
    
    return patterns
```

### Position Encoding: Teaching Order

Since Transformers process all positions simultaneously, they need explicit positional information.

#### Sinusoidal Position Encoding
The original approach uses sine and cosine functions:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length=5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length).unsqueeze(1).float()
        
        # Create div_term for scaling
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))
        
        # Apply sin to even indices
        pe[:, 0::2] = torch.sin(position * div_term)
        
        # Apply cos to odd indices  
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Register as buffer (not a parameter)
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]

def visualize_positional_encoding():
    """Visualize positional encoding patterns"""
    import matplotlib.pyplot as plt
    
    pe = PositionalEncoding(d_model=128, max_seq_length=100)
    
    # Get positional encodings
    dummy_input = torch.zeros(1, 100, 128)
    pos_encodings = pe.pe[0]  # Remove batch dimension
    
    # Plot first few dimensions
    plt.figure(figsize=(12, 8))
    for i in range(0, 8, 2):
        plt.subplot(2, 4, i//2 + 1)
        plt.plot(pos_encodings[:50, i].numpy(), label=f'dim {i}')
        plt.plot(pos_encodings[:50, i+1].numpy(), label=f'dim {i+1}')
        plt.legend()
        plt.title(f'Dimensions {i} and {i+1}')
    
    plt.tight_layout()
    plt.show()
```

#### Why This Encoding Works
1. **Uniqueness**: Each position has a unique encoding
2. **Relative Distances**: The encoding captures relative positions
3. **Extrapolation**: Can handle sequences longer than seen during training

#### Modern Alternatives

**Learned Positional Embeddings:**
```python
class LearnedPositionalEmbedding(nn.Module):
    def __init__(self, max_seq_length, d_model):
        super().__init__()
        self.embedding = nn.Embedding(max_seq_length, d_model)
    
    def forward(self, x):
        seq_len = x.size(1)
        positions = torch.arange(seq_len, device=x.device)
        return x + self.embedding(positions)
```

**Rotary Position Embedding (RoPE):**
Used in modern models like GPT-NeoX, LLaMA:

```python
class RotaryPositionalEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        
    def forward(self, x, seq_len):
        # Rotate embeddings based on position
        # Implementation details simplified for clarity
        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.dim, 2).float() / self.dim))
        t = torch.arange(seq_len).type_as(inv_freq)
        
        freqs = torch.einsum('i,j->ij', t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        
        return emb
```

### Feed-Forward Networks: Processing Information

Each Transformer layer includes a position-wise feed-forward network:

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ReLU()  # or GELU in modern models
        
    def forward(self, x):
        # First linear transformation and activation
        hidden = self.activation(self.linear1(x))
        hidden = self.dropout(hidden)
        
        # Second linear transformation
        output = self.linear2(hidden)
        
        return output
```

#### Modern Activation Functions

**GELU (Gaussian Error Linear Unit):**
$$\text{GELU}(x) = x \cdot \Phi(x)$$

Where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution.

```python
class GELU(nn.Module):
    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))

# PyTorch also provides: nn.GELU()
```

**SwiGLU (used in PaLM, LLaMA):**
```python
class SwiGLU(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_model, d_ff, bias=False)
        self.w3 = nn.Linear(d_ff, d_model, bias=False)
        
    def forward(self, x):
        return self.w3(F.silu(self.w1(x)) * self.w2(x))
```

### Layer Normalization and Residual Connections

#### Layer Normalization
Normalizes across the feature dimension:

$$\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta$$

Where:
- $\mu = \frac{1}{d}\sum_{i=1}^{d} x_i$ (mean)
- $\sigma = \sqrt{\frac{1}{d}\sum_{i=1}^{d}(x_i - \mu)^2}$ (standard deviation)

```python
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps
        
    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.gamma * (x - mean) / (std + self.eps) + self.beta
```

#### Pre-Norm vs Post-Norm
**Post-Norm (Original):**
```python
# Attention
x = x + attention(layer_norm(x))
# Feed-forward  
x = x + feed_forward(layer_norm(x))
```

**Pre-Norm (Modern):**
```python
# Attention
x = x + layer_norm(attention(x))
# Feed-forward
x = x + layer_norm(feed_forward(x))
```

Pre-norm generally provides more stable training for deep models.

## Complete Transformer Block

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, prenorm=True):
        super().__init__()
        self.attention = SelfAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.prenorm = prenorm
        
    def forward(self, x, mask=None):
        if self.prenorm:
            # Pre-norm version
            attn_output = self.attention(self.norm1(x), mask)
            x = x + self.dropout(attn_output)
            
            ff_output = self.feed_forward(self.norm2(x))
            x = x + self.dropout(ff_output)
        else:
            # Post-norm version
            attn_output = self.attention(x, mask)
            x = self.norm1(x + self.dropout(attn_output))
            
            ff_output = self.feed_forward(x)
            x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

## Transformer Variants

### Encoder-Only (BERT-style)

Used for understanding tasks like classification, named entity recognition:

```python
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
        
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, input_ids, attention_mask=None):
        # Token embeddings + positional encoding
        x = self.token_embedding(input_ids)
        x = self.positional_encoding(x)
        
        # Apply transformer blocks
        for block in self.transformer_blocks:
            x = block(x, attention_mask)
        
        # Final layer normalization
        x = self.layer_norm(x)
        
        return x
```

### Decoder-Only (GPT-style)

Used for generation tasks:

```python
class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
        
        self.layer_norm = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
    def forward(self, input_ids):
        batch_size, seq_len = input_ids.shape
        
        # Create causal mask (lower triangular)
        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
        causal_mask = causal_mask.to(input_ids.device)
        
        # Token embeddings + positional encoding
        x = self.token_embedding(input_ids)
        x = self.positional_encoding(x)
        
        # Apply transformer blocks with causal masking
        for block in self.transformer_blocks:
            x = block(x, causal_mask)
        
        # Final layer normalization and projection
        x = self.layer_norm(x)
        logits = self.lm_head(x)
        
        return logits
```

### Encoder-Decoder (T5-style)

Used for sequence-to-sequence tasks like translation:

```python
class TransformerEncoderDecoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff):
        super().__init__()
        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff)
        self.decoder = TransformerDecoder(vocab_size, d_model, num_heads, num_layers, d_ff)
        
    def forward(self, encoder_input_ids, decoder_input_ids):
        # Encode input sequence
        encoder_output = self.encoder(encoder_input_ids)
        
        # Decode with cross-attention to encoder output
        decoder_output = self.decoder(decoder_input_ids, encoder_output)
        
        return decoder_output
```

## Attention Patterns and Interpretability

### Visualizing Attention

```python
def visualize_attention(model, text, tokenizer):
    """Visualize attention patterns in a transformer model"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # Tokenize input
    tokens = tokenizer.encode(text)
    token_names = [tokenizer.decode([t]) for t in tokens]
    
    # Get attention weights from model
    with torch.no_grad():
        outputs = model(torch.tensor([tokens]))
        attention_weights = outputs.attentions  # Assuming model returns attention
    
    # Plot attention for first head of first layer
    attention = attention_weights[0][0, 0]  # [layer, batch, head, seq, seq]
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(attention.numpy(), 
                xticklabels=token_names,
                yticklabels=token_names,
                cmap='Blues')
    plt.title('Attention Pattern')
    plt.show()
```

### Common Attention Patterns

Research has identified several common patterns:

1. **Diagonal Pattern**: Adjacent token attention
2. **Vertical Lines**: Attention to specific tokens (like punctuation)
3. **Block Pattern**: Attention within sentences or phrases
4. **Broadcast Pattern**: All tokens attending to one token

```python
def analyze_attention_patterns(attention_weights):
    """Analyze common attention patterns"""
    
    def diagonal_attention_score(attn_matrix):
        """Measure how much attention follows diagonal pattern"""
        diag_attention = torch.diag(attn_matrix, diagonal=1)  # Next token
        return diag_attention.sum() / attn_matrix.sum()
    
    def vertical_attention_score(attn_matrix):
        """Measure attention concentration on specific positions"""
        column_max = attn_matrix.max(dim=0)[0]
        return column_max.max()  # Highest concentration
    
    patterns = {}
    for layer_idx, layer_attn in enumerate(attention_weights):
        layer_patterns = {}
        
        for head_idx in range(layer_attn.shape[1]):  # Number of heads
            head_attn = layer_attn[0, head_idx]  # Remove batch dim
            
            layer_patterns[f'head_{head_idx}'] = {
                'diagonal_score': diagonal_attention_score(head_attn),
                'vertical_score': vertical_attention_score(head_attn)
            }
        
        patterns[f'layer_{layer_idx}'] = layer_patterns
    
    return patterns
```

## Training Considerations

### Initialization
Proper initialization is crucial for stable training:

```python
def init_transformer_weights(module):
    """Initialize transformer weights"""
    if isinstance(module, nn.Linear):
        # Xavier/Glorot initialization
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    elif isinstance(module, nn.LayerNorm):
        torch.nn.init.zeros_(module.bias)
        torch.nn.init.ones_(module.weight)
```

### Learning Rate Scheduling

Transformers typically use warm-up followed by decay:

```python
class TransformerLRScheduler:
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0
        
    def step(self):
        self.step_num += 1
        lr = self.get_lr()
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_lr(self):
        return self.d_model**(-0.5) * min(
            self.step_num**(-0.5),
            self.step_num * self.warmup_steps**(-1.5)
        )
```

### Memory Optimization

Transformers can be memory-intensive. Optimization techniques:

```python
# Gradient checkpointing to trade compute for memory
def checkpoint_forward(module, input):
    return torch.utils.checkpoint.checkpoint(module, input)

# Flash Attention for efficient attention computation
# (Conceptual - actual implementation is more complex)
def flash_attention(Q, K, V, block_size=64):
    """Memory-efficient attention computation"""
    # Process attention in blocks to reduce memory usage
    # This is a simplified concept - real implementation is in CUDA
    pass
```

## Applications Beyond Language

### Vision Transformer (ViT)
Applies Transformers to computer vision:

```python
class VisionTransformer(nn.Module):
    def __init__(self, img_size, patch_size, num_classes, d_model, num_heads, num_layers):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # Patch embedding
        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)
        
        # Positional embedding
        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))
        
        # Classification token
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))
        
        # Transformer blocks
        self.transformer = TransformerEncoder(None, d_model, num_heads, num_layers, d_model*4)
        
        # Classification head
        self.head = nn.Linear(d_model, num_classes)
        
    def forward(self, x):
        B, C, H, W = x.shape
        
        # Patch embedding: [B, C, H, W] -> [B, N, D]
        x = self.patch_embed(x).flatten(2).transpose(1, 2)
        
        # Add classification token
        cls_token = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_token, x], dim=1)
        
        # Add positional embedding
        x = x + self.pos_embed
        
        # Apply transformer
        x = self.transformer(x)
        
        # Classification using CLS token
        return self.head(x[:, 0])
```

### Scientific Applications

Transformers excel in scientific domains:

#### Protein Folding (AlphaFold-style)
```python
class ProteinTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers):
        super().__init__()
        # Amino acid embedding
        self.aa_embed = nn.Embedding(vocab_size, d_model)
        
        # MSA (Multiple Sequence Alignment) processing
        self.msa_transformer = TransformerEncoder(...)
        
        # Structure prediction head
        self.structure_head = nn.Linear(d_model, 3)  # x, y, z coordinates
        
    def forward(self, sequence, msa=None):
        # Process sequence and MSA information
        # Predict 3D structure
        pass
```

#### Chemical Property Prediction
```python
class MolecularTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers):
        super().__init__()
        # SMILES string tokenization
        self.token_embed = nn.Embedding(vocab_size, d_model)
        self.transformer = TransformerEncoder(...)
        
        # Property prediction heads
        self.property_heads = nn.ModuleDict({
            'solubility': nn.Linear(d_model, 1),
            'toxicity': nn.Linear(d_model, 1),
            'bioactivity': nn.Linear(d_model, 1)
        })
```

## Modern Improvements and Research Directions

### Efficiency Improvements

**Linear Attention:**
Reduce quadratic complexity to linear:

```python
class LinearAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
    def forward(self, q, k, v):
        # Linear attention approximation
        # O(n) instead of O(n²)
        q = F.elu(q) + 1
        k = F.elu(k) + 1
        
        # Compute attention efficiently
        context = torch.einsum('bnd,bne->bde', k, v)
        attn_output = torch.einsum('bnd,bde->bne', q, context)
        
        return attn_output / torch.einsum('bnd,bne->bn', q, k).unsqueeze(-1)
```

**Sparse Attention:**
Only attend to selected positions:

```python
class SparseAttention(nn.Module):
    def __init__(self, d_model, num_heads, sparsity_pattern='local'):
        super().__init__()
        self.sparsity_pattern = sparsity_pattern
        
    def create_sparse_mask(self, seq_len):
        if self.sparsity_pattern == 'local':
            # Local attention window
            mask = torch.zeros(seq_len, seq_len)
            for i in range(seq_len):
                start = max(0, i - 32)
                end = min(seq_len, i + 32)
                mask[i, start:end] = 1
        
        return mask
```

### Architectural Innovations

**Mixture of Experts (MoE):**
```python
class MixtureOfExperts(nn.Module):
    def __init__(self, d_model, num_experts, expert_capacity):
        super().__init__()
        self.num_experts = num_experts
        self.experts = nn.ModuleList([
            FeedForward(d_model, d_model * 4) 
            for _ in range(num_experts)
        ])
        self.gate = nn.Linear(d_model, num_experts)
        
    def forward(self, x):
        # Route tokens to experts based on gating
        gate_scores = F.softmax(self.gate(x), dim=-1)
        
        # Select top-k experts (usually k=1 or k=2)
        top_k_gates, top_k_indices = gate_scores.topk(k=2, dim=-1)
        
        # Process through selected experts
        expert_outputs = []
        for i, expert in enumerate(self.experts):
            mask = (top_k_indices == i).any(-1)
            if mask.any():
                expert_output = expert(x[mask])
                expert_outputs.append(expert_output)
        
        # Combine expert outputs
        return self._combine_expert_outputs(expert_outputs, gate_scores, top_k_indices)
```

## Transformers in Scientific Research

### Literature Analysis (EderSpark Application)

```python
class ScientificTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers):
        super().__init__()
        self.transformer = TransformerEncoder(
            vocab_size, d_model, num_heads, num_layers, d_model*4
        )
        
        # Scientific understanding heads
        self.citation_head = nn.Linear(d_model, vocab_size)  # Citation prediction
        self.entity_head = nn.Linear(d_model, num_entity_types)  # Scientific entities
        self.relation_head = nn.Linear(d_model, num_relations)  # Entity relationships
        
    def forward(self, paper_tokens):
        # Process scientific paper
        hidden_states = self.transformer(paper_tokens)
        
        # Multiple scientific understanding tasks
        citations = self.citation_head(hidden_states)
        entities = self.entity_head(hidden_states)
        relations = self.relation_head(hidden_states)
        
        return {
            'citations': citations,
            'entities': entities,
            'relations': relations
        }
```

### Semantic Search Implementation

```python
class SemanticSearchTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers):
        super().__init__()
        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers)
        self.pooler = nn.Linear(d_model, d_model)
        
    def forward(self, input_ids, attention_mask=None):
        # Encode text
        hidden_states = self.encoder(input_ids, attention_mask)
        
        # Pool to get document representation
        # Use CLS token or mean pooling
        document_repr = self.pooler(hidden_states[:, 0])  # CLS token
        
        return F.normalize(document_repr, p=2, dim=-1)  # L2 normalize
    
    def semantic_similarity(self, query_repr, doc_reprs):
        """Compute semantic similarity scores"""
        return torch.matmul(query_repr, doc_reprs.T)
```

The Transformer architecture has fundamentally changed how we approach AI problems. Its ability to capture long-range dependencies, process sequences in parallel, and scale to massive sizes has made it the foundation for modern AI systems across domains.

Understanding Transformers is essential for working with contemporary AI, from language models like GPT and Claude to vision models, scientific computing applications, and the emerging field of foundation models that power diverse AI capabilities.

## Next Steps

<Card
  title="Attention Mechanisms"
  icon="eye"
  href="/fundamentals/attention-mechanisms"
>
  Deep dive into different types of attention mechanisms and their applications.
</Card>

<Card
  title="Large Language Models"
  icon="message-lines"
  href="/llms/introduction"
>
  See how Transformers scale to create powerful language models.
</Card>

<Card
  title="Scientific Applications"
  icon="flask"
  href="/applications/scientific-research"
>
  Explore how Transformers accelerate scientific discovery.
</Card>