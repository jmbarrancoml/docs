---
title: "Robustness and Failure Modes"
description: "Comprehensive guide to building robust AI systems, understanding failure modes, defending against adversarial attacks, and ensuring reliable performance across diverse conditions"
---

# Robustness and Failure Modes

Building robust AI systems requires understanding and defending against various failure modes, adversarial attacks, and edge cases that can compromise system performance and safety. This guide explores advanced techniques for enhancing model robustness, detecting and mitigating failure modes, and ensuring reliable AI performance in real-world scientific applications.

## Understanding AI System Robustness

### Defining Robustness in AI Systems

Robustness encompasses multiple dimensions of system reliability and resilience:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Callable, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging
import json
from enum import Enum
import matplotlib.pyplot as plt
from collections import defaultdict
import random
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler

class RobustnessType(Enum):
    """Types of robustness to evaluate"""
    ADVERSARIAL = "adversarial"
    DISTRIBUTIONAL = "distributional"
    TEMPORAL = "temporal"
    NOISE = "noise"
    SEMANTIC = "semantic"
    CALIBRATION = "calibration"

@dataclass
class RobustnessMetrics:
    """Comprehensive robustness evaluation metrics"""
    adversarial_accuracy: float = 0.0
    distributional_robustness: float = 0.0
    noise_tolerance: float = 0.0
    semantic_consistency: float = 0.0
    calibration_error: float = 0.0
    worst_case_performance: float = 0.0
    confidence_under_attack: float = 0.0
    recovery_capability: float = 0.0
    
    def overall_robustness_score(self) -> float:
        """Calculate weighted overall robustness score"""
        weights = {
            'adversarial_accuracy': 0.2,
            'distributional_robustness': 0.2,
            'noise_tolerance': 0.15,
            'semantic_consistency': 0.15,
            'calibration_error': -0.1,  # Negative because lower is better
            'worst_case_performance': 0.15,
            'confidence_under_attack': 0.1,
            'recovery_capability': 0.05
        }
        
        score = 0.0
        for metric, weight in weights.items():
            value = getattr(self, metric)
            if metric == 'calibration_error':
                score += weight * (1.0 - value)  # Invert calibration error
            else:
                score += weight * value
        
        return max(0.0, min(1.0, score))

class FailureMode:
    """Representation of a specific failure mode"""
    
    def __init__(self, name: str, description: str, severity: float, 
                 detection_method: Optional[Callable] = None,
                 mitigation_strategy: Optional[str] = None):
        self.name = name
        self.description = description
        self.severity = severity  # 0.0 to 1.0
        self.detection_method = detection_method
        self.mitigation_strategy = mitigation_strategy
        self.occurrence_frequency = 0.0
        self.impact_score = 0.0
    
    def detect(self, model_output: Any, input_data: Any, context: Dict[str, Any]) -> bool:
        """Detect if this failure mode is occurring"""
        if self.detection_method:
            return self.detection_method(model_output, input_data, context)
        return False
    
    def calculate_risk_score(self) -> float:
        """Calculate risk score based on severity and frequency"""
        return self.severity * self.occurrence_frequency * self.impact_score

class RobustnessEvaluator:
    """Comprehensive robustness evaluation framework"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.failure_modes = self._initialize_failure_modes()
        self.robustness_history = []
        
    def _initialize_failure_modes(self) -> List[FailureMode]:
        """Initialize common AI failure modes"""
        return [
            FailureMode(
                name="adversarial_vulnerability",
                description="Susceptibility to adversarial examples",
                severity=0.8,
                detection_method=self._detect_adversarial_vulnerability,
                mitigation_strategy="adversarial_training"
            ),
            FailureMode(
                name="distributional_shift",
                description="Performance degradation on out-of-distribution data",
                severity=0.7,
                detection_method=self._detect_distributional_shift,
                mitigation_strategy="domain_adaptation"
            ),
            FailureMode(
                name="overconfidence",
                description="High confidence on incorrect predictions",
                severity=0.6,
                detection_method=self._detect_overconfidence,
                mitigation_strategy="calibration_training"
            ),
            FailureMode(
                name="catastrophic_forgetting",
                description="Loss of previously learned knowledge",
                severity=0.7,
                detection_method=self._detect_catastrophic_forgetting,
                mitigation_strategy="continual_learning"
            ),
            FailureMode(
                name="mode_collapse",
                description="Model outputs limited to subset of possible outputs",
                severity=0.6,
                detection_method=self._detect_mode_collapse,
                mitigation_strategy="regularization"
            ),
            FailureMode(
                name="spurious_correlations",
                description="Reliance on irrelevant features or biases",
                severity=0.8,
                detection_method=self._detect_spurious_correlations,
                mitigation_strategy="debiasing_techniques"
            ),
            FailureMode(
                name="gradient_exploding",
                description="Unstable training due to exploding gradients",
                severity=0.5,
                detection_method=self._detect_gradient_exploding,
                mitigation_strategy="gradient_clipping"
            ),
            FailureMode(
                name="data_poisoning_vulnerability",
                description="Susceptibility to poisoned training data",
                severity=0.9,
                detection_method=self._detect_data_poisoning,
                mitigation_strategy="robust_training"
            )
        ]
    
    def evaluate_robustness(self, test_data: torch.utils.data.DataLoader,
                           adversarial_examples: Optional[torch.Tensor] = None) -> RobustnessMetrics:
        """Comprehensive robustness evaluation"""
        
        metrics = RobustnessMetrics()
        
        # Evaluate adversarial robustness
        if adversarial_examples is not None:
            metrics.adversarial_accuracy = self._evaluate_adversarial_robustness(adversarial_examples)
        
        # Evaluate distributional robustness
        metrics.distributional_robustness = self._evaluate_distributional_robustness(test_data)
        
        # Evaluate noise tolerance
        metrics.noise_tolerance = self._evaluate_noise_tolerance(test_data)
        
        # Evaluate semantic consistency
        metrics.semantic_consistency = self._evaluate_semantic_consistency(test_data)
        
        # Evaluate calibration
        metrics.calibration_error = self._evaluate_calibration(test_data)
        
        # Evaluate worst-case performance
        metrics.worst_case_performance = self._evaluate_worst_case_performance(test_data)
        
        # Evaluate confidence under attack
        metrics.confidence_under_attack = self._evaluate_confidence_under_attack(test_data)
        
        # Evaluate recovery capability
        metrics.recovery_capability = self._evaluate_recovery_capability(test_data)
        
        # Check for failure modes
        self._check_failure_modes(test_data)
        
        return metrics
    
    def _evaluate_adversarial_robustness(self, adversarial_examples: torch.Tensor) -> float:
        """Evaluate robustness against adversarial examples"""
        
        self.model.eval()
        correct_predictions = 0
        total_examples = 0
        
        with torch.no_grad():
            for batch_start in range(0, len(adversarial_examples), 32):
                batch_end = min(batch_start + 32, len(adversarial_examples))
                batch = adversarial_examples[batch_start:batch_end]
                
                outputs = self.model(batch)
                predictions = torch.argmax(outputs, dim=1)
                
                # Compare with clean predictions (simplified)
                clean_batch = batch  # Would normally have clean versions
                clean_outputs = self.model(clean_batch)
                clean_predictions = torch.argmax(clean_outputs, dim=1)
                
                correct_predictions += (predictions == clean_predictions).sum().item()
                total_examples += batch.size(0)
        
        return correct_predictions / total_examples if total_examples > 0 else 0.0
    
    def _evaluate_distributional_robustness(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate robustness to distributional shift"""
        
        self.model.eval()
        
        # Create shifted versions of test data
        shifted_accuracies = []
        
        for shift_type in ['gaussian_noise', 'brightness', 'contrast']:
            shifted_accuracy = 0.0
            total_batches = 0
            
            with torch.no_grad():
                for batch_data, batch_labels in test_data:
                    # Apply distribution shift
                    shifted_data = self._apply_distribution_shift(batch_data, shift_type)
                    
                    outputs = self.model(shifted_data)
                    predictions = torch.argmax(outputs, dim=1)
                    accuracy = (predictions == batch_labels).float().mean().item()
                    
                    shifted_accuracy += accuracy
                    total_batches += 1
                    
                    if total_batches >= 10:  # Limit for efficiency
                        break
            
            if total_batches > 0:
                shifted_accuracies.append(shifted_accuracy / total_batches)
        
        return np.mean(shifted_accuracies) if shifted_accuracies else 0.0
    
    def _evaluate_noise_tolerance(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate tolerance to different types of noise"""
        
        self.model.eval()
        noise_tolerances = []
        
        noise_levels = [0.01, 0.05, 0.1, 0.2]
        
        for noise_level in noise_levels:
            correct_predictions = 0
            total_predictions = 0
            
            with torch.no_grad():
                for batch_data, batch_labels in test_data:
                    # Add Gaussian noise
                    noise = torch.randn_like(batch_data) * noise_level
                    noisy_data = batch_data + noise
                    
                    outputs = self.model(noisy_data)
                    predictions = torch.argmax(outputs, dim=1)
                    
                    correct_predictions += (predictions == batch_labels).sum().item()
                    total_predictions += batch_labels.size(0)
                    
                    if total_predictions >= 1000:  # Limit for efficiency
                        break
            
            if total_predictions > 0:
                tolerance = correct_predictions / total_predictions
                noise_tolerances.append(tolerance)
        
        return np.mean(noise_tolerances) if noise_tolerances else 0.0
    
    def _evaluate_semantic_consistency(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate semantic consistency across similar inputs"""
        
        self.model.eval()
        consistency_scores = []
        
        with torch.no_grad():
            for batch_data, batch_labels in test_data:
                if batch_data.size(0) < 2:
                    continue
                
                # Create semantically similar pairs (simplified)
                original_outputs = self.model(batch_data)
                
                # Apply small semantic-preserving transformations
                transformed_data = self._apply_semantic_transformations(batch_data)
                transformed_outputs = self.model(transformed_data)
                
                # Calculate consistency
                original_probs = F.softmax(original_outputs, dim=1)
                transformed_probs = F.softmax(transformed_outputs, dim=1)
                
                # KL divergence as consistency measure
                kl_div = F.kl_div(transformed_probs.log(), original_probs, reduction='batchmean')
                consistency = torch.exp(-kl_div).item()
                consistency_scores.append(consistency)
                
                if len(consistency_scores) >= 50:  # Limit for efficiency
                    break
        
        return np.mean(consistency_scores) if consistency_scores else 0.0
    
    def _evaluate_calibration(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate model calibration (Expected Calibration Error)"""
        
        self.model.eval()
        
        all_confidences = []
        all_accuracies = []
        
        with torch.no_grad():
            for batch_data, batch_labels in test_data:
                outputs = self.model(batch_data)
                probabilities = F.softmax(outputs, dim=1)
                
                confidences, predictions = torch.max(probabilities, dim=1)
                accuracies = (predictions == batch_labels).float()
                
                all_confidences.extend(confidences.cpu().numpy())
                all_accuracies.extend(accuracies.cpu().numpy())
        
        # Calculate Expected Calibration Error (ECE)
        n_bins = 15
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0.0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (np.array(all_confidences) > bin_lower) & (np.array(all_confidences) <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = np.array(all_accuracies)[in_bin].mean()
                avg_confidence_in_bin = np.array(all_confidences)[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    def _evaluate_worst_case_performance(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate worst-case performance across different conditions"""
        
        self.model.eval()
        performance_scores = []
        
        # Test under various challenging conditions
        challenging_conditions = [
            lambda x: x,  # Normal condition
            lambda x: self._apply_distribution_shift(x, 'gaussian_noise'),
            lambda x: x + torch.randn_like(x) * 0.1,  # Noise
            lambda x: torch.clamp(x * 0.8, 0, 1),  # Brightness reduction
        ]
        
        for condition in challenging_conditions:
            correct_predictions = 0
            total_predictions = 0
            
            with torch.no_grad():
                for batch_data, batch_labels in test_data:
                    modified_data = condition(batch_data)
                    outputs = self.model(modified_data)
                    predictions = torch.argmax(outputs, dim=1)
                    
                    correct_predictions += (predictions == batch_labels).sum().item()
                    total_predictions += batch_labels.size(0)
                    
                    if total_predictions >= 500:  # Limit for efficiency
                        break
            
            if total_predictions > 0:
                performance = correct_predictions / total_predictions
                performance_scores.append(performance)
        
        # Return minimum performance (worst case)
        return min(performance_scores) if performance_scores else 0.0
    
    def _evaluate_confidence_under_attack(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate how confidence changes under adversarial conditions"""
        
        self.model.eval()
        confidence_changes = []
        
        with torch.no_grad():
            for batch_data, batch_labels in test_data:
                # Original confidence
                original_outputs = self.model(batch_data)
                original_probs = F.softmax(original_outputs, dim=1)
                original_confidence = torch.max(original_probs, dim=1)[0]
                
                # Add small perturbations
                perturbation = torch.randn_like(batch_data) * 0.05
                perturbed_data = batch_data + perturbation
                
                perturbed_outputs = self.model(perturbed_data)
                perturbed_probs = F.softmax(perturbed_outputs, dim=1)
                perturbed_confidence = torch.max(perturbed_probs, dim=1)[0]
                
                # Calculate confidence stability
                confidence_change = torch.abs(original_confidence - perturbed_confidence)
                confidence_changes.extend(confidence_change.cpu().numpy())
                
                if len(confidence_changes) >= 1000:  # Limit for efficiency
                    break
        
        # Return stability score (1 - mean absolute change)
        if confidence_changes:
            mean_change = np.mean(confidence_changes)
            return max(0.0, 1.0 - mean_change)
        return 0.0
    
    def _evaluate_recovery_capability(self, test_data: torch.utils.data.DataLoader) -> float:
        """Evaluate ability to recover from perturbations"""
        
        # This would typically involve testing model behavior after various interventions
        # Simplified implementation for demonstration
        
        self.model.eval()
        recovery_scores = []
        
        with torch.no_grad():
            for batch_data, batch_labels in test_data:
                # Apply strong perturbation
                strong_noise = torch.randn_like(batch_data) * 0.2
                perturbed_data = batch_data + strong_noise
                
                # Apply denoising (simplified)
                denoised_data = perturbed_data * 0.8 + batch_data * 0.2
                
                # Compare performance
                original_outputs = self.model(batch_data)
                recovered_outputs = self.model(denoised_data)
                
                original_acc = (torch.argmax(original_outputs, dim=1) == batch_labels).float().mean()
                recovered_acc = (torch.argmax(recovered_outputs, dim=1) == batch_labels).float().mean()
                
                # Recovery ratio
                recovery_ratio = recovered_acc / (original_acc + 1e-8)
                recovery_scores.append(recovery_ratio.item())
                
                if len(recovery_scores) >= 50:  # Limit for efficiency
                    break
        
        return np.mean(recovery_scores) if recovery_scores else 0.0
    
    # Helper methods for data transformations
    def _apply_distribution_shift(self, data: torch.Tensor, shift_type: str) -> torch.Tensor:
        """Apply various types of distribution shifts"""
        
        if shift_type == 'gaussian_noise':
            noise = torch.randn_like(data) * 0.1
            return data + noise
        elif shift_type == 'brightness':
            brightness_factor = np.random.uniform(0.5, 1.5)
            return torch.clamp(data * brightness_factor, 0, 1)
        elif shift_type == 'contrast':
            contrast_factor = np.random.uniform(0.5, 1.5)
            mean = data.mean()
            return torch.clamp((data - mean) * contrast_factor + mean, 0, 1)
        else:
            return data
    
    def _apply_semantic_transformations(self, data: torch.Tensor) -> torch.Tensor:
        """Apply semantic-preserving transformations"""
        
        # Simple example: small rotations or translations for images
        # For text, this might be synonym substitutions
        
        if len(data.shape) == 4:  # Assume image data
            # Small random rotation (simplified)
            angle = np.random.uniform(-5, 5) * np.pi / 180
            cos_angle = np.cos(angle)
            sin_angle = np.sin(angle)
            
            # Apply very small transformation
            noise = torch.randn_like(data) * 0.01
            return data + noise
        else:
            # For other data types, apply small noise
            return data + torch.randn_like(data) * 0.01
    
    def _check_failure_modes(self, test_data: torch.utils.data.DataLoader):
        """Check for occurrence of known failure modes"""
        
        for failure_mode in self.failure_modes:
            failure_count = 0
            total_checks = 0
            
            for batch_data, batch_labels in test_data:
                outputs = self.model(batch_data)
                
                for i in range(batch_data.size(0)):
                    context = {'batch_index': i, 'total_batches': total_checks}
                    
                    if failure_mode.detect(outputs[i], batch_data[i], context):
                        failure_count += 1
                    
                    total_checks += 1
                    
                    if total_checks >= 100:  # Limit for efficiency
                        break
                
                if total_checks >= 100:
                    break
            
            if total_checks > 0:
                failure_mode.occurrence_frequency = failure_count / total_checks
                failure_mode.impact_score = failure_mode.occurrence_frequency * failure_mode.severity
    
    # Failure mode detection methods
    def _detect_adversarial_vulnerability(self, model_output: torch.Tensor, 
                                        input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect adversarial vulnerability"""
        
        # Check for high confidence on potentially adversarial examples
        probs = F.softmax(model_output, dim=0)
        max_prob = torch.max(probs)
        
        # Simple heuristic: very high confidence might indicate adversarial example
        return max_prob > 0.99
    
    def _detect_distributional_shift(self, model_output: torch.Tensor,
                                   input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect distributional shift indicators"""
        
        # Check for unusual confidence patterns
        probs = F.softmax(model_output, dim=0)
        entropy = -torch.sum(probs * torch.log(probs + 1e-8))
        
        # High entropy might indicate distribution shift
        return entropy > 2.0
    
    def _detect_overconfidence(self, model_output: torch.Tensor,
                             input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect overconfidence in predictions"""
        
        probs = F.softmax(model_output, dim=0)
        max_prob = torch.max(probs)
        
        # Very high confidence might indicate overconfidence
        return max_prob > 0.95
    
    def _detect_catastrophic_forgetting(self, model_output: torch.Tensor,
                                      input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect catastrophic forgetting (simplified)"""
        
        # This would typically require tracking performance over time
        # Simplified: check for very low confidence on previously seen patterns
        
        probs = F.softmax(model_output, dim=0)
        max_prob = torch.max(probs)
        
        return max_prob < 0.1
    
    def _detect_mode_collapse(self, model_output: torch.Tensor,
                            input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect mode collapse"""
        
        # Check if model always predicts same class (simplified)
        probs = F.softmax(model_output, dim=0)
        predicted_class = torch.argmax(probs)
        
        # This would need to track predictions over time
        return predicted_class.item() == 0  # Simplified check
    
    def _detect_spurious_correlations(self, model_output: torch.Tensor,
                                    input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect reliance on spurious correlations"""
        
        # This would require more sophisticated analysis of model attention/gradients
        # Simplified implementation
        
        return False  # Placeholder
    
    def _detect_gradient_exploding(self, model_output: torch.Tensor,
                                 input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect gradient exploding issues"""
        
        # Check for extreme output values
        max_output = torch.max(torch.abs(model_output))
        
        return max_output > 100.0
    
    def _detect_data_poisoning(self, model_output: torch.Tensor,
                             input_data: torch.Tensor, context: Dict[str, Any]) -> bool:
        """Detect potential data poisoning effects"""
        
        # This would require more sophisticated backdoor detection
        # Simplified implementation
        
        probs = F.softmax(model_output, dim=0)
        
        # Check for unusual prediction patterns
        unusual_prediction = torch.max(probs) > 0.98 and torch.sum(probs > 0.01) == 1
        
        return unusual_prediction
```

## Adversarial Robustness Techniques

### Advanced Adversarial Defense Methods

```python
class AdversarialDefense:
    """Comprehensive adversarial defense framework"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.defense_methods = {
            'adversarial_training': self.adversarial_training,
            'defensive_distillation': self.defensive_distillation,
            'input_preprocessing': self.input_preprocessing,
            'randomized_smoothing': self.randomized_smoothing,
            'feature_squeezing': self.feature_squeezing,
            'adversarial_detection': self.adversarial_detection
        }
    
    def adversarial_training(self, train_loader: torch.utils.data.DataLoader,
                           optimizer: torch.optim.Optimizer, num_epochs: int = 10,
                           attack_epsilon: float = 0.1) -> nn.Module:
        """Adversarial training for robustness"""
        
        self.model.train()
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            
            for batch_data, batch_labels in train_loader:
                optimizer.zero_grad()
                
                # Generate adversarial examples using PGD
                adversarial_data = self.pgd_attack(batch_data, batch_labels, 
                                                 epsilon=attack_epsilon, num_steps=10)
                
                # Forward pass on adversarial examples
                adversarial_outputs = self.model(adversarial_data)
                adversarial_loss = F.cross_entropy(adversarial_outputs, batch_labels)
                
                # Forward pass on clean examples
                clean_outputs = self.model(batch_data)
                clean_loss = F.cross_entropy(clean_outputs, batch_labels)
                
                # Combined loss
                total_loss = 0.5 * clean_loss + 0.5 * adversarial_loss
                
                total_loss.backward()
                optimizer.step()
                
                epoch_loss += total_loss.item()
            
            logging.info(f"Adversarial Training Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}")
        
        return self.model
    
    def pgd_attack(self, data: torch.Tensor, labels: torch.Tensor,
                   epsilon: float = 0.1, num_steps: int = 10, step_size: float = 0.01) -> torch.Tensor:
        """Projected Gradient Descent attack"""
        
        adversarial_data = data.clone().detach()
        adversarial_data.requires_grad = True
        
        for step in range(num_steps):
            # Forward pass
            outputs = self.model(adversarial_data)
            loss = F.cross_entropy(outputs, labels)
            
            # Backward pass
            self.model.zero_grad()
            loss.backward()
            
            # Update adversarial example
            with torch.no_grad():
                gradient_sign = adversarial_data.grad.sign()
                adversarial_data = adversarial_data + step_size * gradient_sign
                
                # Project back to epsilon ball
                perturbation = adversarial_data - data
                perturbation = torch.clamp(perturbation, -epsilon, epsilon)
                adversarial_data = data + perturbation
                adversarial_data = torch.clamp(adversarial_data, 0, 1)  # Ensure valid range
            
            adversarial_data.requires_grad = True
        
        return adversarial_data.detach()
    
    def defensive_distillation(self, teacher_model: nn.Module, train_loader: torch.utils.data.DataLoader,
                              optimizer: torch.optim.Optimizer, temperature: float = 20.0,
                              num_epochs: int = 10) -> nn.Module:
        """Defensive distillation for adversarial robustness"""
        
        # First phase: train teacher with high temperature
        teacher_model.train()
        for epoch in range(num_epochs // 2):
            for batch_data, batch_labels in train_loader:
                optimizer.zero_grad()
                
                teacher_outputs = teacher_model(batch_data) / temperature
                loss = F.cross_entropy(teacher_outputs, batch_labels)
                
                loss.backward()
                optimizer.step()
        
        # Second phase: train student model using teacher's soft labels
        self.model.train()
        teacher_model.eval()
        
        student_optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        
        for epoch in range(num_epochs // 2):
            for batch_data, batch_labels in train_loader:
                student_optimizer.zero_grad()
                
                # Get soft labels from teacher
                with torch.no_grad():
                    teacher_outputs = teacher_model(batch_data) / temperature
                    soft_labels = F.softmax(teacher_outputs, dim=1)
                
                # Train student to match soft labels
                student_outputs = self.model(batch_data) / temperature
                student_probs = F.log_softmax(student_outputs, dim=1)
                
                # KL divergence loss
                distillation_loss = F.kl_div(student_probs, soft_labels, reduction='batchmean')
                
                # Also include hard label loss
                hard_loss = F.cross_entropy(self.model(batch_data), batch_labels)
                
                total_loss = 0.7 * distillation_loss * (temperature ** 2) + 0.3 * hard_loss
                
                total_loss.backward()
                student_optimizer.step()
        
        return self.model
    
    def input_preprocessing(self, data: torch.Tensor, method: str = 'jpeg_compression') -> torch.Tensor:
        """Input preprocessing for adversarial defense"""
        
        if method == 'jpeg_compression':
            return self._jpeg_compression(data)
        elif method == 'total_variance_minimization':
            return self._total_variance_minimization(data)
        elif method == 'feature_squeezing':
            return self._feature_squeezing(data)
        else:
            return data
    
    def randomized_smoothing(self, data: torch.Tensor, noise_std: float = 0.12,
                           num_samples: int = 100) -> torch.Tensor:
        """Randomized smoothing for certified robustness"""
        
        self.model.eval()
        
        with torch.no_grad():
            predictions = []
            
            for _ in range(num_samples):
                # Add Gaussian noise
                noise = torch.randn_like(data) * noise_std
                noisy_data = data + noise
                
                # Get prediction
                outputs = self.model(noisy_data)
                predictions.append(F.softmax(outputs, dim=1))
            
            # Average predictions
            avg_predictions = torch.stack(predictions).mean(dim=0)
        
        return avg_predictions
    
    def adversarial_detection(self, data: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:
        """Detect adversarial examples"""
        
        self.model.eval()
        
        with torch.no_grad():
            # Original prediction
            original_outputs = self.model(data)
            original_probs = F.softmax(original_outputs, dim=1)
            
            # Add small random noise and check prediction stability
            noise = torch.randn_like(data) * 0.01
            noisy_data = data + noise
            noisy_outputs = self.model(noisy_data)
            noisy_probs = F.softmax(noisy_outputs, dim=1)
            
            # Calculate prediction stability
            kl_div = F.kl_div(noisy_probs.log(), original_probs, reduction='none').sum(dim=1)
            
            # Flag samples with high instability as potential adversarial examples
            is_adversarial = kl_div > threshold
        
        return is_adversarial
    
    def _jpeg_compression(self, data: torch.Tensor, quality: int = 75) -> torch.Tensor:
        """JPEG compression preprocessing (simplified simulation)"""
        
        # Simulate JPEG compression by adding quantization noise
        noise = torch.randn_like(data) * 0.01
        compressed_data = data + noise
        compressed_data = torch.round(compressed_data * 255) / 255
        
        return torch.clamp(compressed_data, 0, 1)
    
    def _total_variance_minimization(self, data: torch.Tensor, lambda_tv: float = 0.1) -> torch.Tensor:
        """Total variation minimization for denoising"""
        
        # Simplified TV minimization
        if len(data.shape) == 4 and data.shape[2] > 1 and data.shape[3] > 1:
            # Calculate TV loss
            diff_i = torch.sum(torch.abs(data[:, :, 1:, :] - data[:, :, :-1, :]))
            diff_j = torch.sum(torch.abs(data[:, :, :, 1:] - data[:, :, :, :-1]))
            tv_loss = diff_i + diff_j
            
            # Apply small correction based on TV
            correction = torch.randn_like(data) * lambda_tv * 0.001
            return data - correction
        
        return data
    
    def _feature_squeezing(self, data: torch.Tensor, bit_depth: int = 5) -> torch.Tensor:
        """Feature squeezing by reducing bit depth"""
        
        # Reduce bit depth
        scale = 2 ** bit_depth - 1
        squeezed_data = torch.round(data * scale) / scale
        
        return torch.clamp(squeezed_data, 0, 1)

class CertifiedDefense:
    """Certified adversarial defenses"""
    
    def __init__(self, model: nn.Module, noise_std: float = 0.12):
        self.model = model
        self.noise_std = noise_std
        
    def certified_radius(self, data: torch.Tensor, num_samples: int = 1000,
                        alpha: float = 0.001) -> float:
        """Calculate certified radius using randomized smoothing"""
        
        from scipy import stats
        
        self.model.eval()
        
        with torch.no_grad():
            predictions = []
            
            for _ in range(num_samples):
                noise = torch.randn_like(data) * self.noise_std
                noisy_data = data + noise
                outputs = self.model(noisy_data)
                predicted_class = torch.argmax(outputs, dim=1)
                predictions.append(predicted_class.item())
            
            # Count votes for each class
            unique_classes, counts = np.unique(predictions, return_counts=True)
            
            if len(counts) < 2:
                return 0.0
            
            # Sort counts in descending order
            sorted_counts = np.sort(counts)[::-1]
            top_count = sorted_counts[0]
            second_count = sorted_counts[1] if len(sorted_counts) > 1 else 0
            
            # Calculate confidence interval
            p_lower = stats.binom.ppf(alpha/2, num_samples, top_count/num_samples) / num_samples
            
            if p_lower > 0.5:
                # Calculate certified radius
                radius = self.noise_std * stats.norm.ppf(p_lower)
                return max(0, radius)
        
        return 0.0

class ScientificRobustness:
    """Robustness evaluation specific to scientific AI applications"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.scientific_failure_modes = [
            FailureMode(
                name="citation_hallucination",
                description="Generating non-existent citations",
                severity=0.9,
                detection_method=self._detect_citation_hallucination
            ),
            FailureMode(
                name="methodology_confusion",
                description="Confusing different scientific methodologies",
                severity=0.8,
                detection_method=self._detect_methodology_confusion
            ),
            FailureMode(
                name="statistical_misinterpretation",
                description="Misinterpreting statistical results",
                severity=0.8,
                detection_method=self._detect_statistical_misinterpretation
            ),
            FailureMode(
                name="domain_boundary_confusion",
                description="Applying knowledge across inappropriate domain boundaries",
                severity=0.7,
                detection_method=self._detect_domain_confusion
            ),
            FailureMode(
                name="temporal_inconsistency",
                description="Inconsistency in temporal scientific knowledge",
                severity=0.6,
                detection_method=self._detect_temporal_inconsistency
            )
        ]
    
    def evaluate_scientific_robustness(self, scientific_queries: List[str],
                                     expected_responses: List[str]) -> Dict[str, float]:
        """Evaluate robustness for scientific applications"""
        
        robustness_scores = {}
        
        # Test citation accuracy robustness
        robustness_scores['citation_robustness'] = self._test_citation_robustness(
            scientific_queries, expected_responses
        )
        
        # Test methodological consistency
        robustness_scores['methodology_robustness'] = self._test_methodology_robustness(
            scientific_queries
        )
        
        # Test statistical interpretation robustness
        robustness_scores['statistical_robustness'] = self._test_statistical_robustness(
            scientific_queries
        )
        
        # Test domain boundary robustness
        robustness_scores['domain_robustness'] = self._test_domain_robustness(
            scientific_queries
        )
        
        # Overall scientific robustness
        robustness_scores['overall_scientific_robustness'] = np.mean(
            list(robustness_scores.values())
        )
        
        return robustness_scores
    
    def _test_citation_robustness(self, queries: List[str], expected_responses: List[str]) -> float:
        """Test robustness of citation generation"""
        
        accurate_citations = 0
        total_citations = 0
        
        for query, expected in zip(queries, expected_responses):
            # Extract citations from expected response (simplified)
            expected_citations = self._extract_citations(expected)
            
            # Generate response and extract citations
            generated_response = self._generate_response(query)
            generated_citations = self._extract_citations(generated_response)
            
            # Check citation accuracy
            for citation in generated_citations:
                total_citations += 1
                if self._verify_citation_accuracy(citation, expected_citations):
                    accurate_citations += 1
        
        return accurate_citations / total_citations if total_citations > 0 else 1.0
    
    def _test_methodology_robustness(self, queries: List[str]) -> float:
        """Test robustness of methodology understanding"""
        
        # Create variations of methodology-related queries
        methodology_variations = []
        for query in queries:
            if 'method' in query.lower() or 'approach' in query.lower():
                # Create paraphrased versions
                variations = self._create_methodology_variations(query)
                methodology_variations.extend(variations)
        
        consistent_responses = 0
        total_comparisons = 0
        
        # Test consistency across variations
        for i in range(0, len(methodology_variations), 2):
            if i + 1 < len(methodology_variations):
                response1 = self._generate_response(methodology_variations[i])
                response2 = self._generate_response(methodology_variations[i + 1])
                
                # Check for consistency in methodology description
                if self._check_methodology_consistency(response1, response2):
                    consistent_responses += 1
                
                total_comparisons += 1
        
        return consistent_responses / total_comparisons if total_comparisons > 0 else 1.0
    
    def _test_statistical_robustness(self, queries: List[str]) -> float:
        """Test robustness of statistical interpretation"""
        
        # Filter for statistical queries
        statistical_queries = [q for q in queries if any(stat_term in q.lower() 
                             for stat_term in ['p-value', 'correlation', 'regression', 'significance'])]
        
        if not statistical_queries:
            return 1.0  # No statistical queries to test
        
        correct_interpretations = 0
        total_interpretations = 0
        
        for query in statistical_queries[:10]:  # Limit for efficiency
            response = self._generate_response(query)
            
            # Check for statistical interpretation correctness
            interpretation_quality = self._evaluate_statistical_interpretation(response)
            
            if interpretation_quality > 0.7:  # Threshold for acceptable interpretation
                correct_interpretations += 1
            
            total_interpretations += 1
        
        return correct_interpretations / total_interpretations if total_interpretations > 0 else 1.0
    
    def _test_domain_robustness(self, queries: List[str]) -> float:
        """Test robustness across domain boundaries"""
        
        # Create cross-domain queries
        domain_pairs = [
            ('physics', 'chemistry'),
            ('biology', 'computer science'),
            ('medicine', 'engineering')
        ]
        
        appropriate_boundaries = 0
        total_tests = 0
        
        for domain1, domain2 in domain_pairs:
            # Create queries that mix domains
            mixed_query = f"Apply {domain1} principles to solve {domain2} problems"
            response = self._generate_response(mixed_query)
            
            # Check if response appropriately handles domain boundaries
            if self._check_appropriate_domain_handling(response, domain1, domain2):
                appropriate_boundaries += 1
            
            total_tests += 1
        
        return appropriate_boundaries / total_tests if total_tests > 0 else 1.0
    
    # Helper methods for scientific robustness testing
    def _generate_response(self, query: str) -> str:
        """Generate response using the model (simplified)"""
        # This would interface with the actual model
        return f"Generated response for: {query}"
    
    def _extract_citations(self, text: str) -> List[str]:
        """Extract citations from text (simplified)"""
        import re
        citations = re.findall(r'\[(\d+)\]', text)
        return citations
    
    def _verify_citation_accuracy(self, citation: str, expected_citations: List[str]) -> bool:
        """Verify citation accuracy"""
        return citation in expected_citations
    
    def _create_methodology_variations(self, query: str) -> List[str]:
        """Create variations of methodology queries"""
        variations = []
        
        # Simple paraphrasing (in practice, would use more sophisticated methods)
        if 'method' in query:
            variations.append(query.replace('method', 'approach'))
            variations.append(query.replace('method', 'technique'))
        
        return variations
    
    def _check_methodology_consistency(self, response1: str, response2: str) -> bool:
        """Check consistency between methodology descriptions"""
        # Simplified consistency check
        key_terms1 = set(response1.lower().split())
        key_terms2 = set(response2.lower().split())
        
        overlap = len(key_terms1.intersection(key_terms2))
        total_terms = len(key_terms1.union(key_terms2))
        
        similarity = overlap / total_terms if total_terms > 0 else 0
        return similarity > 0.5  # Threshold for consistency
    
    def _evaluate_statistical_interpretation(self, response: str) -> float:
        """Evaluate quality of statistical interpretation"""
        # Simplified evaluation
        statistical_terms = ['p-value', 'significance', 'correlation', 'confidence interval']
        
        correct_usage = 0
        for term in statistical_terms:
            if term in response.lower():
                # Check for appropriate usage context (simplified)
                correct_usage += 1
        
        return min(1.0, correct_usage / len(statistical_terms))
    
    def _check_appropriate_domain_handling(self, response: str, domain1: str, domain2: str) -> bool:
        """Check if response appropriately handles domain boundaries"""
        # Check for acknowledgment of domain differences
        boundary_indicators = ['however', 'although', 'different', 'limitation', 'careful']
        
        response_lower = response.lower()
        boundary_awareness = any(indicator in response_lower for indicator in boundary_indicators)
        
        return boundary_awareness
    
    # Scientific failure mode detection methods
    def _detect_citation_hallucination(self, model_output: Any, input_data: Any, context: Dict[str, Any]) -> bool:
        """Detect citation hallucination"""
        # Would check against citation database
        return False  # Simplified
    
    def _detect_methodology_confusion(self, model_output: Any, input_data: Any, context: Dict[str, Any]) -> bool:
        """Detect methodology confusion"""
        # Would analyze methodology consistency
        return False  # Simplified
    
    def _detect_statistical_misinterpretation(self, model_output: Any, input_data: Any, context: Dict[str, Any]) -> bool:
        """Detect statistical misinterpretation"""
        # Would check statistical reasoning
        return False  # Simplified
    
    def _detect_domain_confusion(self, model_output: Any, input_data: Any, context: Dict[str, Any]) -> bool:
        """Detect inappropriate domain boundary crossing"""
        # Would analyze domain appropriateness
        return False  # Simplified
    
    def _detect_temporal_inconsistency(self, model_output: Any, input_data: Any, context: Dict[str, Any]) -> bool:
        """Detect temporal inconsistencies in scientific knowledge"""
        # Would check temporal consistency
        return False  # Simplified

# Example usage for scientific AI robustness
def create_scientific_robustness_framework():
    """Create comprehensive robustness framework for scientific AI"""
    
    # Example model (placeholder)
    model = nn.Sequential(
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Dropout(0.1),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 1000)  # Scientific classification
    )
    
    # Initialize robustness evaluator
    robustness_evaluator = RobustnessEvaluator(model)
    
    # Initialize adversarial defense
    adversarial_defense = AdversarialDefense(model)
    
    # Initialize certified defense
    certified_defense = CertifiedDefense(model, noise_std=0.1)
    
    # Initialize scientific robustness evaluator
    scientific_robustness = ScientificRobustness(model)
    
    return {
        'robustness_evaluator': robustness_evaluator,
        'adversarial_defense': adversarial_defense,
        'certified_defense': certified_defense,
        'scientific_robustness': scientific_robustness
    }

# Initialize robustness framework
logging.info("Creating comprehensive robustness framework for scientific AI")
robustness_framework = create_scientific_robustness_framework()

# Example robustness evaluation
# This would use actual test data in practice
dummy_test_data = [(torch.randn(10, 512), torch.randint(0, 1000, (10,))) for _ in range(5)]
dummy_loader = dummy_test_data  # Simplified for demonstration

# Evaluate robustness
# metrics = robustness_framework['robustness_evaluator'].evaluate_robustness(dummy_loader)
# logging.info(f"Overall robustness score: {metrics.overall_robustness_score():.3f}")

# Example scientific robustness evaluation
scientific_queries = [
    "What is the relationship between protein folding and thermodynamics?",
    "How do neural networks compare to biological neural systems?",
    "What statistical methods are appropriate for clinical trial analysis?"
]
expected_responses = ["Response about protein thermodynamics...", "Response comparing neural systems...", "Response about clinical statistics..."]

# scientific_robustness_scores = robustness_framework['scientific_robustness'].evaluate_scientific_robustness(
#     scientific_queries, expected_responses
# )
# logging.info(f"Scientific robustness scores: {scientific_robustness_scores}")
```

## Integration with EderSpark Platform

The robustness techniques described in this guide are specifically designed to support EderSpark's mission of reliable scientific AI:

### Scientific AI Robustness

- **Research Integrity**: Robust systems that maintain accuracy across diverse scientific domains
- **Citation Accuracy**: Defense against hallucinated or incorrect citations in scientific literature
- **Methodological Consistency**: Ensuring consistent application of scientific methods and principles
- **Cross-Domain Reliability**: Robust performance when analyzing literature across different scientific fields

### Freiya Platform Robustness

These robustness methods enable:
- **Reliable Literature Analysis**: Consistent and accurate analysis of scientific papers regardless of source or style
- **Adversarial Resistance**: Protection against attempts to manipulate search results or introduce biased information
- **Error Recovery**: Graceful handling of corrupted or incomplete scientific data
- **Quality Assurance**: Continuous monitoring and validation of AI system performance in scientific applications

The comprehensive robustness framework ensures that EderSpark's AI tools maintain high performance standards even when faced with challenging conditions, adversarial inputs, or edge cases, providing researchers with reliable and trustworthy AI assistance for scientific discovery.